{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"<code>fact</code> User Guide","text":"<code>python-blueprint</code> Project <p>For more information on how this was built and deployed, as well as other Python best practices, see <code>python-blueprint</code>.</p> <p>Info</p> <p>This user guide is purely an illustrative example that shows off several features of Material for MkDocs and included Markdown extensions<sup>1</sup>.</p>"},{"location":"#installation","title":"Installation","text":"<p>First, install <code>uv</code>:</p> macOS and LinuxWindows <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <pre><code>powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> <p>Then install the <code>fact</code> package and its dependencies:</p> <pre><code>uv sync\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>To run the included CLI:</p> <pre><code>uv run fact 3\n</code></pre> <p>To use <code>fact</code> as a library within your project, import the <code>factorial</code> function and execute the API like:</p> <pre><code>from fact.lib import factorial\n\nassert factorial(3) == 6  # (1)!\n</code></pre> <ol> <li>This assertion will be <code>True</code></li> </ol> <p>Tip</p> <p>Within PyCharm, use Tab to auto-complete suggested imports while typing.</p>"},{"location":"#expected-results","title":"Expected Results","text":"Input Output 1 1 2 2 3 6 4 24 <ol> <li> <p>See <code>python-blueprint</code>'s <code>mkdocs.yml</code> for how to enable these features.\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/pinn/","title":"pinn","text":""},{"location":"reference/pinn/#pinn","title":"<code>pinn</code>","text":"<p>PINN: Physics-Informed Neural Networks library.</p>"},{"location":"reference/pinn/#pinn.__version__","title":"<code>__version__ = '0.0.1'</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/core/","title":"core","text":""},{"location":"reference/pinn/core/#pinn.core","title":"<code>pinn.core</code>","text":"<p>Core PINN building blocks.</p>"},{"location":"reference/pinn/core/#pinn.core.Activations","title":"<code>Activations: TypeAlias = Literal['tanh', 'relu', 'leaky_relu', 'sigmoid', 'selu', 'softplus', 'identity']</code>  <code>module-attribute</code>","text":"<p>Supported activation functions.</p>"},{"location":"reference/pinn/core/#pinn.core.ArgsRegistry","title":"<code>ArgsRegistry: TypeAlias = dict[str, Argument]</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.DataBatch","title":"<code>DataBatch: TypeAlias = tuple[Tensor, Tensor]</code>  <code>module-attribute</code>","text":"<p>Type alias for data batch: (x, y).</p>"},{"location":"reference/pinn/core/#pinn.core.FieldsRegistry","title":"<code>FieldsRegistry: TypeAlias = dict[str, Field]</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.LOSS_KEY","title":"<code>LOSS_KEY = 'loss'</code>  <code>module-attribute</code>","text":"<p>Key used for logging the total loss.</p>"},{"location":"reference/pinn/core/#pinn.core.ParamsRegistry","title":"<code>ParamsRegistry: TypeAlias = dict[str, Parameter]</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Predictions","title":"<code>Predictions: TypeAlias = tuple[DataBatch, dict[str, Tensor], dict[str, Tensor] | None]</code>  <code>module-attribute</code>","text":"<p>Type alias for model predictions: (input_batch, predictions_dictionary, true_values_dictionary)  where predictions_dictionary is a dictionary of {[field_name | param_name]: prediction} and where true_values_dictionary is a dictionary of {[field_name | param_name]: true_value}. If no validation source is configured, true_values_dictionary is None.</p>"},{"location":"reference/pinn/core/#pinn.core.ResolvedValidation","title":"<code>ResolvedValidation: TypeAlias = dict[str, Callable[[Tensor], Tensor]]</code>  <code>module-attribute</code>","text":"<p>Validation registry after ColumnRef entries have been resolved to callables.</p>"},{"location":"reference/pinn/core/#pinn.core.TrainingBatch","title":"<code>TrainingBatch: TypeAlias = tuple[DataBatch, Tensor]</code>  <code>module-attribute</code>","text":"<p>Training batch tuple: ((x_data, y_data), x_coll).</p>"},{"location":"reference/pinn/core/#pinn.core.ValidationRegistry","title":"<code>ValidationRegistry: TypeAlias = dict[str, ValidationSource]</code>  <code>module-attribute</code>","text":"<p>Registry mapping parameter names to their validation sources.</p> Example <p>validation: ValidationRegistry = { ...     \"beta\": lambda x: torch.sin(x),  # Pure function ...     \"gamma\": ColumnRef(column=\"gamma_true\"),  # From data ...     \"delta\": None,  # No validation ... }</p>"},{"location":"reference/pinn/core/#pinn.core.ValidationSource","title":"<code>ValidationSource: TypeAlias = Callable[[Tensor], Tensor] | ColumnRef | None</code>  <code>module-attribute</code>","text":"<p>A source for ground truth values. Can be: - A callable that takes x coordinates and returns true values - A ColumnRef that references a column in loaded data - None if no validation is needed for this parameter</p>"},{"location":"reference/pinn/core/#pinn.core.__all__","title":"<code>__all__ = ['LOSS_KEY', 'Activations', 'ArgsRegistry', 'Argument', 'ColumnRef', 'Constraint', 'DataBatch', 'DataCallback', 'Domain1D', 'EarlyStoppingConfig', 'Field', 'FieldsRegistry', 'GenerationConfig', 'InferredContext', 'IngestionConfig', 'LogFn', 'MLPConfig', 'PINNDataModule', 'PINNDataset', 'PINNHyperparameters', 'Parameter', 'ParamsRegistry', 'Predictions', 'Problem', 'ResolvedValidation', 'SMMAStoppingConfig', 'ScalarConfig', 'SchedulerConfig', 'TrainingBatch', 'TrainingDataConfig', 'ValidationRegistry', 'ValidationSource', 'get_activation', 'resolve_validation']</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Argument","title":"<code>Argument</code>","text":"<p>Represents an argument that can be passed to an ODE/PDE function. Can be a fixed float value or a callable function.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float | Callable[[Tensor], Tensor]</code> <p>The value (float) or function (callable).</p> required Source code in <code>src/pinn/core/nn.py</code> <pre><code>class Argument:\n    \"\"\"\n    Represents an argument that can be passed to an ODE/PDE function.\n    Can be a fixed float value or a callable function.\n\n    Args:\n        value: The value (float) or function (callable).\n    \"\"\"\n\n    def __init__(self, value: float | Callable[[Tensor], Tensor]):\n        self._value = value\n\n    def __call__(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Evaluate the argument.\n\n        Args:\n            x: Input tensor (context).\n\n        Returns:\n            The value of the argument, broadcasted if necessary.\n        \"\"\"\n        if callable(self._value):\n            return self._value(x)\n        else:\n            return torch.tensor(self._value, device=x.device)\n\n    @override\n    def __repr__(self) -&gt; str:\n        return f\"Argument(value={self._value})\"\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Argument.__call__","title":"<code>__call__(x: Tensor) -&gt; Tensor</code>","text":"<p>Evaluate the argument.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor (context).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The value of the argument, broadcasted if necessary.</p> Source code in <code>src/pinn/core/nn.py</code> <pre><code>def __call__(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Evaluate the argument.\n\n    Args:\n        x: Input tensor (context).\n\n    Returns:\n        The value of the argument, broadcasted if necessary.\n    \"\"\"\n    if callable(self._value):\n        return self._value(x)\n    else:\n        return torch.tensor(self._value, device=x.device)\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Argument.__init__","title":"<code>__init__(value: float | Callable[[Tensor], Tensor])</code>","text":"Source code in <code>src/pinn/core/nn.py</code> <pre><code>def __init__(self, value: float | Callable[[Tensor], Tensor]):\n    self._value = value\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Argument.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"Source code in <code>src/pinn/core/nn.py</code> <pre><code>@override\ndef __repr__(self) -&gt; str:\n    return f\"Argument(value={self._value})\"\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.ColumnRef","title":"<code>ColumnRef</code>  <code>dataclass</code>","text":"<p>Reference to a column in loaded data for ground truth comparison.</p> <p>This allows practitioners to specify validation data by column name without writing custom functions. The column is resolved lazily when data is loaded.</p> <p>Attributes:</p> Name Type Description <code>column</code> <code>str</code> <p>Name of the column in the loaded DataFrame.</p> <code>transform</code> <code>Callable[[Tensor], Tensor] | None</code> <p>Optional transformation to apply to the column values.</p> Example <p>validation = { ...     \"beta\": ColumnRef(column=\"Rt\", transform=lambda rt: rt * delta), ... }</p> Source code in <code>src/pinn/core/validation.py</code> <pre><code>@dataclass\nclass ColumnRef:\n    \"\"\"\n    Reference to a column in loaded data for ground truth comparison.\n\n    This allows practitioners to specify validation data by column name\n    without writing custom functions. The column is resolved lazily when\n    data is loaded.\n\n    Attributes:\n        column: Name of the column in the loaded DataFrame.\n        transform: Optional transformation to apply to the column values.\n\n    Example:\n        &gt;&gt;&gt; validation = {\n        ...     \"beta\": ColumnRef(column=\"Rt\", transform=lambda rt: rt * delta),\n        ... }\n    \"\"\"\n\n    column: str\n    transform: Callable[[Tensor], Tensor] | None = None\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.ColumnRef.column","title":"<code>column: str</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.ColumnRef.transform","title":"<code>transform: Callable[[Tensor], Tensor] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.ColumnRef.__init__","title":"<code>__init__(column: str, transform: Callable[[Tensor], Tensor] | None = None) -&gt; None</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Constraint","title":"<code>Constraint</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for a constraint (loss term) in the PINN. Returns a loss value for the given batch.</p> Source code in <code>src/pinn/core/problem.py</code> <pre><code>class Constraint(ABC):\n    \"\"\"\n    Abstract base class for a constraint (loss term) in the PINN.\n    Returns a loss value for the given batch.\n    \"\"\"\n\n    def inject_context(self, context: InferredContext) -&gt; None:\n        \"\"\"\n        Inject the context into the constraint. This can be used by the constraint to access the\n        data used to compute the loss.\n\n        Args:\n            context: The context to inject.\n        \"\"\"\n        return None\n\n    @abstractmethod\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        \"\"\"\n        Calculate the loss for this constraint.\n\n        Args:\n            batch: The current batch of data/collocation points.\n            criterion: The loss function (e.g. MSE).\n            log: Optional logging function.\n\n        Returns:\n            The calculated loss tensor.\n        \"\"\"\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Constraint.inject_context","title":"<code>inject_context(context: InferredContext) -&gt; None</code>","text":"<p>Inject the context into the constraint. This can be used by the constraint to access the data used to compute the loss.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>InferredContext</code> <p>The context to inject.</p> required Source code in <code>src/pinn/core/problem.py</code> <pre><code>def inject_context(self, context: InferredContext) -&gt; None:\n    \"\"\"\n    Inject the context into the constraint. This can be used by the constraint to access the\n    data used to compute the loss.\n\n    Args:\n        context: The context to inject.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Constraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>  <code>abstractmethod</code>","text":"<p>Calculate the loss for this constraint.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>TrainingBatch</code> <p>The current batch of data/collocation points.</p> required <code>criterion</code> <code>Module</code> <p>The loss function (e.g. MSE).</p> required <code>log</code> <code>LogFn | None</code> <p>Optional logging function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The calculated loss tensor.</p> Source code in <code>src/pinn/core/problem.py</code> <pre><code>@abstractmethod\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    \"\"\"\n    Calculate the loss for this constraint.\n\n    Args:\n        batch: The current batch of data/collocation points.\n        criterion: The loss function (e.g. MSE).\n        log: Optional logging function.\n\n    Returns:\n        The calculated loss tensor.\n    \"\"\"\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.DataCallback","title":"<code>DataCallback</code>","text":"<p>Abstract base class for building new data callbacks.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>class DataCallback:\n    \"\"\"Abstract base class for building new data callbacks.\"\"\"\n\n    def transform_data(self, data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]:\n        \"\"\"Transform the data and collocation points.\"\"\"\n        return data, coll\n\n    def on_after_setup(self, dm: \"PINNDataModule\") -&gt; None:\n        \"\"\"Called after setup is complete.\"\"\"\n        return None\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.DataCallback.on_after_setup","title":"<code>on_after_setup(dm: PINNDataModule) -&gt; None</code>","text":"<p>Called after setup is complete.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>def on_after_setup(self, dm: \"PINNDataModule\") -&gt; None:\n    \"\"\"Called after setup is complete.\"\"\"\n    return None\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.DataCallback.transform_data","title":"<code>transform_data(data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]</code>","text":"<p>Transform the data and collocation points.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>def transform_data(self, data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]:\n    \"\"\"Transform the data and collocation points.\"\"\"\n    return data, coll\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Domain1D","title":"<code>Domain1D</code>  <code>dataclass</code>","text":"<p>One-dimensional domain: time interval [x0, x1] with step size dx.</p> <p>Attributes:</p> Name Type Description <code>x0</code> <code>float</code> <p>Start of the interval.</p> <code>x1</code> <code>float</code> <p>End of the interval.</p> <code>dx</code> <code>float</code> <p>Step size for discretization (if applicable).</p> Source code in <code>src/pinn/core/nn.py</code> <pre><code>@dataclass\nclass Domain1D:\n    \"\"\"\n    One-dimensional domain: time interval [x0, x1] with step size dx.\n\n    Attributes:\n        x0: Start of the interval.\n        x1: End of the interval.\n        dx: Step size for discretization (if applicable).\n    \"\"\"\n\n    x0: float\n    x1: float\n    dx: float\n\n    @classmethod\n    def from_x(cls, x: Tensor) -&gt; Domain1D:\n        \"\"\"Create a domain from x coordinates.\"\"\"\n        assert x.shape[0] &gt; 1, \"At least two points are required to infer the domain.\"\n\n        x0, x1 = x[0].item(), x[-1].item()\n        dx = (x[1] - x[0]).item()\n\n        return cls(x0=x0, x1=x1, dx=dx)\n\n    @override\n    def __repr__(self) -&gt; str:\n        return f\"Domain1D(x0={self.x0}, x1={self.x1}, dx={self.dx})\"\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Domain1D.dx","title":"<code>dx: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Domain1D.x0","title":"<code>x0: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Domain1D.x1","title":"<code>x1: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Domain1D.__init__","title":"<code>__init__(x0: float, x1: float, dx: float) -&gt; None</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Domain1D.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"Source code in <code>src/pinn/core/nn.py</code> <pre><code>@override\ndef __repr__(self) -&gt; str:\n    return f\"Domain1D(x0={self.x0}, x1={self.x1}, dx={self.dx})\"\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Domain1D.from_x","title":"<code>from_x(x: Tensor) -&gt; Domain1D</code>  <code>classmethod</code>","text":"<p>Create a domain from x coordinates.</p> Source code in <code>src/pinn/core/nn.py</code> <pre><code>@classmethod\ndef from_x(cls, x: Tensor) -&gt; Domain1D:\n    \"\"\"Create a domain from x coordinates.\"\"\"\n    assert x.shape[0] &gt; 1, \"At least two points are required to infer the domain.\"\n\n    x0, x1 = x[0].item(), x[-1].item()\n    dx = (x[1] - x[0]).item()\n\n    return cls(x0=x0, x1=x1, dx=dx)\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.EarlyStoppingConfig","title":"<code>EarlyStoppingConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Early Stopping callback.</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass EarlyStoppingConfig:\n    \"\"\"\n    Configuration for Early Stopping callback.\n    \"\"\"\n\n    patience: int\n    mode: Literal[\"min\", \"max\"]\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.EarlyStoppingConfig.mode","title":"<code>mode: Literal['min', 'max']</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.EarlyStoppingConfig.patience","title":"<code>patience: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.EarlyStoppingConfig.__init__","title":"<code>__init__(*, patience: int, mode: Literal['min', 'max']) -&gt; None</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Field","title":"<code>Field</code>","text":"<p>               Bases: <code>Module</code></p> <p>A neural field mapping coordinates -&gt; vector of state variables. Example (ODE): t -&gt; [S, I, R].</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MLPConfig</code> <p>Configuration for the MLP backing this field.</p> required Source code in <code>src/pinn/core/nn.py</code> <pre><code>class Field(nn.Module):\n    \"\"\"\n    A neural field mapping coordinates -&gt; vector of state variables.\n    Example (ODE): t -&gt; [S, I, R].\n\n    Args:\n        config: Configuration for the MLP backing this field.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: MLPConfig,\n    ):\n        super().__init__()\n        self.encode = config.encode\n        dims = [config.in_dim] + config.hidden_layers + [config.out_dim]\n        act = get_activation(config.activation)\n\n        layers: list[nn.Module] = []\n        for i in range(len(dims) - 1):\n            layers.append(nn.Linear(dims[i], dims[i + 1]))\n            if i &lt; len(dims) - 2:\n                layers.append(act)\n\n        if config.output_activation is not None:\n            out_act = get_activation(config.output_activation)\n            layers.append(out_act)\n\n        self.net = nn.Sequential(*layers)\n        self.apply(self._init)\n\n    @staticmethod\n    def _init(m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            nn.init.zeros_(m.bias)\n\n    @override\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Forward pass of the field.\n\n        Args:\n            x: Input coordinates (e.g. time, space).\n\n        Returns:\n            The values of the field at input coordinates.\n        \"\"\"\n        if self.encode is not None:\n            x = self.encode(x)\n        return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Field.encode","title":"<code>encode = config.encode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Field.net","title":"<code>net = nn.Sequential(*layers)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Field.__init__","title":"<code>__init__(config: MLPConfig)</code>","text":"Source code in <code>src/pinn/core/nn.py</code> <pre><code>def __init__(\n    self,\n    config: MLPConfig,\n):\n    super().__init__()\n    self.encode = config.encode\n    dims = [config.in_dim] + config.hidden_layers + [config.out_dim]\n    act = get_activation(config.activation)\n\n    layers: list[nn.Module] = []\n    for i in range(len(dims) - 1):\n        layers.append(nn.Linear(dims[i], dims[i + 1]))\n        if i &lt; len(dims) - 2:\n            layers.append(act)\n\n    if config.output_activation is not None:\n        out_act = get_activation(config.output_activation)\n        layers.append(out_act)\n\n    self.net = nn.Sequential(*layers)\n    self.apply(self._init)\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Field.forward","title":"<code>forward(x: Tensor) -&gt; Tensor</code>","text":"<p>Forward pass of the field.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input coordinates (e.g. time, space).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The values of the field at input coordinates.</p> Source code in <code>src/pinn/core/nn.py</code> <pre><code>@override\ndef forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Forward pass of the field.\n\n    Args:\n        x: Input coordinates (e.g. time, space).\n\n    Returns:\n        The values of the field at input coordinates.\n    \"\"\"\n    if self.encode is not None:\n        x = self.encode(x)\n    return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.GenerationConfig","title":"<code>GenerationConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrainingDataConfig</code></p> <p>Configuration for data generation.</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass GenerationConfig(TrainingDataConfig):\n    \"\"\"\n    Configuration for data generation.\n    \"\"\"\n\n    x: Tensor\n    noise_level: float\n    args_to_train: ArgsRegistry\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.GenerationConfig.args_to_train","title":"<code>args_to_train: ArgsRegistry</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.GenerationConfig.noise_level","title":"<code>noise_level: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.GenerationConfig.x","title":"<code>x: Tensor</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.GenerationConfig.__init__","title":"<code>__init__(*, batch_size: int, data_ratio: int | float, collocations: int, x: Tensor, noise_level: float, args_to_train: ArgsRegistry) -&gt; None</code>","text":""},{"location":"reference/pinn/core/#pinn.core.InferredContext","title":"<code>InferredContext</code>  <code>dataclass</code>","text":"<p>Runtime context inferred from training data.</p> <p>This holds the data that is either explicitly provided in props or inferred from training data.</p> Source code in <code>src/pinn/core/context.py</code> <pre><code>@dataclass\nclass InferredContext:\n    \"\"\"\n    Runtime context inferred from training data.\n\n    This holds the data that is either explicitly provided in props or inferred from training data.\n    \"\"\"\n\n    def __init__(\n        self,\n        x: Tensor,\n        y: Tensor,\n        validation: ResolvedValidation,\n    ):\n        \"\"\"\n        Infer context from either generated or loaded data.\n\n        Args:\n            x: x coordinates.\n            y: observations.\n            validation: Resolved validation dictionary.\n        \"\"\"\n\n        self.domain = Domain1D.from_x(x)\n        self.validation = validation\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.InferredContext.domain","title":"<code>domain = Domain1D.from_x(x)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.InferredContext.validation","title":"<code>validation = validation</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.InferredContext.__init__","title":"<code>__init__(x: Tensor, y: Tensor, validation: ResolvedValidation)</code>","text":"<p>Infer context from either generated or loaded data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>x coordinates.</p> required <code>y</code> <code>Tensor</code> <p>observations.</p> required <code>validation</code> <code>ResolvedValidation</code> <p>Resolved validation dictionary.</p> required Source code in <code>src/pinn/core/context.py</code> <pre><code>def __init__(\n    self,\n    x: Tensor,\n    y: Tensor,\n    validation: ResolvedValidation,\n):\n    \"\"\"\n    Infer context from either generated or loaded data.\n\n    Args:\n        x: x coordinates.\n        y: observations.\n        validation: Resolved validation dictionary.\n    \"\"\"\n\n    self.domain = Domain1D.from_x(x)\n    self.validation = validation\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.IngestionConfig","title":"<code>IngestionConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrainingDataConfig</code></p> <p>Configuration for data ingestion from files. If x_column is None, the data is assumed to be evenly spaced.</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass IngestionConfig(TrainingDataConfig):\n    \"\"\"\n    Configuration for data ingestion from files.\n    If x_column is None, the data is assumed to be evenly spaced.\n    \"\"\"\n\n    df_path: Path\n    x_transform: Callable[[Any], Any] | None = None\n    x_column: str | None = None\n    y_columns: list[str]\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.IngestionConfig.df_path","title":"<code>df_path: Path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.IngestionConfig.x_column","title":"<code>x_column: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.IngestionConfig.x_transform","title":"<code>x_transform: Callable[[Any], Any] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.IngestionConfig.y_columns","title":"<code>y_columns: list[str]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.IngestionConfig.__init__","title":"<code>__init__(*, batch_size: int, data_ratio: int | float, collocations: int, df_path: Path, x_transform: Callable[[Any], Any] | None = None, x_column: str | None = None, y_columns: list[str]) -&gt; None</code>","text":""},{"location":"reference/pinn/core/#pinn.core.LogFn","title":"<code>LogFn</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>A function that logs a value to a dictionary.</p> Source code in <code>src/pinn/core/types.py</code> <pre><code>class LogFn(Protocol):\n    \"\"\"\n    A function that logs a value to a dictionary.\n    \"\"\"\n\n    def __call__(self, name: str, value: Tensor, progress_bar: bool = False) -&gt; None:\n        \"\"\"\n        Log a value.\n\n        Args:\n            name: The name to log the value under.\n            value: The value to log.\n            progress_bar: Whether the value should be logged to the progress bar.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.LogFn.__call__","title":"<code>__call__(name: str, value: Tensor, progress_bar: bool = False) -&gt; None</code>","text":"<p>Log a value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to log the value under.</p> required <code>value</code> <code>Tensor</code> <p>The value to log.</p> required <code>progress_bar</code> <code>bool</code> <p>Whether the value should be logged to the progress bar.</p> <code>False</code> Source code in <code>src/pinn/core/types.py</code> <pre><code>def __call__(self, name: str, value: Tensor, progress_bar: bool = False) -&gt; None:\n    \"\"\"\n    Log a value.\n\n    Args:\n        name: The name to log the value under.\n        value: The value to log.\n        progress_bar: Whether the value should be logged to the progress bar.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.MLPConfig","title":"<code>MLPConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a Multi-Layer Perceptron (MLP).</p> <p>Attributes:</p> Name Type Description <code>in_dim</code> <code>int</code> <p>Dimension of input layer.</p> <code>out_dim</code> <code>int</code> <p>Dimension of output layer.</p> <code>hidden_layers</code> <code>list[int]</code> <p>List of dimensions for hidden layers.</p> <code>activation</code> <code>Activations</code> <p>Activation function to use between layers.</p> <code>output_activation</code> <code>Activations | None</code> <p>Optional activation function for the output layer.</p> <code>encode</code> <code>Callable[[Tensor], Tensor] | None</code> <p>Optional function to encode inputs before passing to MLP.</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass MLPConfig:\n    \"\"\"\n    Configuration for a Multi-Layer Perceptron (MLP).\n\n    Attributes:\n        in_dim: Dimension of input layer.\n        out_dim: Dimension of output layer.\n        hidden_layers: List of dimensions for hidden layers.\n        activation: Activation function to use between layers.\n        output_activation: Optional activation function for the output layer.\n        encode: Optional function to encode inputs before passing to MLP.\n    \"\"\"\n\n    in_dim: int\n    out_dim: int\n    hidden_layers: list[int]\n    activation: Activations\n    output_activation: Activations | None = None\n    encode: Callable[[Tensor], Tensor] | None = None\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.MLPConfig.activation","title":"<code>activation: Activations</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.MLPConfig.encode","title":"<code>encode: Callable[[Tensor], Tensor] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.MLPConfig.hidden_layers","title":"<code>hidden_layers: list[int]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.MLPConfig.in_dim","title":"<code>in_dim: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.MLPConfig.out_dim","title":"<code>out_dim: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.MLPConfig.output_activation","title":"<code>output_activation: Activations | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.MLPConfig.__init__","title":"<code>__init__(*, in_dim: int, out_dim: int, hidden_layers: list[int], activation: Activations, output_activation: Activations | None = None, encode: Callable[[Tensor], Tensor] | None = None) -&gt; None</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNDataModule","title":"<code>PINNDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code>, <code>ABC</code></p> <p>LightningDataModule for PINNs. Manages data and collocation datasets and creates the combined PINNDataset.</p> <p>Attributes:</p> Name Type Description <code>data_ds</code> <p>Dataset containing observed data.</p> <code>coll_ds</code> <p>Dataset containing collocation points.</p> <code>pinn_ds</code> <p>Combined PINNDataset for training.</p> <code>callbacks</code> <code>list[DataCallback]</code> <p>Sequence of DataCallback callbacks applied after data loading.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>class PINNDataModule(pl.LightningDataModule, ABC):\n    \"\"\"\n    LightningDataModule for PINNs.\n    Manages data and collocation datasets and creates the combined PINNDataset.\n\n    Attributes:\n        data_ds: Dataset containing observed data.\n        coll_ds: Dataset containing collocation points.\n        pinn_ds: Combined PINNDataset for training.\n        callbacks: Sequence of DataCallback callbacks applied after data loading.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ) -&gt; None:\n        super().__init__()\n        self.hp = hp\n        self.callbacks: list[DataCallback] = list(callbacks) if callbacks else []\n\n        self._unresolved_validation = validation or {}\n\n    def load_data(self, config: IngestionConfig) -&gt; DataBatch:\n        \"\"\"Load raw data from IngestionConfig.\"\"\"\n        df = pd.read_csv(config.df_path)\n\n        if config.x_column is not None:\n            x_values = df[config.x_column].values\n\n            if config.x_transform is not None:\n                x_values = config.x_transform(x_values)\n\n            x = torch.tensor(x_values, dtype=torch.float32)\n        else:\n            x = torch.arange(len(df), dtype=torch.float32)\n\n        y = torch.tensor(df[config.y_columns].values, dtype=torch.float32)\n\n        if y.shape[1] != 1:\n            y = y.unsqueeze(-1)\n\n        return x.unsqueeze(-1), y\n\n    @abstractmethod\n    def gen_data(self, config: GenerationConfig) -&gt; DataBatch:\n        \"\"\"Generate synthetic data from GenerationConfig.\"\"\"\n\n    @abstractmethod\n    def gen_coll(self, domain: Domain1D) -&gt; Tensor:\n        \"\"\"Generate collocation points.\"\"\"\n\n    @override\n    def setup(self, stage: str | None = None) -&gt; None:\n        \"\"\"\n        Load raw data from IngestionConfig, or generate synthetic data from GenerationConfig.\n        Apply registered callbacks, create InferredContext and datasets.\n        \"\"\"\n        config = self.hp.training_data\n\n        self.validation = resolve_validation(\n            self._unresolved_validation,\n            config.df_path if isinstance(config, IngestionConfig) else None,\n        )\n\n        self.data = (\n            self.load_data(config)\n            if isinstance(config, IngestionConfig)\n            else self.gen_data(config)\n        )\n\n        self.coll = self.gen_coll(\n            Domain1D.from_x(self.data[0]),\n        )\n\n        for callback in self.callbacks:\n            self.data, self.coll = callback.transform_data(self.data, self.coll)\n\n        x_data, y_data = self.data\n\n        assert x_data.shape[0] == y_data.shape[0], \"Size mismatch between x and y.\"\n        assert x_data.ndim == 2, \"x shape differs than (n, 1).\"\n        assert x_data.shape[1] == 1, \"x shape differs than (n, 1).\"\n        assert y_data.ndim &gt; 1, \"y shape cannot be (n).\"\n        assert y_data.shape[-1] == 1, \"y shape differs than (n, 1).\"\n        assert self.coll.ndim == 2, \"coll shape differs than (m, 1).\"\n        assert self.coll.shape[1] == 1, \"coll shape differs than (m, 1).\"\n\n        self._data_size = x_data.shape[0]\n\n        self._context = InferredContext(\n            x_data,\n            y_data,\n            self.validation,\n        )\n\n        self.pinn_ds = PINNDataset(\n            x_data,\n            y_data,\n            self.coll,\n            config.batch_size,\n            config.data_ratio,\n        )\n\n        self.predict_ds = TensorDataset(\n            x_data,\n            y_data,\n        )\n\n        for callback in self.callbacks:\n            callback.on_after_setup(self)\n\n    @override\n    def train_dataloader(self) -&gt; DataLoader[TrainingBatch]:\n        \"\"\"\n        Returns the training dataloader using PINNDataset.\n        \"\"\"\n        return DataLoader[TrainingBatch](\n            self.pinn_ds,\n            batch_size=None,  # handled internally\n            num_workers=7,\n            persistent_workers=True,\n        )\n\n    @override\n    def predict_dataloader(self) -&gt; DataLoader[PredictionBatch]:\n        \"\"\"\n        Returns the prediction dataloader using only the data dataset.\n        \"\"\"\n        return DataLoader[PredictionBatch](\n            cast(Dataset[PredictionBatch], self.predict_ds),\n            batch_size=self._data_size,\n            num_workers=7,\n            persistent_workers=True,\n        )\n\n    @property\n    def context(self) -&gt; InferredContext:\n        assert self._context is not None, (\n            \"Context does not exist. `setup` stage not completed yet.\"\n        )\n        return self._context\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.PINNDataModule.callbacks","title":"<code>callbacks: list[DataCallback] = list(callbacks) if callbacks else []</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNDataModule.context","title":"<code>context: InferredContext</code>  <code>property</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNDataModule.hp","title":"<code>hp = hp</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None) -&gt; None</code>","text":"Source code in <code>src/pinn/core/dataset.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n) -&gt; None:\n    super().__init__()\n    self.hp = hp\n    self.callbacks: list[DataCallback] = list(callbacks) if callbacks else []\n\n    self._unresolved_validation = validation or {}\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.PINNDataModule.gen_coll","title":"<code>gen_coll(domain: Domain1D) -&gt; Tensor</code>  <code>abstractmethod</code>","text":"<p>Generate collocation points.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>@abstractmethod\ndef gen_coll(self, domain: Domain1D) -&gt; Tensor:\n    \"\"\"Generate collocation points.\"\"\"\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.PINNDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; DataBatch</code>  <code>abstractmethod</code>","text":"<p>Generate synthetic data from GenerationConfig.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>@abstractmethod\ndef gen_data(self, config: GenerationConfig) -&gt; DataBatch:\n    \"\"\"Generate synthetic data from GenerationConfig.\"\"\"\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.PINNDataModule.load_data","title":"<code>load_data(config: IngestionConfig) -&gt; DataBatch</code>","text":"<p>Load raw data from IngestionConfig.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>def load_data(self, config: IngestionConfig) -&gt; DataBatch:\n    \"\"\"Load raw data from IngestionConfig.\"\"\"\n    df = pd.read_csv(config.df_path)\n\n    if config.x_column is not None:\n        x_values = df[config.x_column].values\n\n        if config.x_transform is not None:\n            x_values = config.x_transform(x_values)\n\n        x = torch.tensor(x_values, dtype=torch.float32)\n    else:\n        x = torch.arange(len(df), dtype=torch.float32)\n\n    y = torch.tensor(df[config.y_columns].values, dtype=torch.float32)\n\n    if y.shape[1] != 1:\n        y = y.unsqueeze(-1)\n\n    return x.unsqueeze(-1), y\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.PINNDataModule.predict_dataloader","title":"<code>predict_dataloader() -&gt; DataLoader[PredictionBatch]</code>","text":"<p>Returns the prediction dataloader using only the data dataset.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>@override\ndef predict_dataloader(self) -&gt; DataLoader[PredictionBatch]:\n    \"\"\"\n    Returns the prediction dataloader using only the data dataset.\n    \"\"\"\n    return DataLoader[PredictionBatch](\n        cast(Dataset[PredictionBatch], self.predict_ds),\n        batch_size=self._data_size,\n        num_workers=7,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.PINNDataModule.setup","title":"<code>setup(stage: str | None = None) -&gt; None</code>","text":"<p>Load raw data from IngestionConfig, or generate synthetic data from GenerationConfig. Apply registered callbacks, create InferredContext and datasets.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>@override\ndef setup(self, stage: str | None = None) -&gt; None:\n    \"\"\"\n    Load raw data from IngestionConfig, or generate synthetic data from GenerationConfig.\n    Apply registered callbacks, create InferredContext and datasets.\n    \"\"\"\n    config = self.hp.training_data\n\n    self.validation = resolve_validation(\n        self._unresolved_validation,\n        config.df_path if isinstance(config, IngestionConfig) else None,\n    )\n\n    self.data = (\n        self.load_data(config)\n        if isinstance(config, IngestionConfig)\n        else self.gen_data(config)\n    )\n\n    self.coll = self.gen_coll(\n        Domain1D.from_x(self.data[0]),\n    )\n\n    for callback in self.callbacks:\n        self.data, self.coll = callback.transform_data(self.data, self.coll)\n\n    x_data, y_data = self.data\n\n    assert x_data.shape[0] == y_data.shape[0], \"Size mismatch between x and y.\"\n    assert x_data.ndim == 2, \"x shape differs than (n, 1).\"\n    assert x_data.shape[1] == 1, \"x shape differs than (n, 1).\"\n    assert y_data.ndim &gt; 1, \"y shape cannot be (n).\"\n    assert y_data.shape[-1] == 1, \"y shape differs than (n, 1).\"\n    assert self.coll.ndim == 2, \"coll shape differs than (m, 1).\"\n    assert self.coll.shape[1] == 1, \"coll shape differs than (m, 1).\"\n\n    self._data_size = x_data.shape[0]\n\n    self._context = InferredContext(\n        x_data,\n        y_data,\n        self.validation,\n    )\n\n    self.pinn_ds = PINNDataset(\n        x_data,\n        y_data,\n        self.coll,\n        config.batch_size,\n        config.data_ratio,\n    )\n\n    self.predict_ds = TensorDataset(\n        x_data,\n        y_data,\n    )\n\n    for callback in self.callbacks:\n        callback.on_after_setup(self)\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.PINNDataModule.train_dataloader","title":"<code>train_dataloader() -&gt; DataLoader[TrainingBatch]</code>","text":"<p>Returns the training dataloader using PINNDataset.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>@override\ndef train_dataloader(self) -&gt; DataLoader[TrainingBatch]:\n    \"\"\"\n    Returns the training dataloader using PINNDataset.\n    \"\"\"\n    return DataLoader[TrainingBatch](\n        self.pinn_ds,\n        batch_size=None,  # handled internally\n        num_workers=7,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.PINNDataset","title":"<code>PINNDataset</code>","text":"<p>               Bases: <code>Dataset[TrainingBatch]</code></p> <p>Dataset used for PINN training. Combines labeled data and collocation points per sample.  Given a data_ratio, the amount of data points <code>K</code> is determined either by applying <code>data_ratio * batch_size</code> if ratio is a float between 0 and 1 or by an absolute count if ratio is an integer. The remaining <code>C</code> points are used for collocation.  The data points are sampled without replacement per epoch i.e. cycles through all data points and at the last batch, wraps around to the first indices to ensure batch size. The collocation points are sampled with replacement from the pool. The dataset produces a batch of shape ((t_data[K,1], y_data[K,1]), t_coll[C,1]).</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>Tensor</code> <p>Data point x coordinates (time values).</p> required <code>y_data</code> <code>Tensor</code> <p>Data point y values (observations).</p> required <code>x_coll</code> <code>Tensor</code> <p>Collocation point x coordinates.</p> required <code>batch_size</code> <code>int</code> <p>Size of the batch.</p> required <code>data_ratio</code> <code>float | int</code> <p>Ratio of data points to collocation points, either as a ratio [0,1] or absolute count [0,batch_size].</p> required Source code in <code>src/pinn/core/dataset.py</code> <pre><code>class PINNDataset(Dataset[TrainingBatch]):\n    \"\"\"\n    Dataset used for PINN training. Combines labeled data and collocation points\n    per sample.  Given a data_ratio, the amount of data points `K` is determined\n    either by applying `data_ratio * batch_size` if ratio is a float between 0\n    and 1 or by an absolute count if ratio is an integer. The remaining `C`\n    points are used for collocation.  The data points are sampled without\n    replacement per epoch i.e. cycles through all data points and at the last\n    batch, wraps around to the first indices to ensure batch size. The collocation\n    points are sampled with replacement from the pool.\n    The dataset produces a batch of shape ((t_data[K,1], y_data[K,1]), t_coll[C,1]).\n\n    Args:\n        x_data: Data point x coordinates (time values).\n        y_data: Data point y values (observations).\n        x_coll: Collocation point x coordinates.\n        batch_size: Size of the batch.\n        data_ratio: Ratio of data points to collocation points, either as a ratio [0,1] or absolute\n            count [0,batch_size].\n    \"\"\"\n\n    def __init__(\n        self,\n        x_data: Tensor,\n        y_data: Tensor,\n        x_coll: Tensor,\n        batch_size: int,\n        data_ratio: float | int,\n    ):\n        super().__init__()\n        assert batch_size &gt; 0\n\n        if isinstance(data_ratio, float):\n            assert 0.0 &lt;= data_ratio &lt;= 1.0\n            self.K = round(data_ratio * batch_size)\n        else:\n            assert 0 &lt;= data_ratio &lt;= batch_size\n            self.K = data_ratio\n\n        self.x_data = x_data\n        self.y_data = y_data\n        self.x_coll = x_coll\n\n        self.batch_size = batch_size\n        self.C = batch_size - self.K\n\n        self.total_data = x_data.shape[0]\n        self.total_coll = x_coll.shape[0]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Number of steps per epoch to see all data points once. Ceiling division.\"\"\"\n        return (self.total_data + self.K - 1) // self.K\n\n    @override\n    def __getitem__(self, idx: int) -&gt; TrainingBatch:\n        \"\"\"Return one sample containing K data points and C collocation points.\"\"\"\n        data_idx = self._get_data_indices(idx)\n        coll_idx = self._get_coll_indices(idx)\n\n        x_data = self.x_data[data_idx]\n        y_data = self.y_data[data_idx]\n        x_coll = self.x_coll[coll_idx]\n\n        return ((x_data, y_data), x_coll)\n\n    def _get_data_indices(self, idx: int) -&gt; Tensor:\n        \"\"\"Get data indices for this step without replacement.\n        When getting the last batch, wrap around to the first indices to ensure batch size.\n        \"\"\"\n        if self.total_data == 0:\n            return torch.empty(0, 1)\n\n        start = idx * self.K\n        indices = [(start + i) % self.total_data for i in range(self.K)]\n        return torch.tensor(indices)\n\n    def _get_coll_indices(self, idx: int) -&gt; Tensor:\n        \"\"\"Get collocation indices for this step with replacement.\"\"\"\n        if self.total_coll == 0:\n            return torch.empty(0, 1)\n\n        temp_gen = torch.Generator().manual_seed(idx)\n        return torch.randint(0, self.total_coll, (self.C,), generator=temp_gen)\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.PINNDataset.C","title":"<code>C = batch_size - self.K</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNDataset.K","title":"<code>K = round(data_ratio * batch_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNDataset.batch_size","title":"<code>batch_size = batch_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNDataset.total_coll","title":"<code>total_coll = x_coll.shape[0]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNDataset.total_data","title":"<code>total_data = x_data.shape[0]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNDataset.x_coll","title":"<code>x_coll = x_coll</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNDataset.x_data","title":"<code>x_data = x_data</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNDataset.y_data","title":"<code>y_data = y_data</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNDataset.__getitem__","title":"<code>__getitem__(idx: int) -&gt; TrainingBatch</code>","text":"<p>Return one sample containing K data points and C collocation points.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>@override\ndef __getitem__(self, idx: int) -&gt; TrainingBatch:\n    \"\"\"Return one sample containing K data points and C collocation points.\"\"\"\n    data_idx = self._get_data_indices(idx)\n    coll_idx = self._get_coll_indices(idx)\n\n    x_data = self.x_data[data_idx]\n    y_data = self.y_data[data_idx]\n    x_coll = self.x_coll[coll_idx]\n\n    return ((x_data, y_data), x_coll)\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.PINNDataset.__init__","title":"<code>__init__(x_data: Tensor, y_data: Tensor, x_coll: Tensor, batch_size: int, data_ratio: float | int)</code>","text":"Source code in <code>src/pinn/core/dataset.py</code> <pre><code>def __init__(\n    self,\n    x_data: Tensor,\n    y_data: Tensor,\n    x_coll: Tensor,\n    batch_size: int,\n    data_ratio: float | int,\n):\n    super().__init__()\n    assert batch_size &gt; 0\n\n    if isinstance(data_ratio, float):\n        assert 0.0 &lt;= data_ratio &lt;= 1.0\n        self.K = round(data_ratio * batch_size)\n    else:\n        assert 0 &lt;= data_ratio &lt;= batch_size\n        self.K = data_ratio\n\n    self.x_data = x_data\n    self.y_data = y_data\n    self.x_coll = x_coll\n\n    self.batch_size = batch_size\n    self.C = batch_size - self.K\n\n    self.total_data = x_data.shape[0]\n    self.total_coll = x_coll.shape[0]\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.PINNDataset.__len__","title":"<code>__len__() -&gt; int</code>","text":"<p>Number of steps per epoch to see all data points once. Ceiling division.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Number of steps per epoch to see all data points once. Ceiling division.\"\"\"\n    return (self.total_data + self.K - 1) // self.K\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.PINNHyperparameters","title":"<code>PINNHyperparameters</code>  <code>dataclass</code>","text":"<p>Aggregated hyperparameters for the PINN model.</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass PINNHyperparameters:\n    \"\"\"\n    Aggregated hyperparameters for the PINN model.\n    \"\"\"\n\n    lr: float\n    training_data: IngestionConfig | GenerationConfig\n    fields_config: MLPConfig\n    params_config: MLPConfig | ScalarConfig\n    scheduler: SchedulerConfig | None = None\n    early_stopping: EarlyStoppingConfig | None = None\n    smma_stopping: SMMAStoppingConfig | None = None\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.PINNHyperparameters.early_stopping","title":"<code>early_stopping: EarlyStoppingConfig | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNHyperparameters.fields_config","title":"<code>fields_config: MLPConfig</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNHyperparameters.lr","title":"<code>lr: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNHyperparameters.params_config","title":"<code>params_config: MLPConfig | ScalarConfig</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNHyperparameters.scheduler","title":"<code>scheduler: SchedulerConfig | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNHyperparameters.smma_stopping","title":"<code>smma_stopping: SMMAStoppingConfig | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNHyperparameters.training_data","title":"<code>training_data: IngestionConfig | GenerationConfig</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.PINNHyperparameters.__init__","title":"<code>__init__(*, lr: float, training_data: IngestionConfig | GenerationConfig, fields_config: MLPConfig, params_config: MLPConfig | ScalarConfig, scheduler: SchedulerConfig | None = None, early_stopping: EarlyStoppingConfig | None = None, smma_stopping: SMMAStoppingConfig | None = None) -&gt; None</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Parameter","title":"<code>Parameter</code>","text":"<p>               Bases: <code>Module</code>, <code>Argument</code></p> <p>Learnable parameter. Supports scalar or function-valued parameter. For function-valued parameters (e.g. \u03b2(t)), uses a small MLP.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ScalarConfig | MLPConfig</code> <p>Configuration for the parameter (ScalarConfig or MLPConfig).</p> required Source code in <code>src/pinn/core/nn.py</code> <pre><code>class Parameter(nn.Module, Argument):\n    \"\"\"\n    Learnable parameter. Supports scalar or function-valued parameter.\n    For function-valued parameters (e.g. \u03b2(t)), uses a small MLP.\n\n    Args:\n        config: Configuration for the parameter (ScalarConfig or MLPConfig).\n    \"\"\"\n\n    def __init__(\n        self,\n        config: ScalarConfig | MLPConfig,\n    ):\n        super().__init__()\n        self.config = config\n        self._mode: Literal[\"scalar\", \"mlp\"]\n\n        if isinstance(config, ScalarConfig):\n            self._mode = \"scalar\"\n            self.value = nn.Parameter(torch.tensor(float(config.init_value), dtype=torch.float32))\n\n        else:  # isinstance(config, MLPConfig)\n            self._mode = \"mlp\"\n            dims = [config.in_dim] + config.hidden_layers + [config.out_dim]\n            act = get_activation(config.activation)\n\n            layers: list[nn.Module] = []\n            for i in range(len(dims) - 1):\n                layers.append(nn.Linear(dims[i], dims[i + 1]))\n                if i &lt; len(dims) - 2:\n                    layers.append(act)\n\n            if config.output_activation is not None:\n                out_act = get_activation(config.output_activation)\n                layers.append(out_act)\n\n            self.net = nn.Sequential(*layers)\n            self.apply(self._init)\n\n    @property\n    def mode(self) -&gt; Literal[\"scalar\", \"mlp\"]:\n        \"\"\"Mode of the parameter: 'scalar' or 'mlp'.\"\"\"\n        return self._mode\n\n    @staticmethod\n    def _init(m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            nn.init.zeros_(m.bias)\n\n    @override\n    def forward(self, x: Tensor | None = None) -&gt; Tensor:\n        \"\"\"\n        Get the value of the parameter.\n\n        Args:\n            x: Input tensor (required for 'mlp' mode).\n\n        Returns:\n            The parameter value.\n        \"\"\"\n        if self.mode == \"scalar\":\n            return self.value if x is None else self.value.expand_as(x)\n        else:\n            assert x is not None, \"Function-valued parameter requires input\"\n            return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Parameter.config","title":"<code>config = config</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Parameter.mode","title":"<code>mode: Literal['scalar', 'mlp']</code>  <code>property</code>","text":"<p>Mode of the parameter: 'scalar' or 'mlp'.</p>"},{"location":"reference/pinn/core/#pinn.core.Parameter.net","title":"<code>net = nn.Sequential(*layers)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Parameter.value","title":"<code>value = nn.Parameter(torch.tensor(float(config.init_value), dtype=(torch.float32)))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Parameter.__init__","title":"<code>__init__(config: ScalarConfig | MLPConfig)</code>","text":"Source code in <code>src/pinn/core/nn.py</code> <pre><code>def __init__(\n    self,\n    config: ScalarConfig | MLPConfig,\n):\n    super().__init__()\n    self.config = config\n    self._mode: Literal[\"scalar\", \"mlp\"]\n\n    if isinstance(config, ScalarConfig):\n        self._mode = \"scalar\"\n        self.value = nn.Parameter(torch.tensor(float(config.init_value), dtype=torch.float32))\n\n    else:  # isinstance(config, MLPConfig)\n        self._mode = \"mlp\"\n        dims = [config.in_dim] + config.hidden_layers + [config.out_dim]\n        act = get_activation(config.activation)\n\n        layers: list[nn.Module] = []\n        for i in range(len(dims) - 1):\n            layers.append(nn.Linear(dims[i], dims[i + 1]))\n            if i &lt; len(dims) - 2:\n                layers.append(act)\n\n        if config.output_activation is not None:\n            out_act = get_activation(config.output_activation)\n            layers.append(out_act)\n\n        self.net = nn.Sequential(*layers)\n        self.apply(self._init)\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Parameter.forward","title":"<code>forward(x: Tensor | None = None) -&gt; Tensor</code>","text":"<p>Get the value of the parameter.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor | None</code> <p>Input tensor (required for 'mlp' mode).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The parameter value.</p> Source code in <code>src/pinn/core/nn.py</code> <pre><code>@override\ndef forward(self, x: Tensor | None = None) -&gt; Tensor:\n    \"\"\"\n    Get the value of the parameter.\n\n    Args:\n        x: Input tensor (required for 'mlp' mode).\n\n    Returns:\n        The parameter value.\n    \"\"\"\n    if self.mode == \"scalar\":\n        return self.value if x is None else self.value.expand_as(x)\n    else:\n        assert x is not None, \"Function-valued parameter requires input\"\n        return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Problem","title":"<code>Problem</code>","text":"<p>               Bases: <code>Module</code></p> <p>Aggregates operator residuals and constraints into total loss. Manages fields, parameters, constraints, and validation.</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>list[Constraint]</code> <p>List of constraints to enforce.</p> required <code>criterion</code> <code>Module</code> <p>Loss function module.</p> required <code>fields</code> <code>FieldsRegistry</code> <p>List of fields (neural networks) to solve for.</p> required <code>params</code> <code>ParamsRegistry</code> <p>List of learnable parameters.</p> required Source code in <code>src/pinn/core/problem.py</code> <pre><code>class Problem(nn.Module):\n    \"\"\"\n    Aggregates operator residuals and constraints into total loss.\n    Manages fields, parameters, constraints, and validation.\n\n    Args:\n        constraints: List of constraints to enforce.\n        criterion: Loss function module.\n        fields: List of fields (neural networks) to solve for.\n        params: List of learnable parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        constraints: list[Constraint],\n        criterion: nn.Module,\n        fields: FieldsRegistry,\n        params: ParamsRegistry,\n    ):\n        super().__init__()\n        self.constraints = constraints\n        self.criterion = criterion\n        self.fields = fields\n        self.params = params\n\n        self._fields = nn.ModuleList(fields.values())\n        self._params = nn.ModuleList(params.values())\n\n    def inject_context(self, context: InferredContext) -&gt; None:\n        \"\"\"\n        Inject the context into the problem.\n\n        This should be called after data is loaded but before training starts.\n        Pure function entries are passed through unchanged.\n\n        Args:\n            context: The context to inject.\n        \"\"\"\n        self.context = context\n        for c in self.constraints:\n            c.inject_context(context)\n\n    def training_loss(self, batch: TrainingBatch, log: LogFn | None = None) -&gt; Tensor:\n        \"\"\"\n        Calculate the total loss from all constraints.\n\n        Args:\n            batch: Current batch.\n            log: Optional logging function.\n\n        Returns:\n            Sum of losses from all constraints.\n        \"\"\"\n        _, x_coll = batch\n\n        total = torch.tensor(0.0, device=x_coll.device)\n        for c in self.constraints:\n            total = total + c.loss(batch, self.criterion, log)\n\n        if log is not None:\n            for name, param in self.params.items():\n                param_loss = self._param_validation_loss(name, param, x_coll)\n                if param_loss is not None:\n                    log(f\"loss/{name}\", param_loss, progress_bar=True)\n\n            log(LOSS_KEY, total, progress_bar=True)\n\n        return total\n\n    def predict(self, batch: DataBatch) -&gt; tuple[DataBatch, dict[str, Tensor]]:\n        \"\"\"\n        Generate predictions for a given batch of data.\n        Returns unscaled predictions in original domain.\n\n        Args:\n            batch: Batch of input coordinates.\n\n        Returns:\n            Tuple of (original_batch, predictions_dict).\n        \"\"\"\n\n        x, y = batch\n\n        preds = {name: f(x).squeeze(-1) for name, f in self.fields.items()}\n        preds |= {name: p(x).squeeze(-1) for name, p in self.params.items()}\n\n        return (x.squeeze(-1), y.squeeze(-1)), preds\n\n    def true_values(self, x: Tensor) -&gt; dict[str, Tensor] | None:\n        \"\"\"\n        Get the true values for a given x coordinates.\n        Returns None if no validation source is configured.\n        \"\"\"\n\n        return {\n            name: p_true.squeeze(-1)\n            for name, p in self.params.items()\n            if (p_true := self._get_true_param(name, x)) is not None\n        } or None\n\n    def _get_true_param(self, param_name: str, x: Tensor) -&gt; Tensor | None:\n        \"\"\"\n        Get the ground truth values for a parameter at given coordinates.\n\n        Args:\n            param_name: Name of the parameter.\n            x: Input coordinates.\n\n        Returns:\n            Ground truth values, or None if no validation source is configured.\n        \"\"\"\n        if param_name not in self.context.validation:\n            return None\n\n        return self.context.validation[param_name](x)\n\n    def _param_validation_loss(\n        self, param_name: str, param: Parameter, x_coll: Tensor\n    ) -&gt; Tensor | None:\n        \"\"\"\n        Compute validation loss for a parameter against ground truth.\n\n        Args:\n            param: The parameter to compute validation loss for.\n            x_coll: The input coordinates.\n\n        Returns:\n            Loss value, or None if no validation source is configured.\n        \"\"\"\n        true = self._get_true_param(param_name, x_coll)\n        if true is None:\n            return None\n\n        pred = param(x_coll)\n\n        return torch.mean((true - pred) ** 2)\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Problem.constraints","title":"<code>constraints = constraints</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Problem.criterion","title":"<code>criterion = criterion</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Problem.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Problem.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.Problem.__init__","title":"<code>__init__(constraints: list[Constraint], criterion: nn.Module, fields: FieldsRegistry, params: ParamsRegistry)</code>","text":"Source code in <code>src/pinn/core/problem.py</code> <pre><code>def __init__(\n    self,\n    constraints: list[Constraint],\n    criterion: nn.Module,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n):\n    super().__init__()\n    self.constraints = constraints\n    self.criterion = criterion\n    self.fields = fields\n    self.params = params\n\n    self._fields = nn.ModuleList(fields.values())\n    self._params = nn.ModuleList(params.values())\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Problem.inject_context","title":"<code>inject_context(context: InferredContext) -&gt; None</code>","text":"<p>Inject the context into the problem.</p> <p>This should be called after data is loaded but before training starts. Pure function entries are passed through unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>InferredContext</code> <p>The context to inject.</p> required Source code in <code>src/pinn/core/problem.py</code> <pre><code>def inject_context(self, context: InferredContext) -&gt; None:\n    \"\"\"\n    Inject the context into the problem.\n\n    This should be called after data is loaded but before training starts.\n    Pure function entries are passed through unchanged.\n\n    Args:\n        context: The context to inject.\n    \"\"\"\n    self.context = context\n    for c in self.constraints:\n        c.inject_context(context)\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Problem.predict","title":"<code>predict(batch: DataBatch) -&gt; tuple[DataBatch, dict[str, Tensor]]</code>","text":"<p>Generate predictions for a given batch of data. Returns unscaled predictions in original domain.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>DataBatch</code> <p>Batch of input coordinates.</p> required <p>Returns:</p> Type Description <code>tuple[DataBatch, dict[str, Tensor]]</code> <p>Tuple of (original_batch, predictions_dict).</p> Source code in <code>src/pinn/core/problem.py</code> <pre><code>def predict(self, batch: DataBatch) -&gt; tuple[DataBatch, dict[str, Tensor]]:\n    \"\"\"\n    Generate predictions for a given batch of data.\n    Returns unscaled predictions in original domain.\n\n    Args:\n        batch: Batch of input coordinates.\n\n    Returns:\n        Tuple of (original_batch, predictions_dict).\n    \"\"\"\n\n    x, y = batch\n\n    preds = {name: f(x).squeeze(-1) for name, f in self.fields.items()}\n    preds |= {name: p(x).squeeze(-1) for name, p in self.params.items()}\n\n    return (x.squeeze(-1), y.squeeze(-1)), preds\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Problem.training_loss","title":"<code>training_loss(batch: TrainingBatch, log: LogFn | None = None) -&gt; Tensor</code>","text":"<p>Calculate the total loss from all constraints.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>TrainingBatch</code> <p>Current batch.</p> required <code>log</code> <code>LogFn | None</code> <p>Optional logging function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Sum of losses from all constraints.</p> Source code in <code>src/pinn/core/problem.py</code> <pre><code>def training_loss(self, batch: TrainingBatch, log: LogFn | None = None) -&gt; Tensor:\n    \"\"\"\n    Calculate the total loss from all constraints.\n\n    Args:\n        batch: Current batch.\n        log: Optional logging function.\n\n    Returns:\n        Sum of losses from all constraints.\n    \"\"\"\n    _, x_coll = batch\n\n    total = torch.tensor(0.0, device=x_coll.device)\n    for c in self.constraints:\n        total = total + c.loss(batch, self.criterion, log)\n\n    if log is not None:\n        for name, param in self.params.items():\n            param_loss = self._param_validation_loss(name, param, x_coll)\n            if param_loss is not None:\n                log(f\"loss/{name}\", param_loss, progress_bar=True)\n\n        log(LOSS_KEY, total, progress_bar=True)\n\n    return total\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.Problem.true_values","title":"<code>true_values(x: Tensor) -&gt; dict[str, Tensor] | None</code>","text":"<p>Get the true values for a given x coordinates. Returns None if no validation source is configured.</p> Source code in <code>src/pinn/core/problem.py</code> <pre><code>def true_values(self, x: Tensor) -&gt; dict[str, Tensor] | None:\n    \"\"\"\n    Get the true values for a given x coordinates.\n    Returns None if no validation source is configured.\n    \"\"\"\n\n    return {\n        name: p_true.squeeze(-1)\n        for name, p in self.params.items()\n        if (p_true := self._get_true_param(name, x)) is not None\n    } or None\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.SMMAStoppingConfig","title":"<code>SMMAStoppingConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Simple Moving Average Stopping callback.</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass SMMAStoppingConfig:\n    \"\"\"\n    Configuration for Simple Moving Average Stopping callback.\n    \"\"\"\n\n    window: int\n    threshold: float\n    lookback: int\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.SMMAStoppingConfig.lookback","title":"<code>lookback: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.SMMAStoppingConfig.threshold","title":"<code>threshold: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.SMMAStoppingConfig.window","title":"<code>window: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.SMMAStoppingConfig.__init__","title":"<code>__init__(*, window: int, threshold: float, lookback: int) -&gt; None</code>","text":""},{"location":"reference/pinn/core/#pinn.core.ScalarConfig","title":"<code>ScalarConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a scalar parameter.</p> <p>Attributes:</p> Name Type Description <code>init_value</code> <code>float</code> <p>Initial value for the parameter.</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass ScalarConfig:\n    \"\"\"\n    Configuration for a scalar parameter.\n\n    Attributes:\n        init_value: Initial value for the parameter.\n    \"\"\"\n\n    init_value: float\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.ScalarConfig.init_value","title":"<code>init_value: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.ScalarConfig.__init__","title":"<code>__init__(*, init_value: float) -&gt; None</code>","text":""},{"location":"reference/pinn/core/#pinn.core.SchedulerConfig","title":"<code>SchedulerConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Learning Rate Scheduler (ReduceLROnPlateau).</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass SchedulerConfig:\n    \"\"\"\n    Configuration for Learning Rate Scheduler (ReduceLROnPlateau).\n    \"\"\"\n\n    mode: Literal[\"min\", \"max\"]\n    factor: float\n    patience: int\n    threshold: float\n    min_lr: float\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.SchedulerConfig.factor","title":"<code>factor: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.SchedulerConfig.min_lr","title":"<code>min_lr: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.SchedulerConfig.mode","title":"<code>mode: Literal['min', 'max']</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.SchedulerConfig.patience","title":"<code>patience: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.SchedulerConfig.threshold","title":"<code>threshold: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.SchedulerConfig.__init__","title":"<code>__init__(*, mode: Literal['min', 'max'], factor: float, patience: int, threshold: float, min_lr: float) -&gt; None</code>","text":""},{"location":"reference/pinn/core/#pinn.core.TrainingDataConfig","title":"<code>TrainingDataConfig</code>  <code>dataclass</code>","text":"<p>Configuration for data loading and batching.</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass TrainingDataConfig:\n    \"\"\"\n    Configuration for data loading and batching.\n    \"\"\"\n\n    batch_size: int\n    data_ratio: int | float\n    collocations: int\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.TrainingDataConfig.batch_size","title":"<code>batch_size: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.TrainingDataConfig.collocations","title":"<code>collocations: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.TrainingDataConfig.data_ratio","title":"<code>data_ratio: int | float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/#pinn.core.TrainingDataConfig.__init__","title":"<code>__init__(*, batch_size: int, data_ratio: int | float, collocations: int) -&gt; None</code>","text":""},{"location":"reference/pinn/core/#pinn.core.get_activation","title":"<code>get_activation(name: Activations) -&gt; nn.Module</code>","text":"<p>Get the activation function module by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Activations</code> <p>The name of the activation function.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The PyTorch activation module.</p> Source code in <code>src/pinn/core/nn.py</code> <pre><code>def get_activation(name: Activations) -&gt; nn.Module:\n    \"\"\"\n    Get the activation function module by name.\n\n    Args:\n        name: The name of the activation function.\n\n    Returns:\n        The PyTorch activation module.\n    \"\"\"\n    return {\n        \"tanh\": nn.Tanh(),\n        \"relu\": nn.ReLU(),\n        \"leaky_relu\": nn.LeakyReLU(),\n        \"sigmoid\": nn.Sigmoid(),\n        \"selu\": nn.SELU(),\n        \"softplus\": nn.Softplus(),\n        \"identity\": nn.Identity(),\n    }[name]\n</code></pre>"},{"location":"reference/pinn/core/#pinn.core.resolve_validation","title":"<code>resolve_validation(registry: ValidationRegistry, df_path: Path | None = None) -&gt; ResolvedValidation</code>","text":"<p>Resolve a ValidationRegistry by converting ColumnRef entries to callables.</p> <p>Pure function entries are passed through unchanged. ColumnRef entries are resolved using the provided data file path.</p> <p>Parameters:</p> Name Type Description Default <code>registry</code> <code>ValidationRegistry</code> <p>The validation registry to resolve.</p> required <code>df_path</code> <code>Path | None</code> <p>Path to the CSV file for ColumnRef resolution.</p> <code>None</code> <p>Returns:</p> Type Description <code>ResolvedValidation</code> <p>A dictionary mapping parameter names to callable validation functions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a ColumnRef cannot be resolved (missing column or no df_path).</p> Source code in <code>src/pinn/core/validation.py</code> <pre><code>def resolve_validation(\n    registry: ValidationRegistry,\n    df_path: Path | None = None,\n) -&gt; ResolvedValidation:\n    \"\"\"\n    Resolve a ValidationRegistry by converting ColumnRef entries to callables.\n\n    Pure function entries are passed through unchanged. ColumnRef entries\n    are resolved using the provided data file path.\n\n    Args:\n        registry: The validation registry to resolve.\n        df_path: Path to the CSV file for ColumnRef resolution.\n\n    Returns:\n        A dictionary mapping parameter names to callable validation functions.\n\n    Raises:\n        ValueError: If a ColumnRef cannot be resolved (missing column or no df_path).\n    \"\"\"\n\n    resolved: ResolvedValidation = {}\n\n    for name, source in registry.items():\n        if source is None:\n            continue\n\n        if callable(source) and not isinstance(source, ColumnRef):\n            resolved[name] = source\n\n        elif isinstance(source, ColumnRef):\n            if df_path is None:\n                raise ValueError(\n                    f\"Cannot resolve ColumnRef for '{name}': no df_path provided. \"\n                    \"Either pass a df_path or use a callable instead of ColumnRef.\"\n                )\n\n            df = pd.read_csv(df_path)\n\n            if source.column not in df.columns:\n                raise ValueError(\n                    f\"Cannot resolve ColumnRef for '{name}': \"\n                    f\"column '{source.column}' not found in data. \"\n                    f\"Available columns: {list(df.columns)}\"\n                )\n\n            column_values = torch.tensor(df[source.column].values, dtype=torch.float32)\n\n            if source.transform is not None:\n                column_values = source.transform(column_values)\n\n            def make_lookup_fn(values: Tensor) -&gt; Callable[[Tensor], Tensor]:\n                def lookup(x: Tensor) -&gt; Tensor:\n                    idx = x.squeeze(-1).round().to(torch.int32)\n                    return values.to(x.device)[idx]\n\n                return lookup\n\n            resolved[name] = make_lookup_fn(column_values)\n\n    return resolved\n</code></pre>"},{"location":"reference/pinn/core/config/","title":"config","text":""},{"location":"reference/pinn/core/config/#pinn.core.config","title":"<code>pinn.core.config</code>","text":"<p>Configuration dataclasses for PINN models.</p>"},{"location":"reference/pinn/core/config/#pinn.core.config.EarlyStoppingConfig","title":"<code>EarlyStoppingConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Early Stopping callback.</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass EarlyStoppingConfig:\n    \"\"\"\n    Configuration for Early Stopping callback.\n    \"\"\"\n\n    patience: int\n    mode: Literal[\"min\", \"max\"]\n</code></pre>"},{"location":"reference/pinn/core/config/#pinn.core.config.EarlyStoppingConfig.mode","title":"<code>mode: Literal['min', 'max']</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.EarlyStoppingConfig.patience","title":"<code>patience: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.EarlyStoppingConfig.__init__","title":"<code>__init__(*, patience: int, mode: Literal['min', 'max']) -&gt; None</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.GenerationConfig","title":"<code>GenerationConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrainingDataConfig</code></p> <p>Configuration for data generation.</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass GenerationConfig(TrainingDataConfig):\n    \"\"\"\n    Configuration for data generation.\n    \"\"\"\n\n    x: Tensor\n    noise_level: float\n    args_to_train: ArgsRegistry\n</code></pre>"},{"location":"reference/pinn/core/config/#pinn.core.config.GenerationConfig.args_to_train","title":"<code>args_to_train: ArgsRegistry</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.GenerationConfig.noise_level","title":"<code>noise_level: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.GenerationConfig.x","title":"<code>x: Tensor</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.GenerationConfig.__init__","title":"<code>__init__(*, batch_size: int, data_ratio: int | float, collocations: int, x: Tensor, noise_level: float, args_to_train: ArgsRegistry) -&gt; None</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.IngestionConfig","title":"<code>IngestionConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrainingDataConfig</code></p> <p>Configuration for data ingestion from files. If x_column is None, the data is assumed to be evenly spaced.</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass IngestionConfig(TrainingDataConfig):\n    \"\"\"\n    Configuration for data ingestion from files.\n    If x_column is None, the data is assumed to be evenly spaced.\n    \"\"\"\n\n    df_path: Path\n    x_transform: Callable[[Any], Any] | None = None\n    x_column: str | None = None\n    y_columns: list[str]\n</code></pre>"},{"location":"reference/pinn/core/config/#pinn.core.config.IngestionConfig.df_path","title":"<code>df_path: Path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.IngestionConfig.x_column","title":"<code>x_column: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.IngestionConfig.x_transform","title":"<code>x_transform: Callable[[Any], Any] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.IngestionConfig.y_columns","title":"<code>y_columns: list[str]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.IngestionConfig.__init__","title":"<code>__init__(*, batch_size: int, data_ratio: int | float, collocations: int, df_path: Path, x_transform: Callable[[Any], Any] | None = None, x_column: str | None = None, y_columns: list[str]) -&gt; None</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.MLPConfig","title":"<code>MLPConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a Multi-Layer Perceptron (MLP).</p> <p>Attributes:</p> Name Type Description <code>in_dim</code> <code>int</code> <p>Dimension of input layer.</p> <code>out_dim</code> <code>int</code> <p>Dimension of output layer.</p> <code>hidden_layers</code> <code>list[int]</code> <p>List of dimensions for hidden layers.</p> <code>activation</code> <code>Activations</code> <p>Activation function to use between layers.</p> <code>output_activation</code> <code>Activations | None</code> <p>Optional activation function for the output layer.</p> <code>encode</code> <code>Callable[[Tensor], Tensor] | None</code> <p>Optional function to encode inputs before passing to MLP.</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass MLPConfig:\n    \"\"\"\n    Configuration for a Multi-Layer Perceptron (MLP).\n\n    Attributes:\n        in_dim: Dimension of input layer.\n        out_dim: Dimension of output layer.\n        hidden_layers: List of dimensions for hidden layers.\n        activation: Activation function to use between layers.\n        output_activation: Optional activation function for the output layer.\n        encode: Optional function to encode inputs before passing to MLP.\n    \"\"\"\n\n    in_dim: int\n    out_dim: int\n    hidden_layers: list[int]\n    activation: Activations\n    output_activation: Activations | None = None\n    encode: Callable[[Tensor], Tensor] | None = None\n</code></pre>"},{"location":"reference/pinn/core/config/#pinn.core.config.MLPConfig.activation","title":"<code>activation: Activations</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.MLPConfig.encode","title":"<code>encode: Callable[[Tensor], Tensor] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.MLPConfig.hidden_layers","title":"<code>hidden_layers: list[int]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.MLPConfig.in_dim","title":"<code>in_dim: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.MLPConfig.out_dim","title":"<code>out_dim: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.MLPConfig.output_activation","title":"<code>output_activation: Activations | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.MLPConfig.__init__","title":"<code>__init__(*, in_dim: int, out_dim: int, hidden_layers: list[int], activation: Activations, output_activation: Activations | None = None, encode: Callable[[Tensor], Tensor] | None = None) -&gt; None</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.PINNHyperparameters","title":"<code>PINNHyperparameters</code>  <code>dataclass</code>","text":"<p>Aggregated hyperparameters for the PINN model.</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass PINNHyperparameters:\n    \"\"\"\n    Aggregated hyperparameters for the PINN model.\n    \"\"\"\n\n    lr: float\n    training_data: IngestionConfig | GenerationConfig\n    fields_config: MLPConfig\n    params_config: MLPConfig | ScalarConfig\n    scheduler: SchedulerConfig | None = None\n    early_stopping: EarlyStoppingConfig | None = None\n    smma_stopping: SMMAStoppingConfig | None = None\n</code></pre>"},{"location":"reference/pinn/core/config/#pinn.core.config.PINNHyperparameters.early_stopping","title":"<code>early_stopping: EarlyStoppingConfig | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.PINNHyperparameters.fields_config","title":"<code>fields_config: MLPConfig</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.PINNHyperparameters.lr","title":"<code>lr: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.PINNHyperparameters.params_config","title":"<code>params_config: MLPConfig | ScalarConfig</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.PINNHyperparameters.scheduler","title":"<code>scheduler: SchedulerConfig | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.PINNHyperparameters.smma_stopping","title":"<code>smma_stopping: SMMAStoppingConfig | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.PINNHyperparameters.training_data","title":"<code>training_data: IngestionConfig | GenerationConfig</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.PINNHyperparameters.__init__","title":"<code>__init__(*, lr: float, training_data: IngestionConfig | GenerationConfig, fields_config: MLPConfig, params_config: MLPConfig | ScalarConfig, scheduler: SchedulerConfig | None = None, early_stopping: EarlyStoppingConfig | None = None, smma_stopping: SMMAStoppingConfig | None = None) -&gt; None</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.SMMAStoppingConfig","title":"<code>SMMAStoppingConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Simple Moving Average Stopping callback.</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass SMMAStoppingConfig:\n    \"\"\"\n    Configuration for Simple Moving Average Stopping callback.\n    \"\"\"\n\n    window: int\n    threshold: float\n    lookback: int\n</code></pre>"},{"location":"reference/pinn/core/config/#pinn.core.config.SMMAStoppingConfig.lookback","title":"<code>lookback: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.SMMAStoppingConfig.threshold","title":"<code>threshold: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.SMMAStoppingConfig.window","title":"<code>window: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.SMMAStoppingConfig.__init__","title":"<code>__init__(*, window: int, threshold: float, lookback: int) -&gt; None</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.ScalarConfig","title":"<code>ScalarConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a scalar parameter.</p> <p>Attributes:</p> Name Type Description <code>init_value</code> <code>float</code> <p>Initial value for the parameter.</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass ScalarConfig:\n    \"\"\"\n    Configuration for a scalar parameter.\n\n    Attributes:\n        init_value: Initial value for the parameter.\n    \"\"\"\n\n    init_value: float\n</code></pre>"},{"location":"reference/pinn/core/config/#pinn.core.config.ScalarConfig.init_value","title":"<code>init_value: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.ScalarConfig.__init__","title":"<code>__init__(*, init_value: float) -&gt; None</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.SchedulerConfig","title":"<code>SchedulerConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Learning Rate Scheduler (ReduceLROnPlateau).</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass SchedulerConfig:\n    \"\"\"\n    Configuration for Learning Rate Scheduler (ReduceLROnPlateau).\n    \"\"\"\n\n    mode: Literal[\"min\", \"max\"]\n    factor: float\n    patience: int\n    threshold: float\n    min_lr: float\n</code></pre>"},{"location":"reference/pinn/core/config/#pinn.core.config.SchedulerConfig.factor","title":"<code>factor: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.SchedulerConfig.min_lr","title":"<code>min_lr: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.SchedulerConfig.mode","title":"<code>mode: Literal['min', 'max']</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.SchedulerConfig.patience","title":"<code>patience: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.SchedulerConfig.threshold","title":"<code>threshold: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.SchedulerConfig.__init__","title":"<code>__init__(*, mode: Literal['min', 'max'], factor: float, patience: int, threshold: float, min_lr: float) -&gt; None</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.TrainingDataConfig","title":"<code>TrainingDataConfig</code>  <code>dataclass</code>","text":"<p>Configuration for data loading and batching.</p> Source code in <code>src/pinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass TrainingDataConfig:\n    \"\"\"\n    Configuration for data loading and batching.\n    \"\"\"\n\n    batch_size: int\n    data_ratio: int | float\n    collocations: int\n</code></pre>"},{"location":"reference/pinn/core/config/#pinn.core.config.TrainingDataConfig.batch_size","title":"<code>batch_size: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.TrainingDataConfig.collocations","title":"<code>collocations: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.TrainingDataConfig.data_ratio","title":"<code>data_ratio: int | float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/config/#pinn.core.config.TrainingDataConfig.__init__","title":"<code>__init__(*, batch_size: int, data_ratio: int | float, collocations: int) -&gt; None</code>","text":""},{"location":"reference/pinn/core/context/","title":"context","text":""},{"location":"reference/pinn/core/context/#pinn.core.context","title":"<code>pinn.core.context</code>","text":"<p>Runtime context inferred from training data.</p>"},{"location":"reference/pinn/core/context/#pinn.core.context.InferredContext","title":"<code>InferredContext</code>  <code>dataclass</code>","text":"<p>Runtime context inferred from training data.</p> <p>This holds the data that is either explicitly provided in props or inferred from training data.</p> Source code in <code>src/pinn/core/context.py</code> <pre><code>@dataclass\nclass InferredContext:\n    \"\"\"\n    Runtime context inferred from training data.\n\n    This holds the data that is either explicitly provided in props or inferred from training data.\n    \"\"\"\n\n    def __init__(\n        self,\n        x: Tensor,\n        y: Tensor,\n        validation: ResolvedValidation,\n    ):\n        \"\"\"\n        Infer context from either generated or loaded data.\n\n        Args:\n            x: x coordinates.\n            y: observations.\n            validation: Resolved validation dictionary.\n        \"\"\"\n\n        self.domain = Domain1D.from_x(x)\n        self.validation = validation\n</code></pre>"},{"location":"reference/pinn/core/context/#pinn.core.context.InferredContext.domain","title":"<code>domain = Domain1D.from_x(x)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/context/#pinn.core.context.InferredContext.validation","title":"<code>validation = validation</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/context/#pinn.core.context.InferredContext.__init__","title":"<code>__init__(x: Tensor, y: Tensor, validation: ResolvedValidation)</code>","text":"<p>Infer context from either generated or loaded data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>x coordinates.</p> required <code>y</code> <code>Tensor</code> <p>observations.</p> required <code>validation</code> <code>ResolvedValidation</code> <p>Resolved validation dictionary.</p> required Source code in <code>src/pinn/core/context.py</code> <pre><code>def __init__(\n    self,\n    x: Tensor,\n    y: Tensor,\n    validation: ResolvedValidation,\n):\n    \"\"\"\n    Infer context from either generated or loaded data.\n\n    Args:\n        x: x coordinates.\n        y: observations.\n        validation: Resolved validation dictionary.\n    \"\"\"\n\n    self.domain = Domain1D.from_x(x)\n    self.validation = validation\n</code></pre>"},{"location":"reference/pinn/core/dataset/","title":"dataset","text":""},{"location":"reference/pinn/core/dataset/#pinn.core.dataset","title":"<code>pinn.core.dataset</code>","text":"<p>Data handling for PINN training.</p>"},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.DataCallback","title":"<code>DataCallback</code>","text":"<p>Abstract base class for building new data callbacks.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>class DataCallback:\n    \"\"\"Abstract base class for building new data callbacks.\"\"\"\n\n    def transform_data(self, data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]:\n        \"\"\"Transform the data and collocation points.\"\"\"\n        return data, coll\n\n    def on_after_setup(self, dm: \"PINNDataModule\") -&gt; None:\n        \"\"\"Called after setup is complete.\"\"\"\n        return None\n</code></pre>"},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.DataCallback.on_after_setup","title":"<code>on_after_setup(dm: PINNDataModule) -&gt; None</code>","text":"<p>Called after setup is complete.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>def on_after_setup(self, dm: \"PINNDataModule\") -&gt; None:\n    \"\"\"Called after setup is complete.\"\"\"\n    return None\n</code></pre>"},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.DataCallback.transform_data","title":"<code>transform_data(data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]</code>","text":"<p>Transform the data and collocation points.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>def transform_data(self, data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]:\n    \"\"\"Transform the data and collocation points.\"\"\"\n    return data, coll\n</code></pre>"},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataModule","title":"<code>PINNDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code>, <code>ABC</code></p> <p>LightningDataModule for PINNs. Manages data and collocation datasets and creates the combined PINNDataset.</p> <p>Attributes:</p> Name Type Description <code>data_ds</code> <p>Dataset containing observed data.</p> <code>coll_ds</code> <p>Dataset containing collocation points.</p> <code>pinn_ds</code> <p>Combined PINNDataset for training.</p> <code>callbacks</code> <code>list[DataCallback]</code> <p>Sequence of DataCallback callbacks applied after data loading.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>class PINNDataModule(pl.LightningDataModule, ABC):\n    \"\"\"\n    LightningDataModule for PINNs.\n    Manages data and collocation datasets and creates the combined PINNDataset.\n\n    Attributes:\n        data_ds: Dataset containing observed data.\n        coll_ds: Dataset containing collocation points.\n        pinn_ds: Combined PINNDataset for training.\n        callbacks: Sequence of DataCallback callbacks applied after data loading.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ) -&gt; None:\n        super().__init__()\n        self.hp = hp\n        self.callbacks: list[DataCallback] = list(callbacks) if callbacks else []\n\n        self._unresolved_validation = validation or {}\n\n    def load_data(self, config: IngestionConfig) -&gt; DataBatch:\n        \"\"\"Load raw data from IngestionConfig.\"\"\"\n        df = pd.read_csv(config.df_path)\n\n        if config.x_column is not None:\n            x_values = df[config.x_column].values\n\n            if config.x_transform is not None:\n                x_values = config.x_transform(x_values)\n\n            x = torch.tensor(x_values, dtype=torch.float32)\n        else:\n            x = torch.arange(len(df), dtype=torch.float32)\n\n        y = torch.tensor(df[config.y_columns].values, dtype=torch.float32)\n\n        if y.shape[1] != 1:\n            y = y.unsqueeze(-1)\n\n        return x.unsqueeze(-1), y\n\n    @abstractmethod\n    def gen_data(self, config: GenerationConfig) -&gt; DataBatch:\n        \"\"\"Generate synthetic data from GenerationConfig.\"\"\"\n\n    @abstractmethod\n    def gen_coll(self, domain: Domain1D) -&gt; Tensor:\n        \"\"\"Generate collocation points.\"\"\"\n\n    @override\n    def setup(self, stage: str | None = None) -&gt; None:\n        \"\"\"\n        Load raw data from IngestionConfig, or generate synthetic data from GenerationConfig.\n        Apply registered callbacks, create InferredContext and datasets.\n        \"\"\"\n        config = self.hp.training_data\n\n        self.validation = resolve_validation(\n            self._unresolved_validation,\n            config.df_path if isinstance(config, IngestionConfig) else None,\n        )\n\n        self.data = (\n            self.load_data(config)\n            if isinstance(config, IngestionConfig)\n            else self.gen_data(config)\n        )\n\n        self.coll = self.gen_coll(\n            Domain1D.from_x(self.data[0]),\n        )\n\n        for callback in self.callbacks:\n            self.data, self.coll = callback.transform_data(self.data, self.coll)\n\n        x_data, y_data = self.data\n\n        assert x_data.shape[0] == y_data.shape[0], \"Size mismatch between x and y.\"\n        assert x_data.ndim == 2, \"x shape differs than (n, 1).\"\n        assert x_data.shape[1] == 1, \"x shape differs than (n, 1).\"\n        assert y_data.ndim &gt; 1, \"y shape cannot be (n).\"\n        assert y_data.shape[-1] == 1, \"y shape differs than (n, 1).\"\n        assert self.coll.ndim == 2, \"coll shape differs than (m, 1).\"\n        assert self.coll.shape[1] == 1, \"coll shape differs than (m, 1).\"\n\n        self._data_size = x_data.shape[0]\n\n        self._context = InferredContext(\n            x_data,\n            y_data,\n            self.validation,\n        )\n\n        self.pinn_ds = PINNDataset(\n            x_data,\n            y_data,\n            self.coll,\n            config.batch_size,\n            config.data_ratio,\n        )\n\n        self.predict_ds = TensorDataset(\n            x_data,\n            y_data,\n        )\n\n        for callback in self.callbacks:\n            callback.on_after_setup(self)\n\n    @override\n    def train_dataloader(self) -&gt; DataLoader[TrainingBatch]:\n        \"\"\"\n        Returns the training dataloader using PINNDataset.\n        \"\"\"\n        return DataLoader[TrainingBatch](\n            self.pinn_ds,\n            batch_size=None,  # handled internally\n            num_workers=7,\n            persistent_workers=True,\n        )\n\n    @override\n    def predict_dataloader(self) -&gt; DataLoader[PredictionBatch]:\n        \"\"\"\n        Returns the prediction dataloader using only the data dataset.\n        \"\"\"\n        return DataLoader[PredictionBatch](\n            cast(Dataset[PredictionBatch], self.predict_ds),\n            batch_size=self._data_size,\n            num_workers=7,\n            persistent_workers=True,\n        )\n\n    @property\n    def context(self) -&gt; InferredContext:\n        assert self._context is not None, (\n            \"Context does not exist. `setup` stage not completed yet.\"\n        )\n        return self._context\n</code></pre>"},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataModule.callbacks","title":"<code>callbacks: list[DataCallback] = list(callbacks) if callbacks else []</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataModule.context","title":"<code>context: InferredContext</code>  <code>property</code>","text":""},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataModule.hp","title":"<code>hp = hp</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None) -&gt; None</code>","text":"Source code in <code>src/pinn/core/dataset.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n) -&gt; None:\n    super().__init__()\n    self.hp = hp\n    self.callbacks: list[DataCallback] = list(callbacks) if callbacks else []\n\n    self._unresolved_validation = validation or {}\n</code></pre>"},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataModule.gen_coll","title":"<code>gen_coll(domain: Domain1D) -&gt; Tensor</code>  <code>abstractmethod</code>","text":"<p>Generate collocation points.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>@abstractmethod\ndef gen_coll(self, domain: Domain1D) -&gt; Tensor:\n    \"\"\"Generate collocation points.\"\"\"\n</code></pre>"},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; DataBatch</code>  <code>abstractmethod</code>","text":"<p>Generate synthetic data from GenerationConfig.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>@abstractmethod\ndef gen_data(self, config: GenerationConfig) -&gt; DataBatch:\n    \"\"\"Generate synthetic data from GenerationConfig.\"\"\"\n</code></pre>"},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataModule.load_data","title":"<code>load_data(config: IngestionConfig) -&gt; DataBatch</code>","text":"<p>Load raw data from IngestionConfig.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>def load_data(self, config: IngestionConfig) -&gt; DataBatch:\n    \"\"\"Load raw data from IngestionConfig.\"\"\"\n    df = pd.read_csv(config.df_path)\n\n    if config.x_column is not None:\n        x_values = df[config.x_column].values\n\n        if config.x_transform is not None:\n            x_values = config.x_transform(x_values)\n\n        x = torch.tensor(x_values, dtype=torch.float32)\n    else:\n        x = torch.arange(len(df), dtype=torch.float32)\n\n    y = torch.tensor(df[config.y_columns].values, dtype=torch.float32)\n\n    if y.shape[1] != 1:\n        y = y.unsqueeze(-1)\n\n    return x.unsqueeze(-1), y\n</code></pre>"},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataModule.predict_dataloader","title":"<code>predict_dataloader() -&gt; DataLoader[PredictionBatch]</code>","text":"<p>Returns the prediction dataloader using only the data dataset.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>@override\ndef predict_dataloader(self) -&gt; DataLoader[PredictionBatch]:\n    \"\"\"\n    Returns the prediction dataloader using only the data dataset.\n    \"\"\"\n    return DataLoader[PredictionBatch](\n        cast(Dataset[PredictionBatch], self.predict_ds),\n        batch_size=self._data_size,\n        num_workers=7,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataModule.setup","title":"<code>setup(stage: str | None = None) -&gt; None</code>","text":"<p>Load raw data from IngestionConfig, or generate synthetic data from GenerationConfig. Apply registered callbacks, create InferredContext and datasets.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>@override\ndef setup(self, stage: str | None = None) -&gt; None:\n    \"\"\"\n    Load raw data from IngestionConfig, or generate synthetic data from GenerationConfig.\n    Apply registered callbacks, create InferredContext and datasets.\n    \"\"\"\n    config = self.hp.training_data\n\n    self.validation = resolve_validation(\n        self._unresolved_validation,\n        config.df_path if isinstance(config, IngestionConfig) else None,\n    )\n\n    self.data = (\n        self.load_data(config)\n        if isinstance(config, IngestionConfig)\n        else self.gen_data(config)\n    )\n\n    self.coll = self.gen_coll(\n        Domain1D.from_x(self.data[0]),\n    )\n\n    for callback in self.callbacks:\n        self.data, self.coll = callback.transform_data(self.data, self.coll)\n\n    x_data, y_data = self.data\n\n    assert x_data.shape[0] == y_data.shape[0], \"Size mismatch between x and y.\"\n    assert x_data.ndim == 2, \"x shape differs than (n, 1).\"\n    assert x_data.shape[1] == 1, \"x shape differs than (n, 1).\"\n    assert y_data.ndim &gt; 1, \"y shape cannot be (n).\"\n    assert y_data.shape[-1] == 1, \"y shape differs than (n, 1).\"\n    assert self.coll.ndim == 2, \"coll shape differs than (m, 1).\"\n    assert self.coll.shape[1] == 1, \"coll shape differs than (m, 1).\"\n\n    self._data_size = x_data.shape[0]\n\n    self._context = InferredContext(\n        x_data,\n        y_data,\n        self.validation,\n    )\n\n    self.pinn_ds = PINNDataset(\n        x_data,\n        y_data,\n        self.coll,\n        config.batch_size,\n        config.data_ratio,\n    )\n\n    self.predict_ds = TensorDataset(\n        x_data,\n        y_data,\n    )\n\n    for callback in self.callbacks:\n        callback.on_after_setup(self)\n</code></pre>"},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataModule.train_dataloader","title":"<code>train_dataloader() -&gt; DataLoader[TrainingBatch]</code>","text":"<p>Returns the training dataloader using PINNDataset.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>@override\ndef train_dataloader(self) -&gt; DataLoader[TrainingBatch]:\n    \"\"\"\n    Returns the training dataloader using PINNDataset.\n    \"\"\"\n    return DataLoader[TrainingBatch](\n        self.pinn_ds,\n        batch_size=None,  # handled internally\n        num_workers=7,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataset","title":"<code>PINNDataset</code>","text":"<p>               Bases: <code>Dataset[TrainingBatch]</code></p> <p>Dataset used for PINN training. Combines labeled data and collocation points per sample.  Given a data_ratio, the amount of data points <code>K</code> is determined either by applying <code>data_ratio * batch_size</code> if ratio is a float between 0 and 1 or by an absolute count if ratio is an integer. The remaining <code>C</code> points are used for collocation.  The data points are sampled without replacement per epoch i.e. cycles through all data points and at the last batch, wraps around to the first indices to ensure batch size. The collocation points are sampled with replacement from the pool. The dataset produces a batch of shape ((t_data[K,1], y_data[K,1]), t_coll[C,1]).</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>Tensor</code> <p>Data point x coordinates (time values).</p> required <code>y_data</code> <code>Tensor</code> <p>Data point y values (observations).</p> required <code>x_coll</code> <code>Tensor</code> <p>Collocation point x coordinates.</p> required <code>batch_size</code> <code>int</code> <p>Size of the batch.</p> required <code>data_ratio</code> <code>float | int</code> <p>Ratio of data points to collocation points, either as a ratio [0,1] or absolute count [0,batch_size].</p> required Source code in <code>src/pinn/core/dataset.py</code> <pre><code>class PINNDataset(Dataset[TrainingBatch]):\n    \"\"\"\n    Dataset used for PINN training. Combines labeled data and collocation points\n    per sample.  Given a data_ratio, the amount of data points `K` is determined\n    either by applying `data_ratio * batch_size` if ratio is a float between 0\n    and 1 or by an absolute count if ratio is an integer. The remaining `C`\n    points are used for collocation.  The data points are sampled without\n    replacement per epoch i.e. cycles through all data points and at the last\n    batch, wraps around to the first indices to ensure batch size. The collocation\n    points are sampled with replacement from the pool.\n    The dataset produces a batch of shape ((t_data[K,1], y_data[K,1]), t_coll[C,1]).\n\n    Args:\n        x_data: Data point x coordinates (time values).\n        y_data: Data point y values (observations).\n        x_coll: Collocation point x coordinates.\n        batch_size: Size of the batch.\n        data_ratio: Ratio of data points to collocation points, either as a ratio [0,1] or absolute\n            count [0,batch_size].\n    \"\"\"\n\n    def __init__(\n        self,\n        x_data: Tensor,\n        y_data: Tensor,\n        x_coll: Tensor,\n        batch_size: int,\n        data_ratio: float | int,\n    ):\n        super().__init__()\n        assert batch_size &gt; 0\n\n        if isinstance(data_ratio, float):\n            assert 0.0 &lt;= data_ratio &lt;= 1.0\n            self.K = round(data_ratio * batch_size)\n        else:\n            assert 0 &lt;= data_ratio &lt;= batch_size\n            self.K = data_ratio\n\n        self.x_data = x_data\n        self.y_data = y_data\n        self.x_coll = x_coll\n\n        self.batch_size = batch_size\n        self.C = batch_size - self.K\n\n        self.total_data = x_data.shape[0]\n        self.total_coll = x_coll.shape[0]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Number of steps per epoch to see all data points once. Ceiling division.\"\"\"\n        return (self.total_data + self.K - 1) // self.K\n\n    @override\n    def __getitem__(self, idx: int) -&gt; TrainingBatch:\n        \"\"\"Return one sample containing K data points and C collocation points.\"\"\"\n        data_idx = self._get_data_indices(idx)\n        coll_idx = self._get_coll_indices(idx)\n\n        x_data = self.x_data[data_idx]\n        y_data = self.y_data[data_idx]\n        x_coll = self.x_coll[coll_idx]\n\n        return ((x_data, y_data), x_coll)\n\n    def _get_data_indices(self, idx: int) -&gt; Tensor:\n        \"\"\"Get data indices for this step without replacement.\n        When getting the last batch, wrap around to the first indices to ensure batch size.\n        \"\"\"\n        if self.total_data == 0:\n            return torch.empty(0, 1)\n\n        start = idx * self.K\n        indices = [(start + i) % self.total_data for i in range(self.K)]\n        return torch.tensor(indices)\n\n    def _get_coll_indices(self, idx: int) -&gt; Tensor:\n        \"\"\"Get collocation indices for this step with replacement.\"\"\"\n        if self.total_coll == 0:\n            return torch.empty(0, 1)\n\n        temp_gen = torch.Generator().manual_seed(idx)\n        return torch.randint(0, self.total_coll, (self.C,), generator=temp_gen)\n</code></pre>"},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataset.C","title":"<code>C = batch_size - self.K</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataset.K","title":"<code>K = round(data_ratio * batch_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataset.batch_size","title":"<code>batch_size = batch_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataset.total_coll","title":"<code>total_coll = x_coll.shape[0]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataset.total_data","title":"<code>total_data = x_data.shape[0]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataset.x_coll","title":"<code>x_coll = x_coll</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataset.x_data","title":"<code>x_data = x_data</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataset.y_data","title":"<code>y_data = y_data</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataset.__getitem__","title":"<code>__getitem__(idx: int) -&gt; TrainingBatch</code>","text":"<p>Return one sample containing K data points and C collocation points.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>@override\ndef __getitem__(self, idx: int) -&gt; TrainingBatch:\n    \"\"\"Return one sample containing K data points and C collocation points.\"\"\"\n    data_idx = self._get_data_indices(idx)\n    coll_idx = self._get_coll_indices(idx)\n\n    x_data = self.x_data[data_idx]\n    y_data = self.y_data[data_idx]\n    x_coll = self.x_coll[coll_idx]\n\n    return ((x_data, y_data), x_coll)\n</code></pre>"},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataset.__init__","title":"<code>__init__(x_data: Tensor, y_data: Tensor, x_coll: Tensor, batch_size: int, data_ratio: float | int)</code>","text":"Source code in <code>src/pinn/core/dataset.py</code> <pre><code>def __init__(\n    self,\n    x_data: Tensor,\n    y_data: Tensor,\n    x_coll: Tensor,\n    batch_size: int,\n    data_ratio: float | int,\n):\n    super().__init__()\n    assert batch_size &gt; 0\n\n    if isinstance(data_ratio, float):\n        assert 0.0 &lt;= data_ratio &lt;= 1.0\n        self.K = round(data_ratio * batch_size)\n    else:\n        assert 0 &lt;= data_ratio &lt;= batch_size\n        self.K = data_ratio\n\n    self.x_data = x_data\n    self.y_data = y_data\n    self.x_coll = x_coll\n\n    self.batch_size = batch_size\n    self.C = batch_size - self.K\n\n    self.total_data = x_data.shape[0]\n    self.total_coll = x_coll.shape[0]\n</code></pre>"},{"location":"reference/pinn/core/dataset/#pinn.core.dataset.PINNDataset.__len__","title":"<code>__len__() -&gt; int</code>","text":"<p>Number of steps per epoch to see all data points once. Ceiling division.</p> Source code in <code>src/pinn/core/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Number of steps per epoch to see all data points once. Ceiling division.\"\"\"\n    return (self.total_data + self.K - 1) // self.K\n</code></pre>"},{"location":"reference/pinn/core/nn/","title":"nn","text":""},{"location":"reference/pinn/core/nn/#pinn.core.nn","title":"<code>pinn.core.nn</code>","text":"<p>Neural network primitives and building blocks for PINN.</p>"},{"location":"reference/pinn/core/nn/#pinn.core.nn.ArgsRegistry","title":"<code>ArgsRegistry: TypeAlias = dict[str, Argument]</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/core/nn/#pinn.core.nn.FieldsRegistry","title":"<code>FieldsRegistry: TypeAlias = dict[str, Field]</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/core/nn/#pinn.core.nn.ParamsRegistry","title":"<code>ParamsRegistry: TypeAlias = dict[str, Parameter]</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/core/nn/#pinn.core.nn.Argument","title":"<code>Argument</code>","text":"<p>Represents an argument that can be passed to an ODE/PDE function. Can be a fixed float value or a callable function.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float | Callable[[Tensor], Tensor]</code> <p>The value (float) or function (callable).</p> required Source code in <code>src/pinn/core/nn.py</code> <pre><code>class Argument:\n    \"\"\"\n    Represents an argument that can be passed to an ODE/PDE function.\n    Can be a fixed float value or a callable function.\n\n    Args:\n        value: The value (float) or function (callable).\n    \"\"\"\n\n    def __init__(self, value: float | Callable[[Tensor], Tensor]):\n        self._value = value\n\n    def __call__(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Evaluate the argument.\n\n        Args:\n            x: Input tensor (context).\n\n        Returns:\n            The value of the argument, broadcasted if necessary.\n        \"\"\"\n        if callable(self._value):\n            return self._value(x)\n        else:\n            return torch.tensor(self._value, device=x.device)\n\n    @override\n    def __repr__(self) -&gt; str:\n        return f\"Argument(value={self._value})\"\n</code></pre>"},{"location":"reference/pinn/core/nn/#pinn.core.nn.Argument.__call__","title":"<code>__call__(x: Tensor) -&gt; Tensor</code>","text":"<p>Evaluate the argument.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor (context).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The value of the argument, broadcasted if necessary.</p> Source code in <code>src/pinn/core/nn.py</code> <pre><code>def __call__(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Evaluate the argument.\n\n    Args:\n        x: Input tensor (context).\n\n    Returns:\n        The value of the argument, broadcasted if necessary.\n    \"\"\"\n    if callable(self._value):\n        return self._value(x)\n    else:\n        return torch.tensor(self._value, device=x.device)\n</code></pre>"},{"location":"reference/pinn/core/nn/#pinn.core.nn.Argument.__init__","title":"<code>__init__(value: float | Callable[[Tensor], Tensor])</code>","text":"Source code in <code>src/pinn/core/nn.py</code> <pre><code>def __init__(self, value: float | Callable[[Tensor], Tensor]):\n    self._value = value\n</code></pre>"},{"location":"reference/pinn/core/nn/#pinn.core.nn.Argument.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"Source code in <code>src/pinn/core/nn.py</code> <pre><code>@override\ndef __repr__(self) -&gt; str:\n    return f\"Argument(value={self._value})\"\n</code></pre>"},{"location":"reference/pinn/core/nn/#pinn.core.nn.Domain1D","title":"<code>Domain1D</code>  <code>dataclass</code>","text":"<p>One-dimensional domain: time interval [x0, x1] with step size dx.</p> <p>Attributes:</p> Name Type Description <code>x0</code> <code>float</code> <p>Start of the interval.</p> <code>x1</code> <code>float</code> <p>End of the interval.</p> <code>dx</code> <code>float</code> <p>Step size for discretization (if applicable).</p> Source code in <code>src/pinn/core/nn.py</code> <pre><code>@dataclass\nclass Domain1D:\n    \"\"\"\n    One-dimensional domain: time interval [x0, x1] with step size dx.\n\n    Attributes:\n        x0: Start of the interval.\n        x1: End of the interval.\n        dx: Step size for discretization (if applicable).\n    \"\"\"\n\n    x0: float\n    x1: float\n    dx: float\n\n    @classmethod\n    def from_x(cls, x: Tensor) -&gt; Domain1D:\n        \"\"\"Create a domain from x coordinates.\"\"\"\n        assert x.shape[0] &gt; 1, \"At least two points are required to infer the domain.\"\n\n        x0, x1 = x[0].item(), x[-1].item()\n        dx = (x[1] - x[0]).item()\n\n        return cls(x0=x0, x1=x1, dx=dx)\n\n    @override\n    def __repr__(self) -&gt; str:\n        return f\"Domain1D(x0={self.x0}, x1={self.x1}, dx={self.dx})\"\n</code></pre>"},{"location":"reference/pinn/core/nn/#pinn.core.nn.Domain1D.dx","title":"<code>dx: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/nn/#pinn.core.nn.Domain1D.x0","title":"<code>x0: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/nn/#pinn.core.nn.Domain1D.x1","title":"<code>x1: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/nn/#pinn.core.nn.Domain1D.__init__","title":"<code>__init__(x0: float, x1: float, dx: float) -&gt; None</code>","text":""},{"location":"reference/pinn/core/nn/#pinn.core.nn.Domain1D.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"Source code in <code>src/pinn/core/nn.py</code> <pre><code>@override\ndef __repr__(self) -&gt; str:\n    return f\"Domain1D(x0={self.x0}, x1={self.x1}, dx={self.dx})\"\n</code></pre>"},{"location":"reference/pinn/core/nn/#pinn.core.nn.Domain1D.from_x","title":"<code>from_x(x: Tensor) -&gt; Domain1D</code>  <code>classmethod</code>","text":"<p>Create a domain from x coordinates.</p> Source code in <code>src/pinn/core/nn.py</code> <pre><code>@classmethod\ndef from_x(cls, x: Tensor) -&gt; Domain1D:\n    \"\"\"Create a domain from x coordinates.\"\"\"\n    assert x.shape[0] &gt; 1, \"At least two points are required to infer the domain.\"\n\n    x0, x1 = x[0].item(), x[-1].item()\n    dx = (x[1] - x[0]).item()\n\n    return cls(x0=x0, x1=x1, dx=dx)\n</code></pre>"},{"location":"reference/pinn/core/nn/#pinn.core.nn.Field","title":"<code>Field</code>","text":"<p>               Bases: <code>Module</code></p> <p>A neural field mapping coordinates -&gt; vector of state variables. Example (ODE): t -&gt; [S, I, R].</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MLPConfig</code> <p>Configuration for the MLP backing this field.</p> required Source code in <code>src/pinn/core/nn.py</code> <pre><code>class Field(nn.Module):\n    \"\"\"\n    A neural field mapping coordinates -&gt; vector of state variables.\n    Example (ODE): t -&gt; [S, I, R].\n\n    Args:\n        config: Configuration for the MLP backing this field.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: MLPConfig,\n    ):\n        super().__init__()\n        self.encode = config.encode\n        dims = [config.in_dim] + config.hidden_layers + [config.out_dim]\n        act = get_activation(config.activation)\n\n        layers: list[nn.Module] = []\n        for i in range(len(dims) - 1):\n            layers.append(nn.Linear(dims[i], dims[i + 1]))\n            if i &lt; len(dims) - 2:\n                layers.append(act)\n\n        if config.output_activation is not None:\n            out_act = get_activation(config.output_activation)\n            layers.append(out_act)\n\n        self.net = nn.Sequential(*layers)\n        self.apply(self._init)\n\n    @staticmethod\n    def _init(m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            nn.init.zeros_(m.bias)\n\n    @override\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Forward pass of the field.\n\n        Args:\n            x: Input coordinates (e.g. time, space).\n\n        Returns:\n            The values of the field at input coordinates.\n        \"\"\"\n        if self.encode is not None:\n            x = self.encode(x)\n        return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"reference/pinn/core/nn/#pinn.core.nn.Field.encode","title":"<code>encode = config.encode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/nn/#pinn.core.nn.Field.net","title":"<code>net = nn.Sequential(*layers)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/nn/#pinn.core.nn.Field.__init__","title":"<code>__init__(config: MLPConfig)</code>","text":"Source code in <code>src/pinn/core/nn.py</code> <pre><code>def __init__(\n    self,\n    config: MLPConfig,\n):\n    super().__init__()\n    self.encode = config.encode\n    dims = [config.in_dim] + config.hidden_layers + [config.out_dim]\n    act = get_activation(config.activation)\n\n    layers: list[nn.Module] = []\n    for i in range(len(dims) - 1):\n        layers.append(nn.Linear(dims[i], dims[i + 1]))\n        if i &lt; len(dims) - 2:\n            layers.append(act)\n\n    if config.output_activation is not None:\n        out_act = get_activation(config.output_activation)\n        layers.append(out_act)\n\n    self.net = nn.Sequential(*layers)\n    self.apply(self._init)\n</code></pre>"},{"location":"reference/pinn/core/nn/#pinn.core.nn.Field.forward","title":"<code>forward(x: Tensor) -&gt; Tensor</code>","text":"<p>Forward pass of the field.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input coordinates (e.g. time, space).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The values of the field at input coordinates.</p> Source code in <code>src/pinn/core/nn.py</code> <pre><code>@override\ndef forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Forward pass of the field.\n\n    Args:\n        x: Input coordinates (e.g. time, space).\n\n    Returns:\n        The values of the field at input coordinates.\n    \"\"\"\n    if self.encode is not None:\n        x = self.encode(x)\n    return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"reference/pinn/core/nn/#pinn.core.nn.Parameter","title":"<code>Parameter</code>","text":"<p>               Bases: <code>Module</code>, <code>Argument</code></p> <p>Learnable parameter. Supports scalar or function-valued parameter. For function-valued parameters (e.g. \u03b2(t)), uses a small MLP.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ScalarConfig | MLPConfig</code> <p>Configuration for the parameter (ScalarConfig or MLPConfig).</p> required Source code in <code>src/pinn/core/nn.py</code> <pre><code>class Parameter(nn.Module, Argument):\n    \"\"\"\n    Learnable parameter. Supports scalar or function-valued parameter.\n    For function-valued parameters (e.g. \u03b2(t)), uses a small MLP.\n\n    Args:\n        config: Configuration for the parameter (ScalarConfig or MLPConfig).\n    \"\"\"\n\n    def __init__(\n        self,\n        config: ScalarConfig | MLPConfig,\n    ):\n        super().__init__()\n        self.config = config\n        self._mode: Literal[\"scalar\", \"mlp\"]\n\n        if isinstance(config, ScalarConfig):\n            self._mode = \"scalar\"\n            self.value = nn.Parameter(torch.tensor(float(config.init_value), dtype=torch.float32))\n\n        else:  # isinstance(config, MLPConfig)\n            self._mode = \"mlp\"\n            dims = [config.in_dim] + config.hidden_layers + [config.out_dim]\n            act = get_activation(config.activation)\n\n            layers: list[nn.Module] = []\n            for i in range(len(dims) - 1):\n                layers.append(nn.Linear(dims[i], dims[i + 1]))\n                if i &lt; len(dims) - 2:\n                    layers.append(act)\n\n            if config.output_activation is not None:\n                out_act = get_activation(config.output_activation)\n                layers.append(out_act)\n\n            self.net = nn.Sequential(*layers)\n            self.apply(self._init)\n\n    @property\n    def mode(self) -&gt; Literal[\"scalar\", \"mlp\"]:\n        \"\"\"Mode of the parameter: 'scalar' or 'mlp'.\"\"\"\n        return self._mode\n\n    @staticmethod\n    def _init(m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            nn.init.zeros_(m.bias)\n\n    @override\n    def forward(self, x: Tensor | None = None) -&gt; Tensor:\n        \"\"\"\n        Get the value of the parameter.\n\n        Args:\n            x: Input tensor (required for 'mlp' mode).\n\n        Returns:\n            The parameter value.\n        \"\"\"\n        if self.mode == \"scalar\":\n            return self.value if x is None else self.value.expand_as(x)\n        else:\n            assert x is not None, \"Function-valued parameter requires input\"\n            return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"reference/pinn/core/nn/#pinn.core.nn.Parameter.config","title":"<code>config = config</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/nn/#pinn.core.nn.Parameter.mode","title":"<code>mode: Literal['scalar', 'mlp']</code>  <code>property</code>","text":"<p>Mode of the parameter: 'scalar' or 'mlp'.</p>"},{"location":"reference/pinn/core/nn/#pinn.core.nn.Parameter.net","title":"<code>net = nn.Sequential(*layers)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/nn/#pinn.core.nn.Parameter.value","title":"<code>value = nn.Parameter(torch.tensor(float(config.init_value), dtype=(torch.float32)))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/nn/#pinn.core.nn.Parameter.__init__","title":"<code>__init__(config: ScalarConfig | MLPConfig)</code>","text":"Source code in <code>src/pinn/core/nn.py</code> <pre><code>def __init__(\n    self,\n    config: ScalarConfig | MLPConfig,\n):\n    super().__init__()\n    self.config = config\n    self._mode: Literal[\"scalar\", \"mlp\"]\n\n    if isinstance(config, ScalarConfig):\n        self._mode = \"scalar\"\n        self.value = nn.Parameter(torch.tensor(float(config.init_value), dtype=torch.float32))\n\n    else:  # isinstance(config, MLPConfig)\n        self._mode = \"mlp\"\n        dims = [config.in_dim] + config.hidden_layers + [config.out_dim]\n        act = get_activation(config.activation)\n\n        layers: list[nn.Module] = []\n        for i in range(len(dims) - 1):\n            layers.append(nn.Linear(dims[i], dims[i + 1]))\n            if i &lt; len(dims) - 2:\n                layers.append(act)\n\n        if config.output_activation is not None:\n            out_act = get_activation(config.output_activation)\n            layers.append(out_act)\n\n        self.net = nn.Sequential(*layers)\n        self.apply(self._init)\n</code></pre>"},{"location":"reference/pinn/core/nn/#pinn.core.nn.Parameter.forward","title":"<code>forward(x: Tensor | None = None) -&gt; Tensor</code>","text":"<p>Get the value of the parameter.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor | None</code> <p>Input tensor (required for 'mlp' mode).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The parameter value.</p> Source code in <code>src/pinn/core/nn.py</code> <pre><code>@override\ndef forward(self, x: Tensor | None = None) -&gt; Tensor:\n    \"\"\"\n    Get the value of the parameter.\n\n    Args:\n        x: Input tensor (required for 'mlp' mode).\n\n    Returns:\n        The parameter value.\n    \"\"\"\n    if self.mode == \"scalar\":\n        return self.value if x is None else self.value.expand_as(x)\n    else:\n        assert x is not None, \"Function-valued parameter requires input\"\n        return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"reference/pinn/core/nn/#pinn.core.nn.get_activation","title":"<code>get_activation(name: Activations) -&gt; nn.Module</code>","text":"<p>Get the activation function module by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Activations</code> <p>The name of the activation function.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The PyTorch activation module.</p> Source code in <code>src/pinn/core/nn.py</code> <pre><code>def get_activation(name: Activations) -&gt; nn.Module:\n    \"\"\"\n    Get the activation function module by name.\n\n    Args:\n        name: The name of the activation function.\n\n    Returns:\n        The PyTorch activation module.\n    \"\"\"\n    return {\n        \"tanh\": nn.Tanh(),\n        \"relu\": nn.ReLU(),\n        \"leaky_relu\": nn.LeakyReLU(),\n        \"sigmoid\": nn.Sigmoid(),\n        \"selu\": nn.SELU(),\n        \"softplus\": nn.Softplus(),\n        \"identity\": nn.Identity(),\n    }[name]\n</code></pre>"},{"location":"reference/pinn/core/problem/","title":"problem","text":""},{"location":"reference/pinn/core/problem/#pinn.core.problem","title":"<code>pinn.core.problem</code>","text":"<p>Core problem abstractions for PINN.</p>"},{"location":"reference/pinn/core/problem/#pinn.core.problem.Constraint","title":"<code>Constraint</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for a constraint (loss term) in the PINN. Returns a loss value for the given batch.</p> Source code in <code>src/pinn/core/problem.py</code> <pre><code>class Constraint(ABC):\n    \"\"\"\n    Abstract base class for a constraint (loss term) in the PINN.\n    Returns a loss value for the given batch.\n    \"\"\"\n\n    def inject_context(self, context: InferredContext) -&gt; None:\n        \"\"\"\n        Inject the context into the constraint. This can be used by the constraint to access the\n        data used to compute the loss.\n\n        Args:\n            context: The context to inject.\n        \"\"\"\n        return None\n\n    @abstractmethod\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        \"\"\"\n        Calculate the loss for this constraint.\n\n        Args:\n            batch: The current batch of data/collocation points.\n            criterion: The loss function (e.g. MSE).\n            log: Optional logging function.\n\n        Returns:\n            The calculated loss tensor.\n        \"\"\"\n</code></pre>"},{"location":"reference/pinn/core/problem/#pinn.core.problem.Constraint.inject_context","title":"<code>inject_context(context: InferredContext) -&gt; None</code>","text":"<p>Inject the context into the constraint. This can be used by the constraint to access the data used to compute the loss.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>InferredContext</code> <p>The context to inject.</p> required Source code in <code>src/pinn/core/problem.py</code> <pre><code>def inject_context(self, context: InferredContext) -&gt; None:\n    \"\"\"\n    Inject the context into the constraint. This can be used by the constraint to access the\n    data used to compute the loss.\n\n    Args:\n        context: The context to inject.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/pinn/core/problem/#pinn.core.problem.Constraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>  <code>abstractmethod</code>","text":"<p>Calculate the loss for this constraint.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>TrainingBatch</code> <p>The current batch of data/collocation points.</p> required <code>criterion</code> <code>Module</code> <p>The loss function (e.g. MSE).</p> required <code>log</code> <code>LogFn | None</code> <p>Optional logging function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The calculated loss tensor.</p> Source code in <code>src/pinn/core/problem.py</code> <pre><code>@abstractmethod\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    \"\"\"\n    Calculate the loss for this constraint.\n\n    Args:\n        batch: The current batch of data/collocation points.\n        criterion: The loss function (e.g. MSE).\n        log: Optional logging function.\n\n    Returns:\n        The calculated loss tensor.\n    \"\"\"\n</code></pre>"},{"location":"reference/pinn/core/problem/#pinn.core.problem.Problem","title":"<code>Problem</code>","text":"<p>               Bases: <code>Module</code></p> <p>Aggregates operator residuals and constraints into total loss. Manages fields, parameters, constraints, and validation.</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>list[Constraint]</code> <p>List of constraints to enforce.</p> required <code>criterion</code> <code>Module</code> <p>Loss function module.</p> required <code>fields</code> <code>FieldsRegistry</code> <p>List of fields (neural networks) to solve for.</p> required <code>params</code> <code>ParamsRegistry</code> <p>List of learnable parameters.</p> required Source code in <code>src/pinn/core/problem.py</code> <pre><code>class Problem(nn.Module):\n    \"\"\"\n    Aggregates operator residuals and constraints into total loss.\n    Manages fields, parameters, constraints, and validation.\n\n    Args:\n        constraints: List of constraints to enforce.\n        criterion: Loss function module.\n        fields: List of fields (neural networks) to solve for.\n        params: List of learnable parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        constraints: list[Constraint],\n        criterion: nn.Module,\n        fields: FieldsRegistry,\n        params: ParamsRegistry,\n    ):\n        super().__init__()\n        self.constraints = constraints\n        self.criterion = criterion\n        self.fields = fields\n        self.params = params\n\n        self._fields = nn.ModuleList(fields.values())\n        self._params = nn.ModuleList(params.values())\n\n    def inject_context(self, context: InferredContext) -&gt; None:\n        \"\"\"\n        Inject the context into the problem.\n\n        This should be called after data is loaded but before training starts.\n        Pure function entries are passed through unchanged.\n\n        Args:\n            context: The context to inject.\n        \"\"\"\n        self.context = context\n        for c in self.constraints:\n            c.inject_context(context)\n\n    def training_loss(self, batch: TrainingBatch, log: LogFn | None = None) -&gt; Tensor:\n        \"\"\"\n        Calculate the total loss from all constraints.\n\n        Args:\n            batch: Current batch.\n            log: Optional logging function.\n\n        Returns:\n            Sum of losses from all constraints.\n        \"\"\"\n        _, x_coll = batch\n\n        total = torch.tensor(0.0, device=x_coll.device)\n        for c in self.constraints:\n            total = total + c.loss(batch, self.criterion, log)\n\n        if log is not None:\n            for name, param in self.params.items():\n                param_loss = self._param_validation_loss(name, param, x_coll)\n                if param_loss is not None:\n                    log(f\"loss/{name}\", param_loss, progress_bar=True)\n\n            log(LOSS_KEY, total, progress_bar=True)\n\n        return total\n\n    def predict(self, batch: DataBatch) -&gt; tuple[DataBatch, dict[str, Tensor]]:\n        \"\"\"\n        Generate predictions for a given batch of data.\n        Returns unscaled predictions in original domain.\n\n        Args:\n            batch: Batch of input coordinates.\n\n        Returns:\n            Tuple of (original_batch, predictions_dict).\n        \"\"\"\n\n        x, y = batch\n\n        preds = {name: f(x).squeeze(-1) for name, f in self.fields.items()}\n        preds |= {name: p(x).squeeze(-1) for name, p in self.params.items()}\n\n        return (x.squeeze(-1), y.squeeze(-1)), preds\n\n    def true_values(self, x: Tensor) -&gt; dict[str, Tensor] | None:\n        \"\"\"\n        Get the true values for a given x coordinates.\n        Returns None if no validation source is configured.\n        \"\"\"\n\n        return {\n            name: p_true.squeeze(-1)\n            for name, p in self.params.items()\n            if (p_true := self._get_true_param(name, x)) is not None\n        } or None\n\n    def _get_true_param(self, param_name: str, x: Tensor) -&gt; Tensor | None:\n        \"\"\"\n        Get the ground truth values for a parameter at given coordinates.\n\n        Args:\n            param_name: Name of the parameter.\n            x: Input coordinates.\n\n        Returns:\n            Ground truth values, or None if no validation source is configured.\n        \"\"\"\n        if param_name not in self.context.validation:\n            return None\n\n        return self.context.validation[param_name](x)\n\n    def _param_validation_loss(\n        self, param_name: str, param: Parameter, x_coll: Tensor\n    ) -&gt; Tensor | None:\n        \"\"\"\n        Compute validation loss for a parameter against ground truth.\n\n        Args:\n            param: The parameter to compute validation loss for.\n            x_coll: The input coordinates.\n\n        Returns:\n            Loss value, or None if no validation source is configured.\n        \"\"\"\n        true = self._get_true_param(param_name, x_coll)\n        if true is None:\n            return None\n\n        pred = param(x_coll)\n\n        return torch.mean((true - pred) ** 2)\n</code></pre>"},{"location":"reference/pinn/core/problem/#pinn.core.problem.Problem.constraints","title":"<code>constraints = constraints</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/problem/#pinn.core.problem.Problem.criterion","title":"<code>criterion = criterion</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/problem/#pinn.core.problem.Problem.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/problem/#pinn.core.problem.Problem.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/problem/#pinn.core.problem.Problem.__init__","title":"<code>__init__(constraints: list[Constraint], criterion: nn.Module, fields: FieldsRegistry, params: ParamsRegistry)</code>","text":"Source code in <code>src/pinn/core/problem.py</code> <pre><code>def __init__(\n    self,\n    constraints: list[Constraint],\n    criterion: nn.Module,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n):\n    super().__init__()\n    self.constraints = constraints\n    self.criterion = criterion\n    self.fields = fields\n    self.params = params\n\n    self._fields = nn.ModuleList(fields.values())\n    self._params = nn.ModuleList(params.values())\n</code></pre>"},{"location":"reference/pinn/core/problem/#pinn.core.problem.Problem.inject_context","title":"<code>inject_context(context: InferredContext) -&gt; None</code>","text":"<p>Inject the context into the problem.</p> <p>This should be called after data is loaded but before training starts. Pure function entries are passed through unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>InferredContext</code> <p>The context to inject.</p> required Source code in <code>src/pinn/core/problem.py</code> <pre><code>def inject_context(self, context: InferredContext) -&gt; None:\n    \"\"\"\n    Inject the context into the problem.\n\n    This should be called after data is loaded but before training starts.\n    Pure function entries are passed through unchanged.\n\n    Args:\n        context: The context to inject.\n    \"\"\"\n    self.context = context\n    for c in self.constraints:\n        c.inject_context(context)\n</code></pre>"},{"location":"reference/pinn/core/problem/#pinn.core.problem.Problem.predict","title":"<code>predict(batch: DataBatch) -&gt; tuple[DataBatch, dict[str, Tensor]]</code>","text":"<p>Generate predictions for a given batch of data. Returns unscaled predictions in original domain.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>DataBatch</code> <p>Batch of input coordinates.</p> required <p>Returns:</p> Type Description <code>tuple[DataBatch, dict[str, Tensor]]</code> <p>Tuple of (original_batch, predictions_dict).</p> Source code in <code>src/pinn/core/problem.py</code> <pre><code>def predict(self, batch: DataBatch) -&gt; tuple[DataBatch, dict[str, Tensor]]:\n    \"\"\"\n    Generate predictions for a given batch of data.\n    Returns unscaled predictions in original domain.\n\n    Args:\n        batch: Batch of input coordinates.\n\n    Returns:\n        Tuple of (original_batch, predictions_dict).\n    \"\"\"\n\n    x, y = batch\n\n    preds = {name: f(x).squeeze(-1) for name, f in self.fields.items()}\n    preds |= {name: p(x).squeeze(-1) for name, p in self.params.items()}\n\n    return (x.squeeze(-1), y.squeeze(-1)), preds\n</code></pre>"},{"location":"reference/pinn/core/problem/#pinn.core.problem.Problem.training_loss","title":"<code>training_loss(batch: TrainingBatch, log: LogFn | None = None) -&gt; Tensor</code>","text":"<p>Calculate the total loss from all constraints.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>TrainingBatch</code> <p>Current batch.</p> required <code>log</code> <code>LogFn | None</code> <p>Optional logging function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Sum of losses from all constraints.</p> Source code in <code>src/pinn/core/problem.py</code> <pre><code>def training_loss(self, batch: TrainingBatch, log: LogFn | None = None) -&gt; Tensor:\n    \"\"\"\n    Calculate the total loss from all constraints.\n\n    Args:\n        batch: Current batch.\n        log: Optional logging function.\n\n    Returns:\n        Sum of losses from all constraints.\n    \"\"\"\n    _, x_coll = batch\n\n    total = torch.tensor(0.0, device=x_coll.device)\n    for c in self.constraints:\n        total = total + c.loss(batch, self.criterion, log)\n\n    if log is not None:\n        for name, param in self.params.items():\n            param_loss = self._param_validation_loss(name, param, x_coll)\n            if param_loss is not None:\n                log(f\"loss/{name}\", param_loss, progress_bar=True)\n\n        log(LOSS_KEY, total, progress_bar=True)\n\n    return total\n</code></pre>"},{"location":"reference/pinn/core/problem/#pinn.core.problem.Problem.true_values","title":"<code>true_values(x: Tensor) -&gt; dict[str, Tensor] | None</code>","text":"<p>Get the true values for a given x coordinates. Returns None if no validation source is configured.</p> Source code in <code>src/pinn/core/problem.py</code> <pre><code>def true_values(self, x: Tensor) -&gt; dict[str, Tensor] | None:\n    \"\"\"\n    Get the true values for a given x coordinates.\n    Returns None if no validation source is configured.\n    \"\"\"\n\n    return {\n        name: p_true.squeeze(-1)\n        for name, p in self.params.items()\n        if (p_true := self._get_true_param(name, x)) is not None\n    } or None\n</code></pre>"},{"location":"reference/pinn/core/types/","title":"types","text":""},{"location":"reference/pinn/core/types/#pinn.core.types","title":"<code>pinn.core.types</code>","text":"<p>Core type aliases, constants, and protocols for PINN.</p>"},{"location":"reference/pinn/core/types/#pinn.core.types.Activations","title":"<code>Activations: TypeAlias = Literal['tanh', 'relu', 'leaky_relu', 'sigmoid', 'selu', 'softplus', 'identity']</code>  <code>module-attribute</code>","text":"<p>Supported activation functions.</p>"},{"location":"reference/pinn/core/types/#pinn.core.types.DataBatch","title":"<code>DataBatch: TypeAlias = tuple[Tensor, Tensor]</code>  <code>module-attribute</code>","text":"<p>Type alias for data batch: (x, y).</p>"},{"location":"reference/pinn/core/types/#pinn.core.types.LOSS_KEY","title":"<code>LOSS_KEY = 'loss'</code>  <code>module-attribute</code>","text":"<p>Key used for logging the total loss.</p>"},{"location":"reference/pinn/core/types/#pinn.core.types.PredictionBatch","title":"<code>PredictionBatch: TypeAlias = tuple[Tensor, Tensor]</code>  <code>module-attribute</code>","text":"<p>Prediction batch tuple: (x_data, y_data).</p>"},{"location":"reference/pinn/core/types/#pinn.core.types.Predictions","title":"<code>Predictions: TypeAlias = tuple[DataBatch, dict[str, Tensor], dict[str, Tensor] | None]</code>  <code>module-attribute</code>","text":"<p>Type alias for model predictions: (input_batch, predictions_dictionary, true_values_dictionary)  where predictions_dictionary is a dictionary of {[field_name | param_name]: prediction} and where true_values_dictionary is a dictionary of {[field_name | param_name]: true_value}. If no validation source is configured, true_values_dictionary is None.</p>"},{"location":"reference/pinn/core/types/#pinn.core.types.TrainingBatch","title":"<code>TrainingBatch: TypeAlias = tuple[DataBatch, Tensor]</code>  <code>module-attribute</code>","text":"<p>Training batch tuple: ((x_data, y_data), x_coll).</p>"},{"location":"reference/pinn/core/types/#pinn.core.types.LogFn","title":"<code>LogFn</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>A function that logs a value to a dictionary.</p> Source code in <code>src/pinn/core/types.py</code> <pre><code>class LogFn(Protocol):\n    \"\"\"\n    A function that logs a value to a dictionary.\n    \"\"\"\n\n    def __call__(self, name: str, value: Tensor, progress_bar: bool = False) -&gt; None:\n        \"\"\"\n        Log a value.\n\n        Args:\n            name: The name to log the value under.\n            value: The value to log.\n            progress_bar: Whether the value should be logged to the progress bar.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/pinn/core/types/#pinn.core.types.LogFn.__call__","title":"<code>__call__(name: str, value: Tensor, progress_bar: bool = False) -&gt; None</code>","text":"<p>Log a value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to log the value under.</p> required <code>value</code> <code>Tensor</code> <p>The value to log.</p> required <code>progress_bar</code> <code>bool</code> <p>Whether the value should be logged to the progress bar.</p> <code>False</code> Source code in <code>src/pinn/core/types.py</code> <pre><code>def __call__(self, name: str, value: Tensor, progress_bar: bool = False) -&gt; None:\n    \"\"\"\n    Log a value.\n\n    Args:\n        name: The name to log the value under.\n        value: The value to log.\n        progress_bar: Whether the value should be logged to the progress bar.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/pinn/core/validation/","title":"validation","text":""},{"location":"reference/pinn/core/validation/#pinn.core.validation","title":"<code>pinn.core.validation</code>","text":"<p>Validation registry for ground truth comparison during training and prediction.</p>"},{"location":"reference/pinn/core/validation/#pinn.core.validation.ResolvedValidation","title":"<code>ResolvedValidation: TypeAlias = dict[str, Callable[[Tensor], Tensor]]</code>  <code>module-attribute</code>","text":"<p>Validation registry after ColumnRef entries have been resolved to callables.</p>"},{"location":"reference/pinn/core/validation/#pinn.core.validation.ValidationRegistry","title":"<code>ValidationRegistry: TypeAlias = dict[str, ValidationSource]</code>  <code>module-attribute</code>","text":"<p>Registry mapping parameter names to their validation sources.</p> Example <p>validation: ValidationRegistry = { ...     \"beta\": lambda x: torch.sin(x),  # Pure function ...     \"gamma\": ColumnRef(column=\"gamma_true\"),  # From data ...     \"delta\": None,  # No validation ... }</p>"},{"location":"reference/pinn/core/validation/#pinn.core.validation.ValidationSource","title":"<code>ValidationSource: TypeAlias = Callable[[Tensor], Tensor] | ColumnRef | None</code>  <code>module-attribute</code>","text":"<p>A source for ground truth values. Can be: - A callable that takes x coordinates and returns true values - A ColumnRef that references a column in loaded data - None if no validation is needed for this parameter</p>"},{"location":"reference/pinn/core/validation/#pinn.core.validation.ColumnRef","title":"<code>ColumnRef</code>  <code>dataclass</code>","text":"<p>Reference to a column in loaded data for ground truth comparison.</p> <p>This allows practitioners to specify validation data by column name without writing custom functions. The column is resolved lazily when data is loaded.</p> <p>Attributes:</p> Name Type Description <code>column</code> <code>str</code> <p>Name of the column in the loaded DataFrame.</p> <code>transform</code> <code>Callable[[Tensor], Tensor] | None</code> <p>Optional transformation to apply to the column values.</p> Example <p>validation = { ...     \"beta\": ColumnRef(column=\"Rt\", transform=lambda rt: rt * delta), ... }</p> Source code in <code>src/pinn/core/validation.py</code> <pre><code>@dataclass\nclass ColumnRef:\n    \"\"\"\n    Reference to a column in loaded data for ground truth comparison.\n\n    This allows practitioners to specify validation data by column name\n    without writing custom functions. The column is resolved lazily when\n    data is loaded.\n\n    Attributes:\n        column: Name of the column in the loaded DataFrame.\n        transform: Optional transformation to apply to the column values.\n\n    Example:\n        &gt;&gt;&gt; validation = {\n        ...     \"beta\": ColumnRef(column=\"Rt\", transform=lambda rt: rt * delta),\n        ... }\n    \"\"\"\n\n    column: str\n    transform: Callable[[Tensor], Tensor] | None = None\n</code></pre>"},{"location":"reference/pinn/core/validation/#pinn.core.validation.ColumnRef.column","title":"<code>column: str</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/validation/#pinn.core.validation.ColumnRef.transform","title":"<code>transform: Callable[[Tensor], Tensor] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/core/validation/#pinn.core.validation.ColumnRef.__init__","title":"<code>__init__(column: str, transform: Callable[[Tensor], Tensor] | None = None) -&gt; None</code>","text":""},{"location":"reference/pinn/core/validation/#pinn.core.validation.resolve_validation","title":"<code>resolve_validation(registry: ValidationRegistry, df_path: Path | None = None) -&gt; ResolvedValidation</code>","text":"<p>Resolve a ValidationRegistry by converting ColumnRef entries to callables.</p> <p>Pure function entries are passed through unchanged. ColumnRef entries are resolved using the provided data file path.</p> <p>Parameters:</p> Name Type Description Default <code>registry</code> <code>ValidationRegistry</code> <p>The validation registry to resolve.</p> required <code>df_path</code> <code>Path | None</code> <p>Path to the CSV file for ColumnRef resolution.</p> <code>None</code> <p>Returns:</p> Type Description <code>ResolvedValidation</code> <p>A dictionary mapping parameter names to callable validation functions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a ColumnRef cannot be resolved (missing column or no df_path).</p> Source code in <code>src/pinn/core/validation.py</code> <pre><code>def resolve_validation(\n    registry: ValidationRegistry,\n    df_path: Path | None = None,\n) -&gt; ResolvedValidation:\n    \"\"\"\n    Resolve a ValidationRegistry by converting ColumnRef entries to callables.\n\n    Pure function entries are passed through unchanged. ColumnRef entries\n    are resolved using the provided data file path.\n\n    Args:\n        registry: The validation registry to resolve.\n        df_path: Path to the CSV file for ColumnRef resolution.\n\n    Returns:\n        A dictionary mapping parameter names to callable validation functions.\n\n    Raises:\n        ValueError: If a ColumnRef cannot be resolved (missing column or no df_path).\n    \"\"\"\n\n    resolved: ResolvedValidation = {}\n\n    for name, source in registry.items():\n        if source is None:\n            continue\n\n        if callable(source) and not isinstance(source, ColumnRef):\n            resolved[name] = source\n\n        elif isinstance(source, ColumnRef):\n            if df_path is None:\n                raise ValueError(\n                    f\"Cannot resolve ColumnRef for '{name}': no df_path provided. \"\n                    \"Either pass a df_path or use a callable instead of ColumnRef.\"\n                )\n\n            df = pd.read_csv(df_path)\n\n            if source.column not in df.columns:\n                raise ValueError(\n                    f\"Cannot resolve ColumnRef for '{name}': \"\n                    f\"column '{source.column}' not found in data. \"\n                    f\"Available columns: {list(df.columns)}\"\n                )\n\n            column_values = torch.tensor(df[source.column].values, dtype=torch.float32)\n\n            if source.transform is not None:\n                column_values = source.transform(column_values)\n\n            def make_lookup_fn(values: Tensor) -&gt; Callable[[Tensor], Tensor]:\n                def lookup(x: Tensor) -&gt; Tensor:\n                    idx = x.squeeze(-1).round().to(torch.int32)\n                    return values.to(x.device)[idx]\n\n                return lookup\n\n            resolved[name] = make_lookup_fn(column_values)\n\n    return resolved\n</code></pre>"},{"location":"reference/pinn/lib/","title":"lib","text":""},{"location":"reference/pinn/lib/#pinn.lib","title":"<code>pinn.lib</code>","text":""},{"location":"reference/pinn/lib/utils/","title":"utils","text":""},{"location":"reference/pinn/lib/utils/#pinn.lib.utils","title":"<code>pinn.lib.utils</code>","text":""},{"location":"reference/pinn/lib/utils/#pinn.lib.utils.T","title":"<code>T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/lib/utils/#pinn.lib.utils.find","title":"<code>find(iterable: Iterable[T], predicate: Callable[[T], bool], default: T | None = None) -&gt; T | None</code>","text":"<p>Find the first element in an iterable that satisfies a predicate.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable[T]</code> <p>The iterable to search.</p> required <code>predicate</code> <code>Callable[[T], bool]</code> <p>A function that returns True for the desired element.</p> required <code>default</code> <code>T | None</code> <p>The value to return if no element is found. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>T | None</code> <p>The first matching element, or the default value.</p> Source code in <code>src/pinn/lib/utils.py</code> <pre><code>def find(\n    iterable: Iterable[T],\n    predicate: Callable[[T], bool],\n    default: T | None = None,\n) -&gt; T | None:\n    \"\"\"\n    Find the first element in an iterable that satisfies a predicate.\n\n    Args:\n        iterable: The iterable to search.\n        predicate: A function that returns True for the desired element.\n        default: The value to return if no element is found. Defaults to None.\n\n    Returns:\n        The first matching element, or the default value.\n    \"\"\"\n    return next((x for x in iterable if predicate(x)), default)\n</code></pre>"},{"location":"reference/pinn/lib/utils/#pinn.lib.utils.find_or_raise","title":"<code>find_or_raise(iterable: Iterable[T], predicate: Callable[[T], bool], exception: Exception | Callable[[], Exception] | None = None) -&gt; T</code>","text":"<p>Find the first element in an iterable that satisfies a predicate, or raise an exception.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable[T]</code> <p>The iterable to search.</p> required <code>predicate</code> <code>Callable[[T], bool]</code> <p>A function that returns True for the desired element.</p> required <code>exception</code> <code>Exception | Callable[[], Exception] | None</code> <p>The exception to raise if no element is found.        Can be an Exception instance, a callable returning an Exception,        or None (raises ValueError).</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>The first matching element.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no element is found and no specific exception is provided.</p> <code>Exception</code> <p>The provided exception if no element is found.</p> Source code in <code>src/pinn/lib/utils.py</code> <pre><code>def find_or_raise(\n    iterable: Iterable[T],\n    predicate: Callable[[T], bool],\n    exception: Exception | Callable[[], Exception] | None = None,\n) -&gt; T:\n    \"\"\"\n    Find the first element in an iterable that satisfies a predicate, or raise an exception.\n\n    Args:\n        iterable: The iterable to search.\n        predicate: A function that returns True for the desired element.\n        exception: The exception to raise if no element is found.\n                   Can be an Exception instance, a callable returning an Exception,\n                   or None (raises ValueError).\n\n    Returns:\n        The first matching element.\n\n    Raises:\n        ValueError: If no element is found and no specific exception is provided.\n        Exception: The provided exception if no element is found.\n    \"\"\"\n    found = find(iterable, predicate)\n    if found is not None:\n        return found\n\n    if exception is None:\n        raise ValueError(\"Element not found\")\n    if isinstance(exception, Exception):\n        raise exception\n    raise exception()\n</code></pre>"},{"location":"reference/pinn/lib/utils/#pinn.lib.utils.get_tensorboard_logger","title":"<code>get_tensorboard_logger(trainer: Trainer, default: TensorBoardLogger | None = None) -&gt; TensorBoardLogger | None</code>","text":"<p>Retrieve the TensorBoardLogger from the trainer.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The PyTorch Lightning Trainer instance.</p> required <code>default</code> <code>TensorBoardLogger | None</code> <p>Default value if not found.</p> <code>None</code> <p>Returns:</p> Type Description <code>TensorBoardLogger | None</code> <p>The TensorBoardLogger or the default value.</p> Source code in <code>src/pinn/lib/utils.py</code> <pre><code>def get_tensorboard_logger(\n    trainer: Trainer,\n    default: TensorBoardLogger | None = None,\n) -&gt; TensorBoardLogger | None:\n    \"\"\"\n    Retrieve the TensorBoardLogger from the trainer.\n\n    Args:\n        trainer: The PyTorch Lightning Trainer instance.\n        default: Default value if not found.\n\n    Returns:\n        The TensorBoardLogger or the default value.\n    \"\"\"\n    return cast(\n        TensorBoardLogger | None,\n        find(\n            trainer.loggers,\n            lambda l: isinstance(l, TensorBoardLogger),\n            default,\n        ),\n    )\n</code></pre>"},{"location":"reference/pinn/lib/utils/#pinn.lib.utils.get_tensorboard_logger_or_raise","title":"<code>get_tensorboard_logger_or_raise(trainer: Trainer) -&gt; TensorBoardLogger</code>","text":"<p>Retrieve the TensorBoardLogger from the trainer, or raise if not present.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The PyTorch Lightning Trainer instance.</p> required <p>Returns:</p> Type Description <code>TensorBoardLogger</code> <p>The TensorBoardLogger.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no TensorBoardLogger is attached to the trainer.</p> Source code in <code>src/pinn/lib/utils.py</code> <pre><code>def get_tensorboard_logger_or_raise(trainer: Trainer) -&gt; TensorBoardLogger:\n    \"\"\"\n    Retrieve the TensorBoardLogger from the trainer, or raise if not present.\n\n    Args:\n        trainer: The PyTorch Lightning Trainer instance.\n\n    Returns:\n        The TensorBoardLogger.\n\n    Raises:\n        ValueError: If no TensorBoardLogger is attached to the trainer.\n    \"\"\"\n    return cast(\n        TensorBoardLogger,\n        find_or_raise(\n            trainer.loggers,\n            lambda l: isinstance(l, TensorBoardLogger),\n            ValueError(\"TensorBoard logger not found\"),\n        ),\n    )\n</code></pre>"},{"location":"reference/pinn/lightning/","title":"lightning","text":""},{"location":"reference/pinn/lightning/#pinn.lightning","title":"<code>pinn.lightning</code>","text":"<p>Lightning integration for PINN training.</p>"},{"location":"reference/pinn/lightning/#pinn.lightning.__all__","title":"<code>__all__ = ['FormattedProgressBar', 'PINNModule', 'PredictionsWriter', 'SMMAStopping']</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/lightning/#pinn.lightning.FormattedProgressBar","title":"<code>FormattedProgressBar</code>","text":"<p>               Bases: <code>TQDMProgressBar</code></p> <p>Custom progress bar for training that formats metrics for better readability.</p> <p>This class extends the TQDMProgressBar to provide custom formatting for training metrics, particularly for the total loss and beta values.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>FormatFn</code> <p>Function to format the metric values.</p> required Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>class FormattedProgressBar(TQDMProgressBar):\n    \"\"\"\n    Custom progress bar for training that formats metrics for better readability.\n\n    This class extends the TQDMProgressBar to provide custom formatting for\n    training metrics, particularly for the total loss and beta values.\n\n    Args:\n        format: Function to format the metric values.\n    \"\"\"\n\n    def __init__(self, *args: Any, format: FormatFn, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n        self.format = format\n\n    @override\n    def get_metrics(self, *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n        \"\"\"\n        Format metrics for display in the progress bar.\n\n        Returns:\n            Dictionary of formatted metrics with:\n            - Total loss in scientific notation\n            - Beta value with 4 decimal places\n            - Other metrics as provided by the parent class\n        \"\"\"\n        items = super().get_metrics(*args, **kwargs)\n        items.pop(\"v_num\", None)\n        for key, value in items.items():\n            items[key] = self.format(key, value)\n\n        return items\n</code></pre>"},{"location":"reference/pinn/lightning/#pinn.lightning.FormattedProgressBar.format","title":"<code>format = format</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/#pinn.lightning.FormattedProgressBar.__init__","title":"<code>__init__(*args: Any, format: FormatFn, **kwargs: Any)</code>","text":"Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>def __init__(self, *args: Any, format: FormatFn, **kwargs: Any):\n    super().__init__(*args, **kwargs)\n    self.format = format\n</code></pre>"},{"location":"reference/pinn/lightning/#pinn.lightning.FormattedProgressBar.get_metrics","title":"<code>get_metrics(*args: Any, **kwargs: Any) -&gt; dict[str, Any]</code>","text":"<p>Format metrics for display in the progress bar.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of formatted metrics with:</p> <code>dict[str, Any]</code> <ul> <li>Total loss in scientific notation</li> </ul> <code>dict[str, Any]</code> <ul> <li>Beta value with 4 decimal places</li> </ul> <code>dict[str, Any]</code> <ul> <li>Other metrics as provided by the parent class</li> </ul> Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>@override\ndef get_metrics(self, *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"\n    Format metrics for display in the progress bar.\n\n    Returns:\n        Dictionary of formatted metrics with:\n        - Total loss in scientific notation\n        - Beta value with 4 decimal places\n        - Other metrics as provided by the parent class\n    \"\"\"\n    items = super().get_metrics(*args, **kwargs)\n    items.pop(\"v_num\", None)\n    for key, value in items.items():\n        items[key] = self.format(key, value)\n\n    return items\n</code></pre>"},{"location":"reference/pinn/lightning/#pinn.lightning.PINNModule","title":"<code>PINNModule</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Generic PINN Lightning module. Expects external Problem + Sampler + optimizer config.</p> <p>Parameters:</p> Name Type Description Default <code>problem</code> <code>Problem</code> <p>The PINN problem definition (constraints, fields, etc.).</p> required <code>hp</code> <code>PINNHyperparameters</code> <p>Hyperparameters for training.</p> required Source code in <code>src/pinn/lightning/module.py</code> <pre><code>class PINNModule(pl.LightningModule):\n    \"\"\"\n    Generic PINN Lightning module.\n    Expects external Problem + Sampler + optimizer config.\n\n    Args:\n        problem: The PINN problem definition (constraints, fields, etc.).\n        hp: Hyperparameters for training.\n    \"\"\"\n\n    def __init__(\n        self,\n        problem: Problem,\n        hp: PINNHyperparameters,\n    ):\n        super().__init__()\n        self.save_hyperparameters(ignore=[\"problem\"])\n\n        self.problem = problem\n        self.hp = hp\n        self.scheduler = hp.scheduler\n\n        def _log(key: str, value: Tensor, progress_bar: bool = False) -&gt; None:\n            self.log(\n                key,\n                value,\n                on_step=False,\n                on_epoch=True,\n                prog_bar=progress_bar,\n                batch_size=hp.training_data.batch_size,\n            )\n\n        self._log = cast(LogFn, _log)\n\n    @override\n    def on_fit_start(self) -&gt; None:\n        \"\"\"\n        Called when fit begins. Resolves validation sources using loaded data.\n        \"\"\"\n        self.problem.inject_context(self.trainer.datamodule.context)  # type: ignore\n\n    @override\n    def on_predict_start(self) -&gt; None:\n        \"\"\"\n        Called when predict begins. Resolves validation sources using loaded data.\n        \"\"\"\n        self.problem.inject_context(self.trainer.datamodule.context)  # type: ignore\n\n    @override\n    def training_step(self, batch: TrainingBatch, batch_idx: int) -&gt; Tensor:\n        \"\"\"\n        Performs a single training step.\n        Calculates total loss from the problem.\n        \"\"\"\n        return self.problem.training_loss(batch, self._log)\n\n    @override\n    def predict_step(self, batch: PredictionBatch, batch_idx: int) -&gt; Predictions:\n        \"\"\"\n        Performs a prediction step.\n        \"\"\"\n        x_data, y_data = batch\n\n        (data_batch, predictions) = self.problem.predict((x_data, y_data))\n        true_values = self.problem.true_values(x_data)\n\n        return (data_batch, predictions, true_values)\n\n    @override\n    def configure_optimizers(self) -&gt; OptimizerLRScheduler:\n        \"\"\"\n        Configures the optimizer and learning rate scheduler.\n        \"\"\"\n        opt = torch.optim.Adam(self.parameters(), lr=self.hp.lr)\n        if not self.scheduler:\n            return opt\n\n        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            opt,\n            mode=self.scheduler.mode,\n            factor=self.scheduler.factor,\n            patience=self.scheduler.patience,\n            threshold=self.scheduler.threshold,\n            min_lr=self.scheduler.min_lr,\n        )\n\n        return {\n            \"optimizer\": opt,\n            \"lr_scheduler\": {\n                \"name\": \"lr\",\n                \"scheduler\": sch,\n                \"monitor\": LOSS_KEY,\n                \"interval\": \"epoch\",\n                \"frequency\": 1,\n            },\n        }\n</code></pre>"},{"location":"reference/pinn/lightning/#pinn.lightning.PINNModule.hp","title":"<code>hp = hp</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/#pinn.lightning.PINNModule.problem","title":"<code>problem = problem</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/#pinn.lightning.PINNModule.scheduler","title":"<code>scheduler = hp.scheduler</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/#pinn.lightning.PINNModule.__init__","title":"<code>__init__(problem: Problem, hp: PINNHyperparameters)</code>","text":"Source code in <code>src/pinn/lightning/module.py</code> <pre><code>def __init__(\n    self,\n    problem: Problem,\n    hp: PINNHyperparameters,\n):\n    super().__init__()\n    self.save_hyperparameters(ignore=[\"problem\"])\n\n    self.problem = problem\n    self.hp = hp\n    self.scheduler = hp.scheduler\n\n    def _log(key: str, value: Tensor, progress_bar: bool = False) -&gt; None:\n        self.log(\n            key,\n            value,\n            on_step=False,\n            on_epoch=True,\n            prog_bar=progress_bar,\n            batch_size=hp.training_data.batch_size,\n        )\n\n    self._log = cast(LogFn, _log)\n</code></pre>"},{"location":"reference/pinn/lightning/#pinn.lightning.PINNModule.configure_optimizers","title":"<code>configure_optimizers() -&gt; OptimizerLRScheduler</code>","text":"<p>Configures the optimizer and learning rate scheduler.</p> Source code in <code>src/pinn/lightning/module.py</code> <pre><code>@override\ndef configure_optimizers(self) -&gt; OptimizerLRScheduler:\n    \"\"\"\n    Configures the optimizer and learning rate scheduler.\n    \"\"\"\n    opt = torch.optim.Adam(self.parameters(), lr=self.hp.lr)\n    if not self.scheduler:\n        return opt\n\n    sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        opt,\n        mode=self.scheduler.mode,\n        factor=self.scheduler.factor,\n        patience=self.scheduler.patience,\n        threshold=self.scheduler.threshold,\n        min_lr=self.scheduler.min_lr,\n    )\n\n    return {\n        \"optimizer\": opt,\n        \"lr_scheduler\": {\n            \"name\": \"lr\",\n            \"scheduler\": sch,\n            \"monitor\": LOSS_KEY,\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        },\n    }\n</code></pre>"},{"location":"reference/pinn/lightning/#pinn.lightning.PINNModule.on_fit_start","title":"<code>on_fit_start() -&gt; None</code>","text":"<p>Called when fit begins. Resolves validation sources using loaded data.</p> Source code in <code>src/pinn/lightning/module.py</code> <pre><code>@override\ndef on_fit_start(self) -&gt; None:\n    \"\"\"\n    Called when fit begins. Resolves validation sources using loaded data.\n    \"\"\"\n    self.problem.inject_context(self.trainer.datamodule.context)  # type: ignore\n</code></pre>"},{"location":"reference/pinn/lightning/#pinn.lightning.PINNModule.on_predict_start","title":"<code>on_predict_start() -&gt; None</code>","text":"<p>Called when predict begins. Resolves validation sources using loaded data.</p> Source code in <code>src/pinn/lightning/module.py</code> <pre><code>@override\ndef on_predict_start(self) -&gt; None:\n    \"\"\"\n    Called when predict begins. Resolves validation sources using loaded data.\n    \"\"\"\n    self.problem.inject_context(self.trainer.datamodule.context)  # type: ignore\n</code></pre>"},{"location":"reference/pinn/lightning/#pinn.lightning.PINNModule.predict_step","title":"<code>predict_step(batch: PredictionBatch, batch_idx: int) -&gt; Predictions</code>","text":"<p>Performs a prediction step.</p> Source code in <code>src/pinn/lightning/module.py</code> <pre><code>@override\ndef predict_step(self, batch: PredictionBatch, batch_idx: int) -&gt; Predictions:\n    \"\"\"\n    Performs a prediction step.\n    \"\"\"\n    x_data, y_data = batch\n\n    (data_batch, predictions) = self.problem.predict((x_data, y_data))\n    true_values = self.problem.true_values(x_data)\n\n    return (data_batch, predictions, true_values)\n</code></pre>"},{"location":"reference/pinn/lightning/#pinn.lightning.PINNModule.training_step","title":"<code>training_step(batch: TrainingBatch, batch_idx: int) -&gt; Tensor</code>","text":"<p>Performs a single training step. Calculates total loss from the problem.</p> Source code in <code>src/pinn/lightning/module.py</code> <pre><code>@override\ndef training_step(self, batch: TrainingBatch, batch_idx: int) -&gt; Tensor:\n    \"\"\"\n    Performs a single training step.\n    Calculates total loss from the problem.\n    \"\"\"\n    return self.problem.training_loss(batch, self._log)\n</code></pre>"},{"location":"reference/pinn/lightning/#pinn.lightning.PredictionsWriter","title":"<code>PredictionsWriter</code>","text":"<p>               Bases: <code>BasePredictionWriter</code></p> <p>Callback to write predictions to disk at the end of an epoch.</p> <p>Parameters:</p> Name Type Description Default <code>predictions_path</code> <code>Path | None</code> <p>Path to save the predictions tensor/object.</p> <code>None</code> <code>batch_indices_path</code> <code>Path | None</code> <p>Path to save the batch indices.</p> <code>None</code> <code>on_prediction</code> <code>HookFn | None</code> <p>Optional hook function called when predictions are ready.</p> <code>None</code> <code>write_interval</code> <code>Literal['batch', 'epoch', 'batch_and_epoch']</code> <p>Interval to write predictions (\"batch\", \"epoch\", \"batch_and_epoch\").</p> <code>'epoch'</code> Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>class PredictionsWriter(BasePredictionWriter):\n    \"\"\"\n    Callback to write predictions to disk at the end of an epoch.\n\n    Args:\n        predictions_path: Path to save the predictions tensor/object.\n        batch_indices_path: Path to save the batch indices.\n        on_prediction: Optional hook function called when predictions are ready.\n        write_interval: Interval to write predictions (\"batch\", \"epoch\", \"batch_and_epoch\").\n    \"\"\"\n\n    def __init__(\n        self,\n        predictions_path: Path | None = None,\n        batch_indices_path: Path | None = None,\n        on_prediction: HookFn | None = None,\n        write_interval: Literal[\"batch\", \"epoch\", \"batch_and_epoch\"] = \"epoch\",\n    ):\n        super().__init__(write_interval)\n        self.predictions_path = predictions_path\n        self.batch_indices_path = batch_indices_path\n        self.on_prediction = on_prediction\n\n    @override\n    def write_on_epoch_end(\n        self,\n        trainer: Trainer,\n        module: LightningModule,\n        predictions_list: Sequence[Predictions],\n        batch_indices: Sequence[Any],\n    ) -&gt; None:\n        \"\"\"\n        Writes predictions to disk or calls the hook at the end of the epoch.\n        \"\"\"\n        if self.on_prediction is not None:\n            self.on_prediction(trainer, module, predictions_list, batch_indices)\n\n        if self.predictions_path is not None:\n            torch.save(predictions_list, self.predictions_path)\n\n        if self.batch_indices_path is not None:\n            torch.save(batch_indices, self.batch_indices_path)\n</code></pre>"},{"location":"reference/pinn/lightning/#pinn.lightning.PredictionsWriter.batch_indices_path","title":"<code>batch_indices_path = batch_indices_path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/#pinn.lightning.PredictionsWriter.on_prediction","title":"<code>on_prediction = on_prediction</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/#pinn.lightning.PredictionsWriter.predictions_path","title":"<code>predictions_path = predictions_path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/#pinn.lightning.PredictionsWriter.__init__","title":"<code>__init__(predictions_path: Path | None = None, batch_indices_path: Path | None = None, on_prediction: HookFn | None = None, write_interval: Literal['batch', 'epoch', 'batch_and_epoch'] = 'epoch')</code>","text":"Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>def __init__(\n    self,\n    predictions_path: Path | None = None,\n    batch_indices_path: Path | None = None,\n    on_prediction: HookFn | None = None,\n    write_interval: Literal[\"batch\", \"epoch\", \"batch_and_epoch\"] = \"epoch\",\n):\n    super().__init__(write_interval)\n    self.predictions_path = predictions_path\n    self.batch_indices_path = batch_indices_path\n    self.on_prediction = on_prediction\n</code></pre>"},{"location":"reference/pinn/lightning/#pinn.lightning.PredictionsWriter.write_on_epoch_end","title":"<code>write_on_epoch_end(trainer: Trainer, module: LightningModule, predictions_list: Sequence[Predictions], batch_indices: Sequence[Any]) -&gt; None</code>","text":"<p>Writes predictions to disk or calls the hook at the end of the epoch.</p> Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>@override\ndef write_on_epoch_end(\n    self,\n    trainer: Trainer,\n    module: LightningModule,\n    predictions_list: Sequence[Predictions],\n    batch_indices: Sequence[Any],\n) -&gt; None:\n    \"\"\"\n    Writes predictions to disk or calls the hook at the end of the epoch.\n    \"\"\"\n    if self.on_prediction is not None:\n        self.on_prediction(trainer, module, predictions_list, batch_indices)\n\n    if self.predictions_path is not None:\n        torch.save(predictions_list, self.predictions_path)\n\n    if self.batch_indices_path is not None:\n        torch.save(batch_indices, self.batch_indices_path)\n</code></pre>"},{"location":"reference/pinn/lightning/#pinn.lightning.SMMAStopping","title":"<code>SMMAStopping</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Early stopping callback based on the Smoothed Moving Average (SMMA) of the loss. Stops training if the relative improvement of the SMMA drops below a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SMMAStoppingConfig</code> <p>Configuration for SMMA stopping (window, threshold, lookback).</p> required <code>loss_key</code> <code>str</code> <p>The metric key to monitor (e.g., 'loss').</p> required <code>log_key</code> <code>str</code> <p>Key to log the computed SMMA value.</p> <code>SMMA_KEY</code> Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>class SMMAStopping(Callback):\n    \"\"\"\n    Early stopping callback based on the Smoothed Moving Average (SMMA) of the loss.\n    Stops training if the relative improvement of the SMMA drops below a threshold.\n\n    Args:\n        config: Configuration for SMMA stopping (window, threshold, lookback).\n        loss_key: The metric key to monitor (e.g., 'loss').\n        log_key: Key to log the computed SMMA value.\n    \"\"\"\n\n    def __init__(self, config: SMMAStoppingConfig, loss_key: str, log_key: str = SMMA_KEY):\n        super().__init__()\n        self.config = config\n        self.loss_key = loss_key\n        self.log_key = log_key\n        self.loss_buffer: list[float] = []\n        self.smma_buffer: list[float] = []\n\n    @override\n    def on_train_epoch_end(self, trainer: Trainer, module: LightningModule) -&gt; None:\n        \"\"\"\n        Called when the train epoch ends. Updates SMMA and checks stopping condition.\n        \"\"\"\n        # phase 0: get the loss\n        loss_t = trainer.callback_metrics.get(self.loss_key)\n        if loss_t is None:\n            return\n\n        loss = loss_t.item()\n        n = self.config.window\n\n        # phase 1: collect first `window` losses\n        if len(self.loss_buffer) &lt;= n:\n            self.loss_buffer.append(loss)\n            return\n\n        # phase 1.5: compute the first average\n        if len(self.smma_buffer) == 0:\n            first_smma = sum(self.loss_buffer) / n\n            self.smma_buffer.append(first_smma)\n            return\n\n        # phase 2: compute the first `lookback` Smoothed Moving Average (SMMA)\n        smma = self.smma_buffer[-1]\n        smma = ((n - 1) * smma + loss) / n\n        self.smma_buffer.append(smma)\n\n        module.log(self.log_key, smma)\n        if len(self.smma_buffer) &lt; self.config.lookback:\n            return\n\n        # phase 3: compute the improvement between the current and the `lookback` SMMA\n        smma_lookback = self.smma_buffer[0]\n        improvement = (smma_lookback - smma) / smma_lookback\n        self.smma_buffer.pop(0)\n\n        if 0 &lt; improvement &lt; self.config.threshold:\n            trainer.should_stop = True\n            print(\n                f\"\\nStopping training: SMMA improvement over {self.config.lookback} \"\n                f\"epochs ({improvement:.2%}) below threshold ({self.config.threshold:.2%})\"\n            )\n</code></pre>"},{"location":"reference/pinn/lightning/#pinn.lightning.SMMAStopping.config","title":"<code>config = config</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/#pinn.lightning.SMMAStopping.log_key","title":"<code>log_key = log_key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/#pinn.lightning.SMMAStopping.loss_buffer","title":"<code>loss_buffer: list[float] = []</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/#pinn.lightning.SMMAStopping.loss_key","title":"<code>loss_key = loss_key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/#pinn.lightning.SMMAStopping.smma_buffer","title":"<code>smma_buffer: list[float] = []</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/#pinn.lightning.SMMAStopping.__init__","title":"<code>__init__(config: SMMAStoppingConfig, loss_key: str, log_key: str = SMMA_KEY)</code>","text":"Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>def __init__(self, config: SMMAStoppingConfig, loss_key: str, log_key: str = SMMA_KEY):\n    super().__init__()\n    self.config = config\n    self.loss_key = loss_key\n    self.log_key = log_key\n    self.loss_buffer: list[float] = []\n    self.smma_buffer: list[float] = []\n</code></pre>"},{"location":"reference/pinn/lightning/#pinn.lightning.SMMAStopping.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer: Trainer, module: LightningModule) -&gt; None</code>","text":"<p>Called when the train epoch ends. Updates SMMA and checks stopping condition.</p> Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>@override\ndef on_train_epoch_end(self, trainer: Trainer, module: LightningModule) -&gt; None:\n    \"\"\"\n    Called when the train epoch ends. Updates SMMA and checks stopping condition.\n    \"\"\"\n    # phase 0: get the loss\n    loss_t = trainer.callback_metrics.get(self.loss_key)\n    if loss_t is None:\n        return\n\n    loss = loss_t.item()\n    n = self.config.window\n\n    # phase 1: collect first `window` losses\n    if len(self.loss_buffer) &lt;= n:\n        self.loss_buffer.append(loss)\n        return\n\n    # phase 1.5: compute the first average\n    if len(self.smma_buffer) == 0:\n        first_smma = sum(self.loss_buffer) / n\n        self.smma_buffer.append(first_smma)\n        return\n\n    # phase 2: compute the first `lookback` Smoothed Moving Average (SMMA)\n    smma = self.smma_buffer[-1]\n    smma = ((n - 1) * smma + loss) / n\n    self.smma_buffer.append(smma)\n\n    module.log(self.log_key, smma)\n    if len(self.smma_buffer) &lt; self.config.lookback:\n        return\n\n    # phase 3: compute the improvement between the current and the `lookback` SMMA\n    smma_lookback = self.smma_buffer[0]\n    improvement = (smma_lookback - smma) / smma_lookback\n    self.smma_buffer.pop(0)\n\n    if 0 &lt; improvement &lt; self.config.threshold:\n        trainer.should_stop = True\n        print(\n            f\"\\nStopping training: SMMA improvement over {self.config.lookback} \"\n            f\"epochs ({improvement:.2%}) below threshold ({self.config.threshold:.2%})\"\n        )\n</code></pre>"},{"location":"reference/pinn/lightning/callbacks/","title":"callbacks","text":""},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks","title":"<code>pinn.lightning.callbacks</code>","text":""},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.FormatFn","title":"<code>FormatFn: TypeAlias = Callable[[str, Metric], Metric]</code>  <code>module-attribute</code>","text":"<p>A function that formats a metric for display in the progress bar. Takes the key and value of the metric, and returns the formatted metric.</p>"},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.HookFn","title":"<code>HookFn: TypeAlias = Callable[[Trainer, LightningModule, Sequence[Predictions], Sequence[Any]], None]</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.Metric","title":"<code>Metric: TypeAlias = int | str | float | dict[str, float]</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.SMMA_KEY","title":"<code>SMMA_KEY = 'loss/smma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.DataScaling","title":"<code>DataScaling</code>","text":"<p>               Bases: <code>DataCallback</code></p> <p>Callback to transform the data and collocation points.</p> <p>Scales x to [0, 1] and applies per-series scaling factors to y.</p> <p>Parameters:</p> Name Type Description Default <code>y_scale</code> <code>float | Sequence[float]</code> <p>Scaling factor(s) for y data. Can be: - A single float: applied to all series - A sequence of floats: one per series (length must match number of series)</p> required Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>class DataScaling(DataCallback):\n    \"\"\"\n    Callback to transform the data and collocation points.\n\n    Scales x to [0, 1] and applies per-series scaling factors to y.\n\n    Args:\n        y_scale: Scaling factor(s) for y data. Can be:\n            - A single float: applied to all series\n            - A sequence of floats: one per series (length must match number of series)\n    \"\"\"\n\n    def __init__(self, y_scale: float | Sequence[float]):\n        self._y_scale_input = y_scale\n\n    @override\n    def transform_data(self, data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]:\n        x, y = data\n\n        self.x_scale = x.max() - x.min()\n\n        x = (x - x.min()) / (x.max() - x.min())\n        coll = (coll - coll.min()) / (coll.max() - coll.min())\n\n        n_series = y.shape[1] if y.ndim == 3 else 1\n\n        if isinstance(self._y_scale_input, (int, float)):\n            scale_list = [float(self._y_scale_input)] * n_series\n        else:\n            scale_list = list(self._y_scale_input)\n            if len(scale_list) != n_series:\n                raise ValueError(\n                    f\"y_scale has {len(scale_list)} elements but data has {n_series} series\"\n                )\n\n        self.y_scale = torch.tensor(scale_list, dtype=y.dtype, device=y.device)\n\n        # Reshape for broadcasting: [k] -&gt; [1, k, 1] for [n, k, 1] or keep [1] for [n, 1]\n        scale_tensor = self.y_scale.view(1, -1, 1) if y.ndim == 3 else self.y_scale.view(1)\n\n        return (x, y * scale_tensor), coll\n\n    @override\n    def on_after_setup(self, dm: PINNDataModule) -&gt; None:\n        \"\"\"Called after setup is complete.\"\"\"\n\n        for k in dm.validation:\n            orig_fn = dm.validation[k]\n            dm.validation[k] = (lambda fn, scale: (lambda x: fn(x * scale)))(orig_fn, self.x_scale)\n        return None\n</code></pre>"},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.DataScaling.__init__","title":"<code>__init__(y_scale: float | Sequence[float])</code>","text":"Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>def __init__(self, y_scale: float | Sequence[float]):\n    self._y_scale_input = y_scale\n</code></pre>"},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.DataScaling.on_after_setup","title":"<code>on_after_setup(dm: PINNDataModule) -&gt; None</code>","text":"<p>Called after setup is complete.</p> Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>@override\ndef on_after_setup(self, dm: PINNDataModule) -&gt; None:\n    \"\"\"Called after setup is complete.\"\"\"\n\n    for k in dm.validation:\n        orig_fn = dm.validation[k]\n        dm.validation[k] = (lambda fn, scale: (lambda x: fn(x * scale)))(orig_fn, self.x_scale)\n    return None\n</code></pre>"},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.DataScaling.transform_data","title":"<code>transform_data(data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]</code>","text":"Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>@override\ndef transform_data(self, data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]:\n    x, y = data\n\n    self.x_scale = x.max() - x.min()\n\n    x = (x - x.min()) / (x.max() - x.min())\n    coll = (coll - coll.min()) / (coll.max() - coll.min())\n\n    n_series = y.shape[1] if y.ndim == 3 else 1\n\n    if isinstance(self._y_scale_input, (int, float)):\n        scale_list = [float(self._y_scale_input)] * n_series\n    else:\n        scale_list = list(self._y_scale_input)\n        if len(scale_list) != n_series:\n            raise ValueError(\n                f\"y_scale has {len(scale_list)} elements but data has {n_series} series\"\n            )\n\n    self.y_scale = torch.tensor(scale_list, dtype=y.dtype, device=y.device)\n\n    # Reshape for broadcasting: [k] -&gt; [1, k, 1] for [n, k, 1] or keep [1] for [n, 1]\n    scale_tensor = self.y_scale.view(1, -1, 1) if y.ndim == 3 else self.y_scale.view(1)\n\n    return (x, y * scale_tensor), coll\n</code></pre>"},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.FormattedProgressBar","title":"<code>FormattedProgressBar</code>","text":"<p>               Bases: <code>TQDMProgressBar</code></p> <p>Custom progress bar for training that formats metrics for better readability.</p> <p>This class extends the TQDMProgressBar to provide custom formatting for training metrics, particularly for the total loss and beta values.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>FormatFn</code> <p>Function to format the metric values.</p> required Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>class FormattedProgressBar(TQDMProgressBar):\n    \"\"\"\n    Custom progress bar for training that formats metrics for better readability.\n\n    This class extends the TQDMProgressBar to provide custom formatting for\n    training metrics, particularly for the total loss and beta values.\n\n    Args:\n        format: Function to format the metric values.\n    \"\"\"\n\n    def __init__(self, *args: Any, format: FormatFn, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n        self.format = format\n\n    @override\n    def get_metrics(self, *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n        \"\"\"\n        Format metrics for display in the progress bar.\n\n        Returns:\n            Dictionary of formatted metrics with:\n            - Total loss in scientific notation\n            - Beta value with 4 decimal places\n            - Other metrics as provided by the parent class\n        \"\"\"\n        items = super().get_metrics(*args, **kwargs)\n        items.pop(\"v_num\", None)\n        for key, value in items.items():\n            items[key] = self.format(key, value)\n\n        return items\n</code></pre>"},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.FormattedProgressBar.format","title":"<code>format = format</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.FormattedProgressBar.__init__","title":"<code>__init__(*args: Any, format: FormatFn, **kwargs: Any)</code>","text":"Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>def __init__(self, *args: Any, format: FormatFn, **kwargs: Any):\n    super().__init__(*args, **kwargs)\n    self.format = format\n</code></pre>"},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.FormattedProgressBar.get_metrics","title":"<code>get_metrics(*args: Any, **kwargs: Any) -&gt; dict[str, Any]</code>","text":"<p>Format metrics for display in the progress bar.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of formatted metrics with:</p> <code>dict[str, Any]</code> <ul> <li>Total loss in scientific notation</li> </ul> <code>dict[str, Any]</code> <ul> <li>Beta value with 4 decimal places</li> </ul> <code>dict[str, Any]</code> <ul> <li>Other metrics as provided by the parent class</li> </ul> Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>@override\ndef get_metrics(self, *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"\n    Format metrics for display in the progress bar.\n\n    Returns:\n        Dictionary of formatted metrics with:\n        - Total loss in scientific notation\n        - Beta value with 4 decimal places\n        - Other metrics as provided by the parent class\n    \"\"\"\n    items = super().get_metrics(*args, **kwargs)\n    items.pop(\"v_num\", None)\n    for key, value in items.items():\n        items[key] = self.format(key, value)\n\n    return items\n</code></pre>"},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.PredictionsWriter","title":"<code>PredictionsWriter</code>","text":"<p>               Bases: <code>BasePredictionWriter</code></p> <p>Callback to write predictions to disk at the end of an epoch.</p> <p>Parameters:</p> Name Type Description Default <code>predictions_path</code> <code>Path | None</code> <p>Path to save the predictions tensor/object.</p> <code>None</code> <code>batch_indices_path</code> <code>Path | None</code> <p>Path to save the batch indices.</p> <code>None</code> <code>on_prediction</code> <code>HookFn | None</code> <p>Optional hook function called when predictions are ready.</p> <code>None</code> <code>write_interval</code> <code>Literal['batch', 'epoch', 'batch_and_epoch']</code> <p>Interval to write predictions (\"batch\", \"epoch\", \"batch_and_epoch\").</p> <code>'epoch'</code> Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>class PredictionsWriter(BasePredictionWriter):\n    \"\"\"\n    Callback to write predictions to disk at the end of an epoch.\n\n    Args:\n        predictions_path: Path to save the predictions tensor/object.\n        batch_indices_path: Path to save the batch indices.\n        on_prediction: Optional hook function called when predictions are ready.\n        write_interval: Interval to write predictions (\"batch\", \"epoch\", \"batch_and_epoch\").\n    \"\"\"\n\n    def __init__(\n        self,\n        predictions_path: Path | None = None,\n        batch_indices_path: Path | None = None,\n        on_prediction: HookFn | None = None,\n        write_interval: Literal[\"batch\", \"epoch\", \"batch_and_epoch\"] = \"epoch\",\n    ):\n        super().__init__(write_interval)\n        self.predictions_path = predictions_path\n        self.batch_indices_path = batch_indices_path\n        self.on_prediction = on_prediction\n\n    @override\n    def write_on_epoch_end(\n        self,\n        trainer: Trainer,\n        module: LightningModule,\n        predictions_list: Sequence[Predictions],\n        batch_indices: Sequence[Any],\n    ) -&gt; None:\n        \"\"\"\n        Writes predictions to disk or calls the hook at the end of the epoch.\n        \"\"\"\n        if self.on_prediction is not None:\n            self.on_prediction(trainer, module, predictions_list, batch_indices)\n\n        if self.predictions_path is not None:\n            torch.save(predictions_list, self.predictions_path)\n\n        if self.batch_indices_path is not None:\n            torch.save(batch_indices, self.batch_indices_path)\n</code></pre>"},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.PredictionsWriter.batch_indices_path","title":"<code>batch_indices_path = batch_indices_path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.PredictionsWriter.on_prediction","title":"<code>on_prediction = on_prediction</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.PredictionsWriter.predictions_path","title":"<code>predictions_path = predictions_path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.PredictionsWriter.__init__","title":"<code>__init__(predictions_path: Path | None = None, batch_indices_path: Path | None = None, on_prediction: HookFn | None = None, write_interval: Literal['batch', 'epoch', 'batch_and_epoch'] = 'epoch')</code>","text":"Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>def __init__(\n    self,\n    predictions_path: Path | None = None,\n    batch_indices_path: Path | None = None,\n    on_prediction: HookFn | None = None,\n    write_interval: Literal[\"batch\", \"epoch\", \"batch_and_epoch\"] = \"epoch\",\n):\n    super().__init__(write_interval)\n    self.predictions_path = predictions_path\n    self.batch_indices_path = batch_indices_path\n    self.on_prediction = on_prediction\n</code></pre>"},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.PredictionsWriter.write_on_epoch_end","title":"<code>write_on_epoch_end(trainer: Trainer, module: LightningModule, predictions_list: Sequence[Predictions], batch_indices: Sequence[Any]) -&gt; None</code>","text":"<p>Writes predictions to disk or calls the hook at the end of the epoch.</p> Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>@override\ndef write_on_epoch_end(\n    self,\n    trainer: Trainer,\n    module: LightningModule,\n    predictions_list: Sequence[Predictions],\n    batch_indices: Sequence[Any],\n) -&gt; None:\n    \"\"\"\n    Writes predictions to disk or calls the hook at the end of the epoch.\n    \"\"\"\n    if self.on_prediction is not None:\n        self.on_prediction(trainer, module, predictions_list, batch_indices)\n\n    if self.predictions_path is not None:\n        torch.save(predictions_list, self.predictions_path)\n\n    if self.batch_indices_path is not None:\n        torch.save(batch_indices, self.batch_indices_path)\n</code></pre>"},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.SMMAStopping","title":"<code>SMMAStopping</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Early stopping callback based on the Smoothed Moving Average (SMMA) of the loss. Stops training if the relative improvement of the SMMA drops below a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SMMAStoppingConfig</code> <p>Configuration for SMMA stopping (window, threshold, lookback).</p> required <code>loss_key</code> <code>str</code> <p>The metric key to monitor (e.g., 'loss').</p> required <code>log_key</code> <code>str</code> <p>Key to log the computed SMMA value.</p> <code>SMMA_KEY</code> Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>class SMMAStopping(Callback):\n    \"\"\"\n    Early stopping callback based on the Smoothed Moving Average (SMMA) of the loss.\n    Stops training if the relative improvement of the SMMA drops below a threshold.\n\n    Args:\n        config: Configuration for SMMA stopping (window, threshold, lookback).\n        loss_key: The metric key to monitor (e.g., 'loss').\n        log_key: Key to log the computed SMMA value.\n    \"\"\"\n\n    def __init__(self, config: SMMAStoppingConfig, loss_key: str, log_key: str = SMMA_KEY):\n        super().__init__()\n        self.config = config\n        self.loss_key = loss_key\n        self.log_key = log_key\n        self.loss_buffer: list[float] = []\n        self.smma_buffer: list[float] = []\n\n    @override\n    def on_train_epoch_end(self, trainer: Trainer, module: LightningModule) -&gt; None:\n        \"\"\"\n        Called when the train epoch ends. Updates SMMA and checks stopping condition.\n        \"\"\"\n        # phase 0: get the loss\n        loss_t = trainer.callback_metrics.get(self.loss_key)\n        if loss_t is None:\n            return\n\n        loss = loss_t.item()\n        n = self.config.window\n\n        # phase 1: collect first `window` losses\n        if len(self.loss_buffer) &lt;= n:\n            self.loss_buffer.append(loss)\n            return\n\n        # phase 1.5: compute the first average\n        if len(self.smma_buffer) == 0:\n            first_smma = sum(self.loss_buffer) / n\n            self.smma_buffer.append(first_smma)\n            return\n\n        # phase 2: compute the first `lookback` Smoothed Moving Average (SMMA)\n        smma = self.smma_buffer[-1]\n        smma = ((n - 1) * smma + loss) / n\n        self.smma_buffer.append(smma)\n\n        module.log(self.log_key, smma)\n        if len(self.smma_buffer) &lt; self.config.lookback:\n            return\n\n        # phase 3: compute the improvement between the current and the `lookback` SMMA\n        smma_lookback = self.smma_buffer[0]\n        improvement = (smma_lookback - smma) / smma_lookback\n        self.smma_buffer.pop(0)\n\n        if 0 &lt; improvement &lt; self.config.threshold:\n            trainer.should_stop = True\n            print(\n                f\"\\nStopping training: SMMA improvement over {self.config.lookback} \"\n                f\"epochs ({improvement:.2%}) below threshold ({self.config.threshold:.2%})\"\n            )\n</code></pre>"},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.SMMAStopping.config","title":"<code>config = config</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.SMMAStopping.log_key","title":"<code>log_key = log_key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.SMMAStopping.loss_buffer","title":"<code>loss_buffer: list[float] = []</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.SMMAStopping.loss_key","title":"<code>loss_key = loss_key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.SMMAStopping.smma_buffer","title":"<code>smma_buffer: list[float] = []</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.SMMAStopping.__init__","title":"<code>__init__(config: SMMAStoppingConfig, loss_key: str, log_key: str = SMMA_KEY)</code>","text":"Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>def __init__(self, config: SMMAStoppingConfig, loss_key: str, log_key: str = SMMA_KEY):\n    super().__init__()\n    self.config = config\n    self.loss_key = loss_key\n    self.log_key = log_key\n    self.loss_buffer: list[float] = []\n    self.smma_buffer: list[float] = []\n</code></pre>"},{"location":"reference/pinn/lightning/callbacks/#pinn.lightning.callbacks.SMMAStopping.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer: Trainer, module: LightningModule) -&gt; None</code>","text":"<p>Called when the train epoch ends. Updates SMMA and checks stopping condition.</p> Source code in <code>src/pinn/lightning/callbacks.py</code> <pre><code>@override\ndef on_train_epoch_end(self, trainer: Trainer, module: LightningModule) -&gt; None:\n    \"\"\"\n    Called when the train epoch ends. Updates SMMA and checks stopping condition.\n    \"\"\"\n    # phase 0: get the loss\n    loss_t = trainer.callback_metrics.get(self.loss_key)\n    if loss_t is None:\n        return\n\n    loss = loss_t.item()\n    n = self.config.window\n\n    # phase 1: collect first `window` losses\n    if len(self.loss_buffer) &lt;= n:\n        self.loss_buffer.append(loss)\n        return\n\n    # phase 1.5: compute the first average\n    if len(self.smma_buffer) == 0:\n        first_smma = sum(self.loss_buffer) / n\n        self.smma_buffer.append(first_smma)\n        return\n\n    # phase 2: compute the first `lookback` Smoothed Moving Average (SMMA)\n    smma = self.smma_buffer[-1]\n    smma = ((n - 1) * smma + loss) / n\n    self.smma_buffer.append(smma)\n\n    module.log(self.log_key, smma)\n    if len(self.smma_buffer) &lt; self.config.lookback:\n        return\n\n    # phase 3: compute the improvement between the current and the `lookback` SMMA\n    smma_lookback = self.smma_buffer[0]\n    improvement = (smma_lookback - smma) / smma_lookback\n    self.smma_buffer.pop(0)\n\n    if 0 &lt; improvement &lt; self.config.threshold:\n        trainer.should_stop = True\n        print(\n            f\"\\nStopping training: SMMA improvement over {self.config.lookback} \"\n            f\"epochs ({improvement:.2%}) below threshold ({self.config.threshold:.2%})\"\n        )\n</code></pre>"},{"location":"reference/pinn/lightning/module/","title":"module","text":""},{"location":"reference/pinn/lightning/module/#pinn.lightning.module","title":"<code>pinn.lightning.module</code>","text":""},{"location":"reference/pinn/lightning/module/#pinn.lightning.module.PINNModule","title":"<code>PINNModule</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Generic PINN Lightning module. Expects external Problem + Sampler + optimizer config.</p> <p>Parameters:</p> Name Type Description Default <code>problem</code> <code>Problem</code> <p>The PINN problem definition (constraints, fields, etc.).</p> required <code>hp</code> <code>PINNHyperparameters</code> <p>Hyperparameters for training.</p> required Source code in <code>src/pinn/lightning/module.py</code> <pre><code>class PINNModule(pl.LightningModule):\n    \"\"\"\n    Generic PINN Lightning module.\n    Expects external Problem + Sampler + optimizer config.\n\n    Args:\n        problem: The PINN problem definition (constraints, fields, etc.).\n        hp: Hyperparameters for training.\n    \"\"\"\n\n    def __init__(\n        self,\n        problem: Problem,\n        hp: PINNHyperparameters,\n    ):\n        super().__init__()\n        self.save_hyperparameters(ignore=[\"problem\"])\n\n        self.problem = problem\n        self.hp = hp\n        self.scheduler = hp.scheduler\n\n        def _log(key: str, value: Tensor, progress_bar: bool = False) -&gt; None:\n            self.log(\n                key,\n                value,\n                on_step=False,\n                on_epoch=True,\n                prog_bar=progress_bar,\n                batch_size=hp.training_data.batch_size,\n            )\n\n        self._log = cast(LogFn, _log)\n\n    @override\n    def on_fit_start(self) -&gt; None:\n        \"\"\"\n        Called when fit begins. Resolves validation sources using loaded data.\n        \"\"\"\n        self.problem.inject_context(self.trainer.datamodule.context)  # type: ignore\n\n    @override\n    def on_predict_start(self) -&gt; None:\n        \"\"\"\n        Called when predict begins. Resolves validation sources using loaded data.\n        \"\"\"\n        self.problem.inject_context(self.trainer.datamodule.context)  # type: ignore\n\n    @override\n    def training_step(self, batch: TrainingBatch, batch_idx: int) -&gt; Tensor:\n        \"\"\"\n        Performs a single training step.\n        Calculates total loss from the problem.\n        \"\"\"\n        return self.problem.training_loss(batch, self._log)\n\n    @override\n    def predict_step(self, batch: PredictionBatch, batch_idx: int) -&gt; Predictions:\n        \"\"\"\n        Performs a prediction step.\n        \"\"\"\n        x_data, y_data = batch\n\n        (data_batch, predictions) = self.problem.predict((x_data, y_data))\n        true_values = self.problem.true_values(x_data)\n\n        return (data_batch, predictions, true_values)\n\n    @override\n    def configure_optimizers(self) -&gt; OptimizerLRScheduler:\n        \"\"\"\n        Configures the optimizer and learning rate scheduler.\n        \"\"\"\n        opt = torch.optim.Adam(self.parameters(), lr=self.hp.lr)\n        if not self.scheduler:\n            return opt\n\n        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            opt,\n            mode=self.scheduler.mode,\n            factor=self.scheduler.factor,\n            patience=self.scheduler.patience,\n            threshold=self.scheduler.threshold,\n            min_lr=self.scheduler.min_lr,\n        )\n\n        return {\n            \"optimizer\": opt,\n            \"lr_scheduler\": {\n                \"name\": \"lr\",\n                \"scheduler\": sch,\n                \"monitor\": LOSS_KEY,\n                \"interval\": \"epoch\",\n                \"frequency\": 1,\n            },\n        }\n</code></pre>"},{"location":"reference/pinn/lightning/module/#pinn.lightning.module.PINNModule.hp","title":"<code>hp = hp</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/module/#pinn.lightning.module.PINNModule.problem","title":"<code>problem = problem</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/module/#pinn.lightning.module.PINNModule.scheduler","title":"<code>scheduler = hp.scheduler</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/lightning/module/#pinn.lightning.module.PINNModule.__init__","title":"<code>__init__(problem: Problem, hp: PINNHyperparameters)</code>","text":"Source code in <code>src/pinn/lightning/module.py</code> <pre><code>def __init__(\n    self,\n    problem: Problem,\n    hp: PINNHyperparameters,\n):\n    super().__init__()\n    self.save_hyperparameters(ignore=[\"problem\"])\n\n    self.problem = problem\n    self.hp = hp\n    self.scheduler = hp.scheduler\n\n    def _log(key: str, value: Tensor, progress_bar: bool = False) -&gt; None:\n        self.log(\n            key,\n            value,\n            on_step=False,\n            on_epoch=True,\n            prog_bar=progress_bar,\n            batch_size=hp.training_data.batch_size,\n        )\n\n    self._log = cast(LogFn, _log)\n</code></pre>"},{"location":"reference/pinn/lightning/module/#pinn.lightning.module.PINNModule.configure_optimizers","title":"<code>configure_optimizers() -&gt; OptimizerLRScheduler</code>","text":"<p>Configures the optimizer and learning rate scheduler.</p> Source code in <code>src/pinn/lightning/module.py</code> <pre><code>@override\ndef configure_optimizers(self) -&gt; OptimizerLRScheduler:\n    \"\"\"\n    Configures the optimizer and learning rate scheduler.\n    \"\"\"\n    opt = torch.optim.Adam(self.parameters(), lr=self.hp.lr)\n    if not self.scheduler:\n        return opt\n\n    sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        opt,\n        mode=self.scheduler.mode,\n        factor=self.scheduler.factor,\n        patience=self.scheduler.patience,\n        threshold=self.scheduler.threshold,\n        min_lr=self.scheduler.min_lr,\n    )\n\n    return {\n        \"optimizer\": opt,\n        \"lr_scheduler\": {\n            \"name\": \"lr\",\n            \"scheduler\": sch,\n            \"monitor\": LOSS_KEY,\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        },\n    }\n</code></pre>"},{"location":"reference/pinn/lightning/module/#pinn.lightning.module.PINNModule.on_fit_start","title":"<code>on_fit_start() -&gt; None</code>","text":"<p>Called when fit begins. Resolves validation sources using loaded data.</p> Source code in <code>src/pinn/lightning/module.py</code> <pre><code>@override\ndef on_fit_start(self) -&gt; None:\n    \"\"\"\n    Called when fit begins. Resolves validation sources using loaded data.\n    \"\"\"\n    self.problem.inject_context(self.trainer.datamodule.context)  # type: ignore\n</code></pre>"},{"location":"reference/pinn/lightning/module/#pinn.lightning.module.PINNModule.on_predict_start","title":"<code>on_predict_start() -&gt; None</code>","text":"<p>Called when predict begins. Resolves validation sources using loaded data.</p> Source code in <code>src/pinn/lightning/module.py</code> <pre><code>@override\ndef on_predict_start(self) -&gt; None:\n    \"\"\"\n    Called when predict begins. Resolves validation sources using loaded data.\n    \"\"\"\n    self.problem.inject_context(self.trainer.datamodule.context)  # type: ignore\n</code></pre>"},{"location":"reference/pinn/lightning/module/#pinn.lightning.module.PINNModule.predict_step","title":"<code>predict_step(batch: PredictionBatch, batch_idx: int) -&gt; Predictions</code>","text":"<p>Performs a prediction step.</p> Source code in <code>src/pinn/lightning/module.py</code> <pre><code>@override\ndef predict_step(self, batch: PredictionBatch, batch_idx: int) -&gt; Predictions:\n    \"\"\"\n    Performs a prediction step.\n    \"\"\"\n    x_data, y_data = batch\n\n    (data_batch, predictions) = self.problem.predict((x_data, y_data))\n    true_values = self.problem.true_values(x_data)\n\n    return (data_batch, predictions, true_values)\n</code></pre>"},{"location":"reference/pinn/lightning/module/#pinn.lightning.module.PINNModule.training_step","title":"<code>training_step(batch: TrainingBatch, batch_idx: int) -&gt; Tensor</code>","text":"<p>Performs a single training step. Calculates total loss from the problem.</p> Source code in <code>src/pinn/lightning/module.py</code> <pre><code>@override\ndef training_step(self, batch: TrainingBatch, batch_idx: int) -&gt; Tensor:\n    \"\"\"\n    Performs a single training step.\n    Calculates total loss from the problem.\n    \"\"\"\n    return self.problem.training_loss(batch, self._log)\n</code></pre>"},{"location":"reference/pinn/problems/","title":"problems","text":""},{"location":"reference/pinn/problems/#pinn.problems","title":"<code>pinn.problems</code>","text":"<p>Problem templates and implementations.</p>"},{"location":"reference/pinn/problems/#pinn.problems.__all__","title":"<code>__all__ = ['InferredContext', 'ODECallable', 'ODEProperties', 'SIRInvDataModule', 'SIRInvHyperparameters', 'SIRInvProblem']</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/problems/#pinn.problems.ODECallable","title":"<code>ODECallable</code>","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>src/pinn/problems/ode.py</code> <pre><code>class ODECallable(Protocol):\n    def __call__(\n        self,\n        x: Tensor,\n        y: Tensor,\n        args: ArgsRegistry,\n    ) -&gt; Tensor: ...\n</code></pre>"},{"location":"reference/pinn/problems/#pinn.problems.ODECallable.__call__","title":"<code>__call__(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"Source code in <code>src/pinn/problems/ode.py</code> <pre><code>def __call__(\n    self,\n    x: Tensor,\n    y: Tensor,\n    args: ArgsRegistry,\n) -&gt; Tensor: ...\n</code></pre>"},{"location":"reference/pinn/problems/#pinn.problems.ODEProperties","title":"<code>ODEProperties</code>  <code>dataclass</code>","text":"<p>Properties defining an Ordinary Differential Equation problem.</p> <p>Attributes:</p> Name Type Description <code>ode</code> <code>ODECallable</code> <p>The ODE function (callable).</p> <code>args</code> <code>ArgsRegistry</code> <p>Arguments/Parameters for the ODE.</p> <code>y0</code> <code>Tensor</code> <p>Initial conditions.</p> Source code in <code>src/pinn/problems/ode.py</code> <pre><code>@dataclass\nclass ODEProperties:\n    \"\"\"\n    Properties defining an Ordinary Differential Equation problem.\n\n    Attributes:\n        ode: The ODE function (callable).\n        args: Arguments/Parameters for the ODE.\n        y0: Initial conditions.\n    \"\"\"\n\n    ode: ODECallable\n    args: ArgsRegistry\n    y0: Tensor\n</code></pre>"},{"location":"reference/pinn/problems/#pinn.problems.ODEProperties.args","title":"<code>args: ArgsRegistry</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/#pinn.problems.ODEProperties.ode","title":"<code>ode: ODECallable</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/#pinn.problems.ODEProperties.y0","title":"<code>y0: Tensor</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/#pinn.problems.ODEProperties.__init__","title":"<code>__init__(ode: ODECallable, args: ArgsRegistry, y0: Tensor) -&gt; None</code>","text":""},{"location":"reference/pinn/problems/#pinn.problems.SIRInvDataModule","title":"<code>SIRInvDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for SIR Inverse problem.</p> Source code in <code>src/pinn/problems/sir_inverse.py</code> <pre><code>class SIRInvDataModule(PINNDataModule):\n    \"\"\"\n    DataModule for SIR Inverse problem.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: SIRInvHyperparameters,\n        gen_props: ODEProperties | None = None,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ):\n        super().__init__(hp, validation, callbacks)\n        self.gen_props = gen_props\n\n    @override\n    def gen_coll(self, domain: Domain1D) -&gt; Tensor:\n        \"\"\"Generate collocation points.\"\"\"\n        x0 = torch.tensor(domain.x0, dtype=torch.float32)\n        x1 = torch.tensor(domain.x1, dtype=torch.float32)\n\n        coll = torch.rand((self.hp.training_data.collocations, 1))\n        coll = coll * (torch.log1p(x1) - torch.log1p(x0)) + torch.log1p(x0)\n        coll = torch.expm1(coll)\n        return coll\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate synthetic data.\"\"\"\n        assert self.gen_props is not None, \"SIR properties are required to generate data\"\n\n        args = self.gen_props.args.copy()\n        args.update(config.args_to_train)\n\n        data = odeint(\n            lambda x, y: self.gen_props.ode(x, y, args),\n            self.gen_props.y0,\n            config.x,\n        )\n\n        I_true = data[:, 1].clamp_min(0.0)\n\n        I_obs = self._noise(I_true, config.noise_level)\n\n        return config.x.unsqueeze(-1), I_obs.unsqueeze(-1)\n\n    def _noise(self, I_true: Tensor, noise_level: float) -&gt; Tensor:\n        if noise_level &lt; 1.0:\n            return I_true\n        else:\n            return torch.poisson(I_true / noise_level) * noise_level\n</code></pre>"},{"location":"reference/pinn/problems/#pinn.problems.SIRInvDataModule.gen_props","title":"<code>gen_props = gen_props</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/#pinn.problems.SIRInvDataModule.__init__","title":"<code>__init__(hp: SIRInvHyperparameters, gen_props: ODEProperties | None = None, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None)</code>","text":"Source code in <code>src/pinn/problems/sir_inverse.py</code> <pre><code>def __init__(\n    self,\n    hp: SIRInvHyperparameters,\n    gen_props: ODEProperties | None = None,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n):\n    super().__init__(hp, validation, callbacks)\n    self.gen_props = gen_props\n</code></pre>"},{"location":"reference/pinn/problems/#pinn.problems.SIRInvDataModule.gen_coll","title":"<code>gen_coll(domain: Domain1D) -&gt; Tensor</code>","text":"<p>Generate collocation points.</p> Source code in <code>src/pinn/problems/sir_inverse.py</code> <pre><code>@override\ndef gen_coll(self, domain: Domain1D) -&gt; Tensor:\n    \"\"\"Generate collocation points.\"\"\"\n    x0 = torch.tensor(domain.x0, dtype=torch.float32)\n    x1 = torch.tensor(domain.x1, dtype=torch.float32)\n\n    coll = torch.rand((self.hp.training_data.collocations, 1))\n    coll = coll * (torch.log1p(x1) - torch.log1p(x0)) + torch.log1p(x0)\n    coll = torch.expm1(coll)\n    return coll\n</code></pre>"},{"location":"reference/pinn/problems/#pinn.problems.SIRInvDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate synthetic data.</p> Source code in <code>src/pinn/problems/sir_inverse.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate synthetic data.\"\"\"\n    assert self.gen_props is not None, \"SIR properties are required to generate data\"\n\n    args = self.gen_props.args.copy()\n    args.update(config.args_to_train)\n\n    data = odeint(\n        lambda x, y: self.gen_props.ode(x, y, args),\n        self.gen_props.y0,\n        config.x,\n    )\n\n    I_true = data[:, 1].clamp_min(0.0)\n\n    I_obs = self._noise(I_true, config.noise_level)\n\n    return config.x.unsqueeze(-1), I_obs.unsqueeze(-1)\n</code></pre>"},{"location":"reference/pinn/problems/#pinn.problems.SIRInvHyperparameters","title":"<code>SIRInvHyperparameters</code>  <code>dataclass</code>","text":"<p>               Bases: <code>PINNHyperparameters</code></p> <p>Hyperparameters for the SIR Inverse problem.</p> Source code in <code>src/pinn/problems/sir_inverse.py</code> <pre><code>@dataclass(kw_only=True)\nclass SIRInvHyperparameters(PINNHyperparameters):\n    \"\"\"\n    Hyperparameters for the SIR Inverse problem.\n    \"\"\"\n\n    # TODO: implement adaptive weights\n    pde_weight: float = 1.0\n    ic_weight: float = 1.0\n    data_weight: float = 1.0\n</code></pre>"},{"location":"reference/pinn/problems/#pinn.problems.SIRInvHyperparameters.data_weight","title":"<code>data_weight: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/#pinn.problems.SIRInvHyperparameters.ic_weight","title":"<code>ic_weight: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/#pinn.problems.SIRInvHyperparameters.pde_weight","title":"<code>pde_weight: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/#pinn.problems.SIRInvHyperparameters.__init__","title":"<code>__init__(*, lr: float, training_data: IngestionConfig | GenerationConfig, fields_config: MLPConfig, params_config: MLPConfig | ScalarConfig, scheduler: SchedulerConfig | None = None, early_stopping: EarlyStoppingConfig | None = None, smma_stopping: SMMAStoppingConfig | None = None, pde_weight: float = 1.0, ic_weight: float = 1.0, data_weight: float = 1.0) -&gt; None</code>","text":""},{"location":"reference/pinn/problems/#pinn.problems.SIRInvProblem","title":"<code>SIRInvProblem</code>","text":"<p>               Bases: <code>Problem</code></p> <p>Definition of the SIR Inverse Problem. Infers parameters (beta) from data while satisfying the SIR ODE.</p> Source code in <code>src/pinn/problems/sir_inverse.py</code> <pre><code>class SIRInvProblem(Problem):\n    \"\"\"\n    Definition of the SIR Inverse Problem.\n    Infers parameters (beta) from data while satisfying the SIR ODE.\n    \"\"\"\n\n    def __init__(\n        self,\n        props: ODEProperties,\n        hp: SIRInvHyperparameters,\n        fields: FieldsRegistry,\n        params: ParamsRegistry,\n    ) -&gt; None:\n        def predict_data(\n            x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry\n        ) -&gt; Tensor:\n            I = fields[I_KEY]\n            I_pred = I(x_data)\n            return cast(Tensor, I_pred)\n\n        constraints: list[Constraint] = [\n            ResidualsConstraint(\n                props=props,\n                fields=fields,\n                params=params,\n                weight=hp.pde_weight,\n            ),\n            ICConstraint(\n                props=props,\n                fields=fields,\n                weight=hp.ic_weight,\n            ),\n            DataConstraint(\n                fields=fields,\n                params=params,\n                predict_data=predict_data,\n                weight=hp.data_weight,\n            ),\n        ]\n\n        criterion = nn.MSELoss()\n\n        super().__init__(\n            constraints=constraints,\n            criterion=criterion,\n            fields=fields,\n            params=params,\n        )\n</code></pre>"},{"location":"reference/pinn/problems/#pinn.problems.SIRInvProblem.__init__","title":"<code>__init__(props: ODEProperties, hp: SIRInvHyperparameters, fields: FieldsRegistry, params: ParamsRegistry) -&gt; None</code>","text":"Source code in <code>src/pinn/problems/sir_inverse.py</code> <pre><code>def __init__(\n    self,\n    props: ODEProperties,\n    hp: SIRInvHyperparameters,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n) -&gt; None:\n    def predict_data(\n        x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry\n    ) -&gt; Tensor:\n        I = fields[I_KEY]\n        I_pred = I(x_data)\n        return cast(Tensor, I_pred)\n\n    constraints: list[Constraint] = [\n        ResidualsConstraint(\n            props=props,\n            fields=fields,\n            params=params,\n            weight=hp.pde_weight,\n        ),\n        ICConstraint(\n            props=props,\n            fields=fields,\n            weight=hp.ic_weight,\n        ),\n        DataConstraint(\n            fields=fields,\n            params=params,\n            predict_data=predict_data,\n            weight=hp.data_weight,\n        ),\n    ]\n\n    criterion = nn.MSELoss()\n\n    super().__init__(\n        constraints=constraints,\n        criterion=criterion,\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/pinn/problems/ode/","title":"ode","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode","title":"<code>pinn.problems.ode</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.PredictDataFn","title":"<code>PredictDataFn: TypeAlias = Callable[[Tensor, FieldsRegistry, ParamsRegistry], Tensor]</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.DataConstraint","title":"<code>DataConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Constraint enforcing fit to observed data. Minimizes ||Predictions - Data||^2.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>FieldsRegistry</code> <p>Fields registry.</p> required <code>params</code> <code>ParamsRegistry</code> <p>Parameters registry.</p> required <code>predict_data</code> <code>PredictDataFn</code> <p>Function to predict data values from fields.</p> required <code>weight</code> <code>float</code> <p>Weight for this loss term.</p> <code>1.0</code> Source code in <code>src/pinn/problems/ode.py</code> <pre><code>class DataConstraint(Constraint):\n    \"\"\"\n    Constraint enforcing fit to observed data.\n    Minimizes ||Predictions - Data||^2.\n\n    Args:\n        fields: Fields registry.\n        params: Parameters registry.\n        predict_data: Function to predict data values from fields.\n        weight: Weight for this loss term.\n    \"\"\"\n\n    def __init__(\n        self,\n        fields: FieldsRegistry,\n        params: ParamsRegistry,\n        predict_data: PredictDataFn,\n        weight: float = 1.0,\n    ):\n        self.fields = fields\n        self.params = params\n        self.predict_data = predict_data\n        self.weight = weight\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        (x_data, y_data), _ = batch\n\n        y_data_pred = self.predict_data(x_data, self.fields, self.params)\n\n        loss: Tensor = criterion(y_data_pred, y_data)\n        loss = self.weight * loss\n\n        if log is not None:\n            log(\"loss/data\", loss)\n\n        return loss\n</code></pre>"},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.DataConstraint.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.DataConstraint.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.DataConstraint.predict_data","title":"<code>predict_data = predict_data</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.DataConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.DataConstraint.__init__","title":"<code>__init__(fields: FieldsRegistry, params: ParamsRegistry, predict_data: PredictDataFn, weight: float = 1.0)</code>","text":"Source code in <code>src/pinn/problems/ode.py</code> <pre><code>def __init__(\n    self,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n    predict_data: PredictDataFn,\n    weight: float = 1.0,\n):\n    self.fields = fields\n    self.params = params\n    self.predict_data = predict_data\n    self.weight = weight\n</code></pre>"},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.DataConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/pinn/problems/ode.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    (x_data, y_data), _ = batch\n\n    y_data_pred = self.predict_data(x_data, self.fields, self.params)\n\n    loss: Tensor = criterion(y_data_pred, y_data)\n    loss = self.weight * loss\n\n    if log is not None:\n        log(\"loss/data\", loss)\n\n    return loss\n</code></pre>"},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ICConstraint","title":"<code>ICConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Constraint enforcing Initial Conditions (IC). Minimizes ||y(t0) - Y0||^2.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>FieldsRegistry</code> <p>Fields registry.</p> required <code>weight</code> <code>float</code> <p>Weight for this loss term.</p> <code>1.0</code> Source code in <code>src/pinn/problems/ode.py</code> <pre><code>class ICConstraint(Constraint):\n    \"\"\"\n    Constraint enforcing Initial Conditions (IC).\n    Minimizes ||y(t0) - Y0||^2.\n\n    Args:\n        fields: Fields registry.\n        weight: Weight for this loss term.\n    \"\"\"\n\n    def __init__(\n        self,\n        props: ODEProperties,\n        fields: FieldsRegistry,\n        weight: float = 1.0,\n    ):\n        self.Y0 = props.y0.clone().reshape(-1, 1, 1)\n        self.fields = fields\n        self.weight = weight\n\n    @override\n    def inject_context(self, context: InferredContext) -&gt; None:\n        \"\"\"\n        Inject the context into the constraint.\n        \"\"\"\n        self.t0 = torch.tensor(context.domain.x0, dtype=torch.float32).reshape(1, 1)\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        device = batch[1].device\n\n        t0 = self.t0.to(device)\n        Y0 = self.Y0.to(device)\n\n        Y0_preds = torch.stack([f(t0) for f in self.fields.values()])\n\n        loss: Tensor = criterion(Y0_preds, Y0)\n        loss = self.weight * loss\n\n        if log is not None:\n            log(\"loss/ic\", loss)\n\n        return loss\n</code></pre>"},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ICConstraint.Y0","title":"<code>Y0 = props.y0.clone().reshape(-1, 1, 1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ICConstraint.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ICConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ICConstraint.__init__","title":"<code>__init__(props: ODEProperties, fields: FieldsRegistry, weight: float = 1.0)</code>","text":"Source code in <code>src/pinn/problems/ode.py</code> <pre><code>def __init__(\n    self,\n    props: ODEProperties,\n    fields: FieldsRegistry,\n    weight: float = 1.0,\n):\n    self.Y0 = props.y0.clone().reshape(-1, 1, 1)\n    self.fields = fields\n    self.weight = weight\n</code></pre>"},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ICConstraint.inject_context","title":"<code>inject_context(context: InferredContext) -&gt; None</code>","text":"<p>Inject the context into the constraint.</p> Source code in <code>src/pinn/problems/ode.py</code> <pre><code>@override\ndef inject_context(self, context: InferredContext) -&gt; None:\n    \"\"\"\n    Inject the context into the constraint.\n    \"\"\"\n    self.t0 = torch.tensor(context.domain.x0, dtype=torch.float32).reshape(1, 1)\n</code></pre>"},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ICConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/pinn/problems/ode.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    device = batch[1].device\n\n    t0 = self.t0.to(device)\n    Y0 = self.Y0.to(device)\n\n    Y0_preds = torch.stack([f(t0) for f in self.fields.values()])\n\n    loss: Tensor = criterion(Y0_preds, Y0)\n    loss = self.weight * loss\n\n    if log is not None:\n        log(\"loss/ic\", loss)\n\n    return loss\n</code></pre>"},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ODECallable","title":"<code>ODECallable</code>","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>src/pinn/problems/ode.py</code> <pre><code>class ODECallable(Protocol):\n    def __call__(\n        self,\n        x: Tensor,\n        y: Tensor,\n        args: ArgsRegistry,\n    ) -&gt; Tensor: ...\n</code></pre>"},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ODECallable.__call__","title":"<code>__call__(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"Source code in <code>src/pinn/problems/ode.py</code> <pre><code>def __call__(\n    self,\n    x: Tensor,\n    y: Tensor,\n    args: ArgsRegistry,\n) -&gt; Tensor: ...\n</code></pre>"},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ODEProperties","title":"<code>ODEProperties</code>  <code>dataclass</code>","text":"<p>Properties defining an Ordinary Differential Equation problem.</p> <p>Attributes:</p> Name Type Description <code>ode</code> <code>ODECallable</code> <p>The ODE function (callable).</p> <code>args</code> <code>ArgsRegistry</code> <p>Arguments/Parameters for the ODE.</p> <code>y0</code> <code>Tensor</code> <p>Initial conditions.</p> Source code in <code>src/pinn/problems/ode.py</code> <pre><code>@dataclass\nclass ODEProperties:\n    \"\"\"\n    Properties defining an Ordinary Differential Equation problem.\n\n    Attributes:\n        ode: The ODE function (callable).\n        args: Arguments/Parameters for the ODE.\n        y0: Initial conditions.\n    \"\"\"\n\n    ode: ODECallable\n    args: ArgsRegistry\n    y0: Tensor\n</code></pre>"},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ODEProperties.args","title":"<code>args: ArgsRegistry</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ODEProperties.ode","title":"<code>ode: ODECallable</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ODEProperties.y0","title":"<code>y0: Tensor</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ODEProperties.__init__","title":"<code>__init__(ode: ODECallable, args: ArgsRegistry, y0: Tensor) -&gt; None</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ResidualsConstraint","title":"<code>ResidualsConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Constraint enforcing the ODE residuals. Minimizes ||dy/dt - f(t, y)||^2.</p> <p>Parameters:</p> Name Type Description Default <code>props</code> <code>ODEProperties</code> <p>ODE properties.</p> required <code>fields</code> <code>FieldsRegistry</code> <p>List of fields.</p> required <code>params</code> <code>ParamsRegistry</code> <p>List of parameters.</p> required <code>weight</code> <code>float</code> <p>Weight for this loss term.</p> <code>1.0</code> Source code in <code>src/pinn/problems/ode.py</code> <pre><code>class ResidualsConstraint(Constraint):\n    \"\"\"\n    Constraint enforcing the ODE residuals.\n    Minimizes ||dy/dt - f(t, y)||^2.\n\n    Args:\n        props: ODE properties.\n        fields: List of fields.\n        params: List of parameters.\n        weight: Weight for this loss term.\n    \"\"\"\n\n    def __init__(\n        self,\n        props: ODEProperties,\n        fields: FieldsRegistry,\n        params: ParamsRegistry,\n        weight: float = 1.0,\n    ):\n        self.fields = fields\n        self.weight = weight\n\n        self.ode = props.ode\n\n        # add the trainable params as args\n        self.args = props.args.copy()\n        self.args.update(params)\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        _, x_coll = batch\n        x_coll.requires_grad_()\n\n        preds = [f(x_coll) for f in self.fields.values()]\n        y = torch.stack(preds)\n\n        dy_dt_pred = self.ode(x_coll, y, self.args)\n\n        dy_dt = torch.stack(\n            [\n                torch.autograd.grad(pred, x_coll, torch.ones_like(pred), create_graph=True)[0]\n                for pred in preds\n            ]\n        )\n\n        loss: Tensor = self.weight * criterion(dy_dt, dy_dt_pred)\n\n        if log is not None:\n            log(\"loss/res\", loss)\n\n        return loss\n</code></pre>"},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ResidualsConstraint.args","title":"<code>args = props.args.copy()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ResidualsConstraint.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ResidualsConstraint.ode","title":"<code>ode = props.ode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ResidualsConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ResidualsConstraint.__init__","title":"<code>__init__(props: ODEProperties, fields: FieldsRegistry, params: ParamsRegistry, weight: float = 1.0)</code>","text":"Source code in <code>src/pinn/problems/ode.py</code> <pre><code>def __init__(\n    self,\n    props: ODEProperties,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n    weight: float = 1.0,\n):\n    self.fields = fields\n    self.weight = weight\n\n    self.ode = props.ode\n\n    # add the trainable params as args\n    self.args = props.args.copy()\n    self.args.update(params)\n</code></pre>"},{"location":"reference/pinn/problems/ode/#pinn.problems.ode.ResidualsConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/pinn/problems/ode.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    _, x_coll = batch\n    x_coll.requires_grad_()\n\n    preds = [f(x_coll) for f in self.fields.values()]\n    y = torch.stack(preds)\n\n    dy_dt_pred = self.ode(x_coll, y, self.args)\n\n    dy_dt = torch.stack(\n        [\n            torch.autograd.grad(pred, x_coll, torch.ones_like(pred), create_graph=True)[0]\n            for pred in preds\n        ]\n    )\n\n    loss: Tensor = self.weight * criterion(dy_dt, dy_dt_pred)\n\n    if log is not None:\n        log(\"loss/res\", loss)\n\n    return loss\n</code></pre>"},{"location":"reference/pinn/problems/sir_inverse/","title":"sir_inverse","text":""},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse","title":"<code>pinn.problems.sir_inverse</code>","text":""},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.BETA_KEY","title":"<code>BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.DELTA_KEY","title":"<code>DELTA_KEY = 'delta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.I_KEY","title":"<code>I_KEY = 'I'</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.N_KEY","title":"<code>N_KEY = 'N'</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.Rt_KEY","title":"<code>Rt_KEY = 'Rt'</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.S_KEY","title":"<code>S_KEY = 'S'</code>  <code>module-attribute</code>","text":""},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.SIRInvDataModule","title":"<code>SIRInvDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for SIR Inverse problem.</p> Source code in <code>src/pinn/problems/sir_inverse.py</code> <pre><code>class SIRInvDataModule(PINNDataModule):\n    \"\"\"\n    DataModule for SIR Inverse problem.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: SIRInvHyperparameters,\n        gen_props: ODEProperties | None = None,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ):\n        super().__init__(hp, validation, callbacks)\n        self.gen_props = gen_props\n\n    @override\n    def gen_coll(self, domain: Domain1D) -&gt; Tensor:\n        \"\"\"Generate collocation points.\"\"\"\n        x0 = torch.tensor(domain.x0, dtype=torch.float32)\n        x1 = torch.tensor(domain.x1, dtype=torch.float32)\n\n        coll = torch.rand((self.hp.training_data.collocations, 1))\n        coll = coll * (torch.log1p(x1) - torch.log1p(x0)) + torch.log1p(x0)\n        coll = torch.expm1(coll)\n        return coll\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate synthetic data.\"\"\"\n        assert self.gen_props is not None, \"SIR properties are required to generate data\"\n\n        args = self.gen_props.args.copy()\n        args.update(config.args_to_train)\n\n        data = odeint(\n            lambda x, y: self.gen_props.ode(x, y, args),\n            self.gen_props.y0,\n            config.x,\n        )\n\n        I_true = data[:, 1].clamp_min(0.0)\n\n        I_obs = self._noise(I_true, config.noise_level)\n\n        return config.x.unsqueeze(-1), I_obs.unsqueeze(-1)\n\n    def _noise(self, I_true: Tensor, noise_level: float) -&gt; Tensor:\n        if noise_level &lt; 1.0:\n            return I_true\n        else:\n            return torch.poisson(I_true / noise_level) * noise_level\n</code></pre>"},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.SIRInvDataModule.gen_props","title":"<code>gen_props = gen_props</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.SIRInvDataModule.__init__","title":"<code>__init__(hp: SIRInvHyperparameters, gen_props: ODEProperties | None = None, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None)</code>","text":"Source code in <code>src/pinn/problems/sir_inverse.py</code> <pre><code>def __init__(\n    self,\n    hp: SIRInvHyperparameters,\n    gen_props: ODEProperties | None = None,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n):\n    super().__init__(hp, validation, callbacks)\n    self.gen_props = gen_props\n</code></pre>"},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.SIRInvDataModule.gen_coll","title":"<code>gen_coll(domain: Domain1D) -&gt; Tensor</code>","text":"<p>Generate collocation points.</p> Source code in <code>src/pinn/problems/sir_inverse.py</code> <pre><code>@override\ndef gen_coll(self, domain: Domain1D) -&gt; Tensor:\n    \"\"\"Generate collocation points.\"\"\"\n    x0 = torch.tensor(domain.x0, dtype=torch.float32)\n    x1 = torch.tensor(domain.x1, dtype=torch.float32)\n\n    coll = torch.rand((self.hp.training_data.collocations, 1))\n    coll = coll * (torch.log1p(x1) - torch.log1p(x0)) + torch.log1p(x0)\n    coll = torch.expm1(coll)\n    return coll\n</code></pre>"},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.SIRInvDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate synthetic data.</p> Source code in <code>src/pinn/problems/sir_inverse.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate synthetic data.\"\"\"\n    assert self.gen_props is not None, \"SIR properties are required to generate data\"\n\n    args = self.gen_props.args.copy()\n    args.update(config.args_to_train)\n\n    data = odeint(\n        lambda x, y: self.gen_props.ode(x, y, args),\n        self.gen_props.y0,\n        config.x,\n    )\n\n    I_true = data[:, 1].clamp_min(0.0)\n\n    I_obs = self._noise(I_true, config.noise_level)\n\n    return config.x.unsqueeze(-1), I_obs.unsqueeze(-1)\n</code></pre>"},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.SIRInvHyperparameters","title":"<code>SIRInvHyperparameters</code>  <code>dataclass</code>","text":"<p>               Bases: <code>PINNHyperparameters</code></p> <p>Hyperparameters for the SIR Inverse problem.</p> Source code in <code>src/pinn/problems/sir_inverse.py</code> <pre><code>@dataclass(kw_only=True)\nclass SIRInvHyperparameters(PINNHyperparameters):\n    \"\"\"\n    Hyperparameters for the SIR Inverse problem.\n    \"\"\"\n\n    # TODO: implement adaptive weights\n    pde_weight: float = 1.0\n    ic_weight: float = 1.0\n    data_weight: float = 1.0\n</code></pre>"},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.SIRInvHyperparameters.data_weight","title":"<code>data_weight: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.SIRInvHyperparameters.ic_weight","title":"<code>ic_weight: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.SIRInvHyperparameters.pde_weight","title":"<code>pde_weight: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.SIRInvHyperparameters.__init__","title":"<code>__init__(*, lr: float, training_data: IngestionConfig | GenerationConfig, fields_config: MLPConfig, params_config: MLPConfig | ScalarConfig, scheduler: SchedulerConfig | None = None, early_stopping: EarlyStoppingConfig | None = None, smma_stopping: SMMAStoppingConfig | None = None, pde_weight: float = 1.0, ic_weight: float = 1.0, data_weight: float = 1.0) -&gt; None</code>","text":""},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.SIRInvProblem","title":"<code>SIRInvProblem</code>","text":"<p>               Bases: <code>Problem</code></p> <p>Definition of the SIR Inverse Problem. Infers parameters (beta) from data while satisfying the SIR ODE.</p> Source code in <code>src/pinn/problems/sir_inverse.py</code> <pre><code>class SIRInvProblem(Problem):\n    \"\"\"\n    Definition of the SIR Inverse Problem.\n    Infers parameters (beta) from data while satisfying the SIR ODE.\n    \"\"\"\n\n    def __init__(\n        self,\n        props: ODEProperties,\n        hp: SIRInvHyperparameters,\n        fields: FieldsRegistry,\n        params: ParamsRegistry,\n    ) -&gt; None:\n        def predict_data(\n            x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry\n        ) -&gt; Tensor:\n            I = fields[I_KEY]\n            I_pred = I(x_data)\n            return cast(Tensor, I_pred)\n\n        constraints: list[Constraint] = [\n            ResidualsConstraint(\n                props=props,\n                fields=fields,\n                params=params,\n                weight=hp.pde_weight,\n            ),\n            ICConstraint(\n                props=props,\n                fields=fields,\n                weight=hp.ic_weight,\n            ),\n            DataConstraint(\n                fields=fields,\n                params=params,\n                predict_data=predict_data,\n                weight=hp.data_weight,\n            ),\n        ]\n\n        criterion = nn.MSELoss()\n\n        super().__init__(\n            constraints=constraints,\n            criterion=criterion,\n            fields=fields,\n            params=params,\n        )\n</code></pre>"},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.SIRInvProblem.__init__","title":"<code>__init__(props: ODEProperties, hp: SIRInvHyperparameters, fields: FieldsRegistry, params: ParamsRegistry) -&gt; None</code>","text":"Source code in <code>src/pinn/problems/sir_inverse.py</code> <pre><code>def __init__(\n    self,\n    props: ODEProperties,\n    hp: SIRInvHyperparameters,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n) -&gt; None:\n    def predict_data(\n        x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry\n    ) -&gt; Tensor:\n        I = fields[I_KEY]\n        I_pred = I(x_data)\n        return cast(Tensor, I_pred)\n\n    constraints: list[Constraint] = [\n        ResidualsConstraint(\n            props=props,\n            fields=fields,\n            params=params,\n            weight=hp.pde_weight,\n        ),\n        ICConstraint(\n            props=props,\n            fields=fields,\n            weight=hp.ic_weight,\n        ),\n        DataConstraint(\n            fields=fields,\n            params=params,\n            predict_data=predict_data,\n            weight=hp.data_weight,\n        ),\n    ]\n\n    criterion = nn.MSELoss()\n\n    super().__init__(\n        constraints=constraints,\n        criterion=criterion,\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.SIR","title":"<code>SIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>The SIR ODE system. $$ \\begin{align} \\frac{dS}{dt} &amp;= -beta * S * I / N \\ \\frac{dI}{dt} &amp;= beta * S * I / N - delta * I \\ \\frac{dR}{dt} &amp;= delta * I \\ \\end{align} $$</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Time variable.</p> required <code>y</code> <code>Tensor</code> <p>State variables [S, I].</p> required <code>args</code> <code>ArgsRegistry</code> <p>Arguments dictionary (beta, delta, N).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Derivatives [dS/dt, dI/dt].</p> Source code in <code>src/pinn/problems/sir_inverse.py</code> <pre><code>def SIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"\n    The SIR ODE system.\n    $$\n    \\\\begin{align}\n    \\\\frac{dS}{dt} &amp;= -beta * S * I / N \\\\\\\\\n    \\\\frac{dI}{dt} &amp;= beta * S * I / N - delta * I \\\\\\\\\n    \\\\frac{dR}{dt} &amp;= delta * I \\\\\\\\\n    \\\\end{align}\n    $$\n\n    Args:\n        x: Time variable.\n        y: State variables [S, I].\n        args: Arguments dictionary (beta, delta, N).\n\n    Returns:\n        Derivatives [dS/dt, dI/dt].\n    \"\"\"\n    S, I = y\n    b, d, N = args[BETA_KEY], args[DELTA_KEY], args[N_KEY]\n\n    dS = -b(x) * S * I / N(x)\n    dI = b(x) * S * I / N(x) - d(x) * I\n    return torch.stack([dS, dI])\n</code></pre>"},{"location":"reference/pinn/problems/sir_inverse/#pinn.problems.sir_inverse.rSIR","title":"<code>rSIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>The reduced SIR ODE system. $$ \\begin{align} \\frac{dS}{dt} &amp;= -delta * R * I \\ \\frac{dI}{dt} &amp;= delta * (R - 1) * I \\ \\end{align} $$</p> <p>dI/dt = delta * (R - 1) * I</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Time variable.</p> required <code>y</code> <code>Tensor</code> <p>State variables [I].</p> required <code>args</code> <code>ArgsRegistry</code> <p>Arguments dictionary (delta, Rt).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Derivatives [dI/dt].</p> Source code in <code>src/pinn/problems/sir_inverse.py</code> <pre><code>def rSIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"\n    The reduced SIR ODE system.\n    $$\n    \\\\begin{align}\n    \\\\frac{dS}{dt} &amp;= -delta * R * I \\\\\n    \\\\frac{dI}{dt} &amp;= delta * (R - 1) * I \\\\\n    \\\\end{align}\n    $$\n\n    dI/dt = delta * (R - 1) * I\n\n    Args:\n        x: Time variable.\n        y: State variables [I].\n        args: Arguments dictionary (delta, Rt).\n\n    Returns:\n        Derivatives [dI/dt].\n    \"\"\"\n    I = y\n    d, Rt = args[DELTA_KEY], args[Rt_KEY]\n\n    dI = d(x) * (Rt(x) - 1) * I\n    return dI\n</code></pre>"}]}